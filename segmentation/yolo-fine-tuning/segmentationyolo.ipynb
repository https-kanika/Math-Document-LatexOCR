{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T17:10:14.603656Z",
     "iopub.status.busy": "2025-10-14T17:10:14.602720Z",
     "iopub.status.idle": "2025-10-14T17:11:37.908206Z",
     "shell.execute_reply": "2025-10-14T17:11:37.907314Z",
     "shell.execute_reply.started": "2025-10-14T17:10:14.603629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-14T17:28:23.316218Z",
     "iopub.status.busy": "2025-10-14T17:28:23.315872Z",
     "iopub.status.idle": "2025-10-14T17:28:58.444973Z",
     "shell.execute_reply": "2025-10-14T17:28:58.444027Z",
     "shell.execute_reply.started": "2025-10-14T17:28:23.316186Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "# Define Kaggle paths  \n",
    "kaggle_input_dir = \"/kaggle/input/document-dataset-for-yolo/document-dataset-for-yolo\"\n",
    "#kaggle_input_dir =\"/kaggle/input/mini-document-dataset-for-yolo/mini-document-dataset-for-yolo\"\n",
    "\n",
    "kaggle_working_dir = \"/kaggle/working\"\n",
    "os.makedirs(kaggle_working_dir, exist_ok=True)\n",
    "\n",
    "# Check available GPUs and print info\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.device_count() > 0:\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Path to existing yaml file\n",
    "yaml_path = os.path.join(kaggle_input_dir, \"dataset.yaml\")\n",
    "\n",
    "# Check if YAML file exists\n",
    "if not os.path.exists(yaml_path):\n",
    "    print(f\"Error: YAML file not found at {yaml_path}\")\n",
    "else:\n",
    "    # Load the existing YAML to display info\n",
    "    with open(yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(f\"Using existing YAML config at {yaml_path}\")\n",
    "    print(f\"Dataset classes: {config.get('names', {})}\")\n",
    "    print(f\"Train path: {config.get('train', '')}\")\n",
    "    print(f\"Val path: {config.get('val', '')}\")\n",
    "\n",
    "# Initialize model - using YOLOv11m for better performance\n",
    "model = YOLO('yolo11s.pt')\n",
    "\n",
    "# Training parameters for single GPU\n",
    "training_params = {\n",
    "    # Training parameters\n",
    "    \"epochs\": 30, \n",
    "    \"imgsz\": 1024,\n",
    "    \"batch\": 8,     # Reduced for single GPU\n",
    "    \"patience\": 15,\n",
    "    \"save\": True,\n",
    "    \"device\": 0,    # Use single GPU\n",
    "    \"workers\": 4,   # Reduced for single GPU\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"verbose\": True,\n",
    "    \"seed\": 42,\n",
    "    \"deterministic\": True,\n",
    "    \"single_cls\": False,  # Multiple classes for documents\n",
    "    \"rect\": False,\n",
    "    \"cos_lr\": True,\n",
    "    \"close_mosaic\": 10,\n",
    "    \"resume\": False,\n",
    "    \"amp\": True,     # Automatic Mixed Precision\n",
    "    \"fraction\": 1.0,\n",
    "    \"exist_ok\": True,\n",
    "    \"pretrained\": True,\n",
    "    \"plots\": True,\n",
    "    \"name\": \"document_detector_single_gpu\",\n",
    "    \n",
    "    # Hyperparameters\n",
    "    \"lr0\": 0.01,\n",
    "    \"lrf\": 0.01,\n",
    "    \"momentum\": 0.937,\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \"warmup_epochs\": 3.0,\n",
    "    \"warmup_momentum\": 0.8,\n",
    "    \"warmup_bias_lr\": 0.1,\n",
    "    \"box\": 7.5,      # Box loss gain (default)\n",
    "    \"cls\": 0.5,      # Class loss gain (default)\n",
    "    \"dfl\": 1.5,\n",
    "    \n",
    "    \"hsv_h\": 0.015,  # Hue variation (very slight - documents are mostly B&W)\n",
    "    \"hsv_s\": 0.3,    # Saturation variation (moderate - for scanned documents)\n",
    "    \"hsv_v\": 0.3,    # Value/brightness variation (moderate - lighting conditions)\n",
    "    \n",
    "    # GEOMETRIC AUGMENTATIONS - CONSERVATIVE (preserve text readability)\n",
    "    \"degrees\": 5.0,      # Small rotation (±5°) - realistic for slight page tilt\n",
    "    \"translate\": 0.05,   # Small translation (5%) - document position variation\n",
    "    \"scale\": 0.15,       # Moderate scaling (±15%) - distance from camera/scanner\n",
    "    \"shear\": 2.0,        # Minimal shear (2°) - perspective distortion\n",
    "    \"perspective\": 0.0001, # Very minimal perspective - almost flat documents\n",
    "    \n",
    "    # FLIP AUGMENTATIONS - STRATEGIC\n",
    "    \"flipud\": 0.0,       # NO vertical flip (text would be upside down)\n",
    "    \"fliplr\": 0.0,       # NO horizontal flip (equations have directionality)\n",
    "    \n",
    "    # ADVANCED AUGMENTATIONS - DISABLED (can distort text/equations)\n",
    "    \"mosaic\": 0.0,       # NO mosaic (would break document structure)\n",
    "    \"mixup\": 0.0,        # NO mixup (would blend different documents)\n",
    "    \"copy_paste\": 0.0,   # NO copy-paste (can create unrealistic layouts)\n",
    "    \n",
    "    # ADDITIONAL RECOMMENDED SETTINGS\n",
    "    \"dropout\": 0.0,      # No dropout augmentation\n",
    "    \"auto_augment\": None,\n",
    "}\n",
    "\n",
    "print(\"Starting training on single GPU...\")\n",
    "print(f\"Training parameters: {training_params}\")\n",
    "\n",
    "# Train the model directly using the original YAML\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    **training_params\n",
    ")\n",
    "# Save the trained model to Kaggle working directory\n",
    "model_save_path = os.path.join(kaggle_working_dir, \"document_detection_model.pt\")\n",
    "model.save(model_save_path)\n",
    "print(f\"Model saved to Kaggle working directory: {model_save_path}\")\n",
    "\n",
    "# Also export to ONNX format\n",
    "try:\n",
    "    onnx_save_path = os.path.join(kaggle_working_dir, \"document_detection_model.onnx\") \n",
    "    model.export(format=\"onnx\", imgsz=1024)\n",
    "    print(f\"Model exported to ONNX format at: {onnx_save_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")\n",
    "\n",
    "# Safely save training history if available\n",
    "try:\n",
    "    # First, check if the results object has the results_dict attribute\n",
    "    if hasattr(results, 'results_dict') and results.results_dict:\n",
    "        # Try to convert to DataFrame with explicit index\n",
    "        if isinstance(results.results_dict, dict):\n",
    "            # If it's a dictionary of epoch data\n",
    "            history_df = pd.DataFrame.from_dict(results.results_dict, orient='index')\n",
    "            history_path = os.path.join(kaggle_working_dir, \"training_history.csv\")\n",
    "            history_df.to_csv(history_path)\n",
    "            print(f\"Training history saved to: {history_path}\")\n",
    "        else:\n",
    "            # If it's a list or another format\n",
    "            history_df = pd.DataFrame(results.results_dict, index=range(len(results.results_dict)))\n",
    "            history_path = os.path.join(kaggle_working_dir, \"training_history.csv\")\n",
    "            history_df.to_csv(history_path, index=False)\n",
    "            print(f\"Training history saved to: {history_path}\")\n",
    "    else:\n",
    "        print(\"No results dictionary found in training results. Looking for CSV file...\")\n",
    "        \n",
    "    # Try to find the CSV file that YOLO automatically creates\n",
    "    runs_dir = os.path.join(kaggle_working_dir, \"runs\", \"detect\", \"document_detector_single_gpu\")\n",
    "    results_csv_path = os.path.join(runs_dir, \"results.csv\")\n",
    "    \n",
    "    if os.path.exists(results_csv_path):\n",
    "        print(f\"Found training history at: {results_csv_path}\")\n",
    "        history_df = pd.read_csv(results_csv_path)\n",
    "        history_path = os.path.join(kaggle_working_dir, \"training_history.csv\")\n",
    "        history_df.to_csv(history_path, index=False)\n",
    "        print(f\"Training history copied to: {history_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving training history: {str(e)}\")\n",
    "    print(\"Continuing with validation...\")\n",
    "\n",
    "# Validate the model\n",
    "val_results = model.val(\n",
    "    data=yaml_path,\n",
    "    imgsz=1024,\n",
    "    batch=8,\n",
    "    device=0,\n",
    "    plots=True\n",
    ")\n",
    "\n",
    "print(f\"Validation results:\")\n",
    "print(f\"  mAP@0.5 = {val_results.box.map50:.4f}\")\n",
    "print(f\"  mAP@0.5:0.95 = {val_results.box.map:.4f}\")\n",
    "print(f\"  Precision = {val_results.box.mp:.4f}\")\n",
    "print(f\"  Recall = {val_results.box.mr:.4f}\")\n",
    "\n",
    "# Save validation metrics\n",
    "metrics = {\n",
    "    \"mAP50\": val_results.box.map50,\n",
    "    \"mAP50-95\": val_results.box.map,\n",
    "    \"precision\": val_results.box.mp,\n",
    "    \"recall\": val_results.box.mr,\n",
    "    \"maps\":val_results.box.maps\n",
    "}\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_path = os.path.join(kaggle_working_dir, \"validation_metrics.csv\")\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"Validation metrics saved to: {metrics_path}\")\n",
    "\n",
    "# Get class-wise metrics\n",
    "class_names = list(config.get('names', {}).values()) if isinstance(config.get('names'), dict) else config.get('names', [])\n",
    "if len(class_names) > 0:\n",
    "    print(\"\\nClass-wise metrics:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        if hasattr(val_results.box, 'maps') and i < len(val_results.box.maps):\n",
    "            print(f\"  {class_name}: mAP50 = {val_results.box.maps[i]:.4f}\")\n",
    "\n",
    "\n",
    "# Add this code after your validation section (after line 197)\n",
    "\n",
    "# Get class-wise and overall metrics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE VALIDATION METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall metrics\n",
    "overall_precision = val_results.box.mp\n",
    "overall_recall = val_results.box.mr\n",
    "overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall + 1e-6)\n",
    "\n",
    "print(\"\\n--- OVERALL METRICS ---\")\n",
    "print(f\"Precision: {overall_precision:.4f}\")\n",
    "print(f\"Recall: {overall_recall:.4f}\")\n",
    "print(f\"F1-Score: {overall_f1:.4f}\")\n",
    "print(f\"mAP@0.5: {val_results.box.map50:.4f}\")\n",
    "print(f\"mAP@0.5:0.95: {val_results.box.map:.4f}\")\n",
    "\n",
    "# Class-wise metrics\n",
    "print(\"\\n--- CLASS-WISE METRICS ---\")\n",
    "\n",
    "class_metrics_list = []\n",
    "\n",
    "# Check if per-class metrics are available\n",
    "if hasattr(val_results.box, 'ap_class_index'):\n",
    "    ap_class_index = val_results.box.ap_class_index  # Classes that were found in validation\n",
    "    \n",
    "    # Get per-class precision and recall\n",
    "    if hasattr(val_results.box, 'p') and hasattr(val_results.box, 'r'):\n",
    "        precision_per_class = val_results.box.p  # Precision per class\n",
    "        recall_per_class = val_results.box.r      # Recall per class\n",
    "    else:\n",
    "        # Fallback if p and r attributes don't exist\n",
    "        precision_per_class = [overall_precision] * len(ap_class_index)\n",
    "        recall_per_class = [overall_recall] * len(ap_class_index)\n",
    "    \n",
    "    # Get mAP per class\n",
    "    maps = val_results.box.maps  # mAP@0.5 per class\n",
    "    \n",
    "    # Get mAP@0.5:0.95 per class if available\n",
    "    if hasattr(val_results.box, 'ap') and val_results.box.ap is not None:\n",
    "        ap = val_results.box.ap\n",
    "        if len(ap.shape) > 1:\n",
    "            map50_95_per_class = ap.mean(axis=1)  # Average across IoU thresholds\n",
    "        else:\n",
    "            map50_95_per_class = ap\n",
    "    else:\n",
    "        map50_95_per_class = [0] * len(ap_class_index)\n",
    "    \n",
    "    # Print and collect metrics for each class\n",
    "    for i, cls_idx in enumerate(ap_class_index):\n",
    "        cls_name = class_names[cls_idx] if cls_idx < len(class_names) else f\"Class {cls_idx}\"\n",
    "        \n",
    "        prec = float(precision_per_class[i]) if i < len(precision_per_class) else 0.0\n",
    "        rec = float(recall_per_class[i]) if i < len(recall_per_class) else 0.0\n",
    "        f1 = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        map50 = float(maps[i]) if i < len(maps) else 0.0\n",
    "        map50_95 = float(map50_95_per_class[i]) if i < len(map50_95_per_class) else 0.0\n",
    "        \n",
    "        print(f\"\\n{cls_name}:\")\n",
    "        print(f\"  Precision:    {prec:.4f}\")\n",
    "        print(f\"  Recall:       {rec:.4f}\")\n",
    "        print(f\"  F1-Score:     {f1:.4f}\")\n",
    "        print(f\"  mAP@0.5:      {map50:.4f}\")\n",
    "        print(f\"  mAP@0.5:0.95: {map50_95:.4f}\")\n",
    "        \n",
    "        class_metrics_list.append({\n",
    "            'class_id': int(cls_idx),\n",
    "            'class_name': cls_name,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1_score': f1,\n",
    "            'mAP50': map50,\n",
    "            'mAP50-95': map50_95\n",
    "        })\n",
    "else:\n",
    "    print(\"Per-class metrics not available in validation results\")\n",
    "\n",
    "# Save detailed metrics to CSV\n",
    "if class_metrics_list:\n",
    "    # Create DataFrame with both overall and class-wise metrics\n",
    "    overall_row = {\n",
    "        'class_id': -1,\n",
    "        'class_name': 'OVERALL',\n",
    "        'precision': overall_precision,\n",
    "        'recall': overall_recall,\n",
    "        'f1_score': overall_f1,\n",
    "        'mAP50': val_results.box.map50,\n",
    "        'mAP50-95': val_results.box.map\n",
    "    }\n",
    "    \n",
    "    all_metrics = [overall_row] + class_metrics_list\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "    \n",
    "    detailed_metrics_path = os.path.join(kaggle_working_dir, \"detailed_class_metrics.csv\")\n",
    "    metrics_df.to_csv(detailed_metrics_path, index=False)\n",
    "    print(f\"\\nDetailed metrics saved to: {detailed_metrics_path}\")\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8401506,
     "sourceId": 13258362,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8404612,
     "sourceId": 13262864,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8460285,
     "sourceId": 13341397,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8483719,
     "sourceId": 13372445,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8487338,
     "sourceId": 13377570,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
