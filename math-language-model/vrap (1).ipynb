{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0103fb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Preprocessing training set...\n",
      "Preprocessing validation set...\n",
      "Preprocessing test set...\n",
      "Training set: 1000 samples\n",
      "Validation set: 100 samples\n",
      "Test set: 100 samples\n",
      "Vocabulary size: 230 symbols\n",
      "Expected: 108 symbols (including <pad> and <eos>)\n",
      "Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# Set GPU device\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset path\n",
    "DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "\n",
    "# Load the CSV files\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train_database.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_database.csv'))\n",
    "val_df = pd.read_csv(os.path.join(DATA_PATH, 'val_database.csv'))\n",
    "\n",
    "def preprocess_latex(latex_str):\n",
    "    \"\"\"\n",
    "    Preprocessing steps as described in the paper:\n",
    "    1. Remove style-related characters (\\mathrm, \\textrm, \\operatorname, \\displaystyle)\n",
    "    2. Normalize LaTeX sequences (e.g., {a}^{2} -> a^{2})\n",
    "    \"\"\"\n",
    "    if pd.isna(latex_str):\n",
    "        return None\n",
    "    \n",
    "    # Remove style-related commands\n",
    "    latex_str = re.sub(r'\\\\mathrm\\{([^}]*)\\}', r'\\1', latex_str)\n",
    "    latex_str = re.sub(r'\\\\textrm\\{([^}]*)\\}', r'\\1', latex_str)\n",
    "    latex_str = re.sub(r'\\\\operatorname\\{([^}]*)\\}', r'\\1', latex_str)\n",
    "    latex_str = re.sub(r'\\\\displaystyle', '', latex_str)\n",
    "    \n",
    "    # Normalize single character in braces before superscript/subscript\n",
    "    # {a}^{2} -> a^{2}\n",
    "    latex_str = re.sub(r'\\{([^}])\\}\\^', r'\\1^', latex_str)\n",
    "    latex_str = re.sub(r'\\{([^}])\\}_', r'\\1_', latex_str)\n",
    "    \n",
    "    return latex_str.strip()\n",
    "\n",
    "def filter_invalid_syntax(latex_str):\n",
    "    \"\"\"\n",
    "    Filter out LaTeX sequences with invalid syntax\n",
    "    Check for balanced braces\n",
    "    \"\"\"\n",
    "    if pd.isna(latex_str) or latex_str == '':\n",
    "        return False\n",
    "    \n",
    "    # Check balanced braces\n",
    "    brace_count = 0\n",
    "    for char in latex_str:\n",
    "        if char == '{':\n",
    "            brace_count += 1\n",
    "        elif char == '}':\n",
    "            brace_count -= 1\n",
    "        if brace_count < 0:\n",
    "            return False\n",
    "    \n",
    "    return brace_count == 0\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Preprocessing training set...\")\n",
    "train_df['preprocessed_label'] = train_df['normalized_label'].apply(preprocess_latex)\n",
    "train_df = train_df[train_df['preprocessed_label'].apply(filter_invalid_syntax)]\n",
    "\n",
    "print(\"Preprocessing validation set...\")\n",
    "val_df['preprocessed_label'] = val_df['normalized_label'].apply(preprocess_latex)\n",
    "val_df = val_df[val_df['preprocessed_label'].apply(filter_invalid_syntax)]\n",
    "\n",
    "print(\"Preprocessing test set...\")\n",
    "test_df['preprocessed_label'] = test_df['normalized_label'].apply(preprocess_latex)\n",
    "test_df = test_df[test_df['preprocessed_label'].apply(filter_invalid_syntax)]\n",
    "\n",
    "# Save preprocessed data\n",
    "train_df.to_csv(os.path.join(DATA_PATH, 'train_database_preprocessed.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(DATA_PATH, 'val_database_preprocessed.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(DATA_PATH, 'test_database_preprocessed.csv'), index=False)\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Validation set: {len(val_df)} samples\")\n",
    "print(f\"Test set: {len(test_df)} samples\")\n",
    "\n",
    "# Build vocabulary (108 symbols including <pad> and <eos>)\n",
    "def tokenize_latex(latex_str):\n",
    "    \"\"\"\n",
    "    Tokenize LaTeX string into individual symbols\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(latex_str):\n",
    "        if latex_str[i] == '\\\\':\n",
    "            # LaTeX command\n",
    "            j = i + 1\n",
    "            while j < len(latex_str) and latex_str[j].isalpha():\n",
    "                j += 1\n",
    "            tokens.append(latex_str[i:j])\n",
    "            i = j\n",
    "        elif latex_str[i] in ['{', '}', '^', '_', '(', ')', '[', ']', '|', ',', '.', '=', '+', '-', '*', '/', '<', '>', '!', '&']:\n",
    "            # Special characters\n",
    "            tokens.append(latex_str[i])\n",
    "            i += 1\n",
    "        elif latex_str[i] == ' ':\n",
    "            # Skip spaces\n",
    "            i += 1\n",
    "        else:\n",
    "            # Regular character\n",
    "            tokens.append(latex_str[i])\n",
    "            i += 1\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def build_vocabulary(df_list):\n",
    "    \"\"\"\n",
    "    Build vocabulary from preprocessed labels\n",
    "    \"\"\"\n",
    "    vocab = set()\n",
    "    \n",
    "    for df in df_list:\n",
    "        for latex_str in df['preprocessed_label']:\n",
    "            if pd.notna(latex_str):\n",
    "                # Tokenize LaTeX string into symbols\n",
    "                tokens = tokenize_latex(latex_str)\n",
    "                vocab.update(tokens)\n",
    "    \n",
    "    # Add special tokens\n",
    "    vocab = ['<pad>', '<eos>'] + sorted(list(vocab))\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = build_vocabulary([train_df, val_df, test_df])\n",
    "\n",
    "# Save vocabulary\n",
    "with open(os.path.join(DATA_PATH, 'vocabulary.txt'), 'w') as f:\n",
    "    for token in vocab:\n",
    "        f.write(token + '\\n')\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)} symbols\")\n",
    "print(f\"Expected: 108 symbols (including <pad> and <eos>)\")\n",
    "\n",
    "# Create token to index mapping\n",
    "token2idx = {token: idx for idx, token in enumerate(vocab)}\n",
    "idx2token = {idx: token for token, idx in token2idx.items()}\n",
    "\n",
    "# Save mappings\n",
    "import pickle\n",
    "with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'wb') as f:\n",
    "    pickle.dump(token2idx, f)\n",
    "with open(os.path.join(DATA_PATH, 'idx2token.pkl'), 'wb') as f:\n",
    "    pickle.dump(idx2token, f)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8233ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Vocabulary size: 230\n",
      "\n",
      "Input Layer created successfully!\n",
      "Parameters:\n",
      "  - Vocabulary size: 230\n",
      "  - Embedding dimension (d_model): 256\n",
      "  - Maximum sequence length: 256\n",
      "  - Dropout rate: 0.1\n",
      "\n",
      "Test successful!\n",
      "  Input shape: torch.Size([4, 20])\n",
      "  Output shape: torch.Size([4, 20, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Input Embedding Layer\n",
    "    Embeds discrete tokens into continuous space\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (230 in your case)\n",
    "            d_model: Dimension of input embedding (256 as per paper)\n",
    "        \"\"\"\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            Embedded vectors, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding Layer\n",
    "    Adds positional information to embedded vectors using sine and cosine functions\n",
    "    Formula from paper (Vaswani et al., \"Attention Is All You Need\"):\n",
    "    PE(p, 2i) = sin(p / 10000^(2i/d_model))\n",
    "    PE(p, 2i+1) = cos(p / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of input embedding (256 as per paper)\n",
    "            max_seq_len: Maximum sequence length / context length (256 as per paper)\n",
    "            dropout: Dropout rate (0.1 as per paper)\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Compute div_term for all dimensions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices (2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices (2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension: (1, max_seq_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter, but part of module state)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Embedded vectors, shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Embedded vectors with positional encoding added, same shape\n",
    "        \"\"\"\n",
    "        # Add positional encoding to input\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class InputLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Input Layer combining Embedding and Positional Encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=256, max_seq_len=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (230 in your case)\n",
    "            d_model: Dimension of input embedding (256 as per paper)\n",
    "            max_seq_len: Maximum sequence length (256 as per paper)\n",
    "            dropout: Dropout rate (0.1 as per paper)\n",
    "        \"\"\"\n",
    "        super(InputLayer, self).__init__()\n",
    "        self.embedding = InputEmbedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            Embedded vectors with positional encoding, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_encoding(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the input layer\n",
    "if __name__ == \"__main__\":\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "    with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'rb') as f:\n",
    "        token2idx = pickle.load(f)\n",
    "    \n",
    "    vocab_size = len(token2idx)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Hyperparameters from paper\n",
    "    d_model = 256\n",
    "    max_seq_len = 256\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Create input layer\n",
    "    input_layer = InputLayer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        max_seq_len=max_seq_len,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\nInput Layer created successfully!\")\n",
    "    print(f\"Parameters:\")\n",
    "    print(f\"  - Vocabulary size: {vocab_size}\")\n",
    "    print(f\"  - Embedding dimension (d_model): {d_model}\")\n",
    "    print(f\"  - Maximum sequence length: {max_seq_len}\")\n",
    "    print(f\"  - Dropout rate: {dropout}\")\n",
    "    \n",
    "    # Test with dummy input\n",
    "    batch_size = 4\n",
    "    seq_len = 20\n",
    "    dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "    \n",
    "    output = input_layer(dummy_input)\n",
    "    print(f\"\\nTest successful!\")\n",
    "    print(f\"  Input shape: {dummy_input.shape}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4dae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "\n",
      "Transformer Layer created successfully!\n",
      "Parameters:\n",
      "  - Input/output dimension (d_model): 256\n",
      "  - Number of heads: 4\n",
      "  - Dimension per head: 64\n",
      "  - Feedforward hidden dimension: 1024\n",
      "  - Dropout rate: 0.1\n",
      "\n",
      "Test successful!\n",
      "  Input shape: torch.Size([4, 20, 256])\n",
      "  Output shape: torch.Size([4, 20, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MaskedMultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Masked Multi-Head Self-Attention (MMSA)\n",
    "    Uses scaled dot-product attention with mask to prevent attending to future tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, num_heads=4, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of input (256 as per paper)\n",
    "            num_heads: Number of attention heads (4 as per paper)\n",
    "            dropout: Dropout rate (0.1 as per paper)\n",
    "        \"\"\"\n",
    "        super(MaskedMultiHeadSelfAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head = 16\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Scaled Dot-Product Attention as per paper formula:\n",
    "        Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V\n",
    "        \n",
    "        Args:\n",
    "            Q: Queries, shape (batch_size, num_heads, seq_len, d_k)\n",
    "            K: Keys, shape (batch_size, num_heads, seq_len, d_k)\n",
    "            V: Values, shape (batch_size, num_heads, seq_len, d_k)\n",
    "            mask: Mask to prevent attending to future tokens\n",
    "        \"\"\"\n",
    "        # Compute attention scores: QK^T / sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask (set future positions to -inf before softmax)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Compute output: attention_weights * V\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch_size, seq_len, d_model)\n",
    "            mask: Causal mask to prevent attending to future tokens\n",
    "        Returns:\n",
    "            Output, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Linear projections and reshape for multi-head attention\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, num_heads, seq_len, d_k)\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads and apply output projection\n",
    "        # (batch_size, num_heads, seq_len, d_k) -> (batch_size, seq_len, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feedforward Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, d_ff=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Input/output dimension (256 as per paper)\n",
    "            d_ff: Hidden layer dimension (1024 as per paper)\n",
    "            dropout: Dropout rate (0.1 as per paper)\n",
    "        \"\"\"\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch_size, seq_len, d_model)\n",
    "        Returns:\n",
    "            Output, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single Transformer Layer\n",
    "    Architecture: MMSA -> Add & Norm -> FFN -> Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, num_heads=4, d_ff=1024, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Dimension of input (256 as per paper)\n",
    "            num_heads: Number of attention heads (4 as per paper)\n",
    "            d_ff: Feedforward hidden dimension (1024 as per paper)\n",
    "            dropout: Dropout rate (0.1 as per paper)\n",
    "        \"\"\"\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        \n",
    "        # Masked Multi-Head Self-Attention\n",
    "        self.mmsa = MaskedMultiHeadSelfAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Feedforward Neural Network\n",
    "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input, shape (batch_size, seq_len, d_model)\n",
    "            mask: Causal mask\n",
    "        Returns:\n",
    "            Output, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # MMSA with residual connection and layer norm\n",
    "        attn_output = self.mmsa(x, mask)\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "        \n",
    "        # FFN with residual connection and layer norm\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Test the transformer layer\n",
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Hyperparameters from paper\n",
    "    d_model = 256\n",
    "    num_heads = 4\n",
    "    d_ff = 1024\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Create transformer layer\n",
    "    transformer_layer = TransformerLayer(\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"\\nTransformer Layer created successfully!\")\n",
    "    print(f\"Parameters:\")\n",
    "    print(f\"  - Input/output dimension (d_model): {d_model}\")\n",
    "    print(f\"  - Number of heads: {num_heads}\")\n",
    "    print(f\"  - Dimension per head: {d_model // num_heads}\")\n",
    "    print(f\"  - Feedforward hidden dimension: {d_ff}\")\n",
    "    print(f\"  - Dropout rate: {dropout}\")\n",
    "    \n",
    "    # Test with dummy input\n",
    "    batch_size = 4\n",
    "    seq_len = 20\n",
    "    dummy_input = torch.randn(batch_size, seq_len, d_model).to(device)\n",
    "    \n",
    "    # Create causal mask (left-to-right)\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "    \n",
    "    output = transformer_layer(dummy_input, mask)\n",
    "    print(f\"\\nTest successful!\")\n",
    "    print(f\"  Input shape: {dummy_input.shape}\")\n",
    "    print(f\"  Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d45ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Vocabulary size: 230\n",
      "\n",
      "TMLM2L Model created successfully!\n",
      "Parameters:\n",
      "  - Number of transformer layers: 2\n",
      "  - Vocabulary size: 230\n",
      "  - Embedding dimension (d_model): 256\n",
      "  - Number of heads: 4\n",
      "  - Dimension per head: 64\n",
      "  - Feedforward hidden dimension: 1024\n",
      "  - Maximum sequence length: 256\n",
      "  - Dropout rate: 0.1\n",
      "  - Total parameters: 1.67M\n",
      "\n",
      "Test successful!\n",
      "  Input shape: torch.Size([4, 20])\n",
      "  Training loss: 7.5077\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TMLM(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based Math Language Model (TMLM)\n",
    "    Complete model with Input Layer, Stacked Transformer Layers, and Output Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, num_layers=2, d_model=256, num_heads=4, \n",
    "                 d_ff=1024, max_seq_len=256, dropout=0.1,pad_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (230 in your case)\n",
    "            num_layers: Number of transformer layers (2 for TMLM2L as per paper)\n",
    "            d_model: Dimension of input embedding (256 as per paper)\n",
    "            num_heads: Number of attention heads (4 as per paper)\n",
    "            d_ff: Feedforward hidden dimension (1024 as per paper)\n",
    "            max_seq_len: Maximum sequence length (256 as per paper)\n",
    "            dropout: Dropout rate (0.1 as per paper)\n",
    "        \"\"\"\n",
    "        super(TMLM, self).__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        \n",
    "        # Input Layer (Embedding + Positional Encoding)\n",
    "        self.input_layer = InputLayer(vocab_size, d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Stack of Transformer Layers\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output Layer (Adaptive Softmax)\n",
    "        self.output_layer = nn.AdaptiveLogSoftmaxWithLoss(\n",
    "            d_model, vocab_size, \n",
    "            cutoffs=[vocab_size // 10, 3 * vocab_size // 10]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input token indices, shape (batch_size, seq_len)\n",
    "            targets: Target token indices for training, shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            If targets provided: loss\n",
    "            Otherwise: logits for prediction\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        # Create causal mask (left-to-right)\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).to(x.device)\n",
    "        \n",
    "        # Input Layer\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        # Pass through transformer layers\n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x, mask)\n",
    "        \n",
    "        # Output Layer\n",
    "        if targets is not None:\n",
    "            # Training mode: compute loss\n",
    "            x = x.reshape(-1, x.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "            output = self.output_layer(x, targets)\n",
    "            return output.loss\n",
    "        else:\n",
    "            # Inference mode: return logits\n",
    "            output = self.output_layer.log_prob(x)\n",
    "            return output\n",
    "\n",
    "\n",
    "# Test the complete TMLM model\n",
    "if __name__ == \"__main__\":\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Dataset path\n",
    "    DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'rb') as f:\n",
    "        token2idx = pickle.load(f)\n",
    "    \n",
    "    vocab_size = len(token2idx)\n",
    "    pad_idx = token2idx['<pad>']\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    # Hyperparameters from paper\n",
    "    num_layers = 2  # TMLM2L\n",
    "    d_model = 256\n",
    "    num_heads = 4\n",
    "    d_ff = 1024\n",
    "    max_seq_len = 256\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # Create TMLM model\n",
    "    model = TMLM(\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    dropout=dropout,\n",
    "    pad_idx=pad_idx\n",
    ").to(device)\n",
    "\n",
    "    \n",
    "    print(f\"\\nTMLM2L Model created successfully!\")\n",
    "    print(f\"Parameters:\")\n",
    "    print(f\"  - Number of transformer layers: {num_layers}\")\n",
    "    print(f\"  - Vocabulary size: {vocab_size}\")\n",
    "    print(f\"  - Embedding dimension (d_model): {d_model}\")\n",
    "    print(f\"  - Number of heads: {num_heads}\")\n",
    "    print(f\"  - Dimension per head: {d_model // num_heads}\")\n",
    "    print(f\"  - Feedforward hidden dimension: {d_ff}\")\n",
    "    print(f\"  - Maximum sequence length: {max_seq_len}\")\n",
    "    print(f\"  - Dropout rate: {dropout}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  - Total parameters: {total_params / 1e6:.2f}M\")\n",
    "    \n",
    "    # Test with dummy input\n",
    "    batch_size = 4\n",
    "    seq_len = 20\n",
    "    dummy_input = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "    dummy_targets = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\n",
    "    \n",
    "    # Test training mode\n",
    "    model.train()\n",
    "    loss = model(dummy_input, dummy_targets)\n",
    "    print(f\"\\nTest successful!\")\n",
    "    print(f\"  Input shape: {dummy_input.shape}\")\n",
    "    print(f\"  Training loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1ab3c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "\n",
      "Model Configuration:\n",
      "  - Number of layers: 2 (TMLM2L)\n",
      "  - Vocabulary size: 230\n",
      "  - d_model: 256\n",
      "  - Number of heads: 4\n",
      "  - Feedforward hidden: 1024\n",
      "  - Max sequence length: 256\n",
      "  - Dropout: 0.1\n",
      "  - Learning rate: 1e-05\n",
      "  - Batch size: 32\n",
      "  - Epochs: 50\n",
      "\n",
      "Dataset sizes:\n",
      "  - Training: 1000\n",
      "  - Validation: 100\n",
      "  - Testing: 100\n",
      "\n",
      "Total parameters: 1.67M\n",
      "\n",
      "Starting training...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:01<00:00, 26.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:\n",
      "  Train Perplexity: 17.9829\n",
      "  Val Perplexity: 5.8458\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:00<00:00, 41.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50:\n",
      "  Train Perplexity: 3.8602\n",
      "  Val Perplexity: 2.3629\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:00<00:00, 44.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50:\n",
      "  Train Perplexity: 2.2733\n",
      "  Val Perplexity: 1.9281\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:01<00:00, 24.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50:\n",
      "  Train Perplexity: 1.9626\n",
      "  Val Perplexity: 1.7846\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50:\n",
      "  Train Perplexity: 1.8395\n",
      "  Val Perplexity: 1.7100\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:00<00:00, 39.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50:\n",
      "  Train Perplexity: 1.7732\n",
      "  Val Perplexity: 1.6659\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:01<00:00, 29.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50:\n",
      "  Train Perplexity: 1.7219\n",
      "  Val Perplexity: 1.6370\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50:\n",
      "  Train Perplexity: 1.6805\n",
      "  Val Perplexity: 1.6154\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50 [Train]: 100%|████████████████████████████████████████████████| 32/32 [00:01<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50:\n",
      "  Train Perplexity: 1.6657\n",
      "  Val Perplexity: 1.5982\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 41.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50:\n",
      "  Train Perplexity: 1.6425\n",
      "  Val Perplexity: 1.5832\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 29.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50:\n",
      "  Train Perplexity: 1.6249\n",
      "  Val Perplexity: 1.5702\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50:\n",
      "  Train Perplexity: 1.6172\n",
      "  Val Perplexity: 1.5587\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 28.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50:\n",
      "  Train Perplexity: 1.5937\n",
      "  Val Perplexity: 1.5484\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 32.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50:\n",
      "  Train Perplexity: 1.5948\n",
      "  Val Perplexity: 1.5390\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 37.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50:\n",
      "  Train Perplexity: 1.5749\n",
      "  Val Perplexity: 1.5305\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 38.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50:\n",
      "  Train Perplexity: 1.5599\n",
      "  Val Perplexity: 1.5229\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 38.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50:\n",
      "  Train Perplexity: 1.5600\n",
      "  Val Perplexity: 1.5159\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50:\n",
      "  Train Perplexity: 1.5450\n",
      "  Val Perplexity: 1.5096\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 27.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50:\n",
      "  Train Perplexity: 1.5523\n",
      "  Val Perplexity: 1.5037\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50:\n",
      "  Train Perplexity: 1.5408\n",
      "  Val Perplexity: 1.4980\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 42.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50:\n",
      "  Train Perplexity: 1.5281\n",
      "  Val Perplexity: 1.4930\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 36.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50:\n",
      "  Train Perplexity: 1.5240\n",
      "  Val Perplexity: 1.4884\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 28.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50:\n",
      "  Train Perplexity: 1.5289\n",
      "  Val Perplexity: 1.4841\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 40.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50:\n",
      "  Train Perplexity: 1.5131\n",
      "  Val Perplexity: 1.4800\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 32.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50:\n",
      "  Train Perplexity: 1.5117\n",
      "  Val Perplexity: 1.4762\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 44.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50:\n",
      "  Train Perplexity: 1.5058\n",
      "  Val Perplexity: 1.4727\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 27.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50:\n",
      "  Train Perplexity: 1.5092\n",
      "  Val Perplexity: 1.4691\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50:\n",
      "  Train Perplexity: 1.4980\n",
      "  Val Perplexity: 1.4659\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 43.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50:\n",
      "  Train Perplexity: 1.4964\n",
      "  Val Perplexity: 1.4627\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 27.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50:\n",
      "  Train Perplexity: 1.5016\n",
      "  Val Perplexity: 1.4598\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 29.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50:\n",
      "  Train Perplexity: 1.4849\n",
      "  Val Perplexity: 1.4568\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 31.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50:\n",
      "  Train Perplexity: 1.4848\n",
      "  Val Perplexity: 1.4541\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 33.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50:\n",
      "  Train Perplexity: 1.4826\n",
      "  Val Perplexity: 1.4512\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 42.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50:\n",
      "  Train Perplexity: 1.4823\n",
      "  Val Perplexity: 1.4486\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 27.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50:\n",
      "  Train Perplexity: 1.4747\n",
      "  Val Perplexity: 1.4460\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 44.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50:\n",
      "  Train Perplexity: 1.4735\n",
      "  Val Perplexity: 1.4435\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 27.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50:\n",
      "  Train Perplexity: 1.4717\n",
      "  Val Perplexity: 1.4410\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 32.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50:\n",
      "  Train Perplexity: 1.4636\n",
      "  Val Perplexity: 1.4385\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 37.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50:\n",
      "  Train Perplexity: 1.4708\n",
      "  Val Perplexity: 1.4361\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 33.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50:\n",
      "  Train Perplexity: 1.4618\n",
      "  Val Perplexity: 1.4336\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 24.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50:\n",
      "  Train Perplexity: 1.4591\n",
      "  Val Perplexity: 1.4313\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 29.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50:\n",
      "  Train Perplexity: 1.4573\n",
      "  Val Perplexity: 1.4290\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 42.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50:\n",
      "  Train Perplexity: 1.4487\n",
      "  Val Perplexity: 1.4265\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 28.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50:\n",
      "  Train Perplexity: 1.4477\n",
      "  Val Perplexity: 1.4243\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 29.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50:\n",
      "  Train Perplexity: 1.4447\n",
      "  Val Perplexity: 1.4220\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50:\n",
      "  Train Perplexity: 1.4438\n",
      "  Val Perplexity: 1.4198\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 38.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50:\n",
      "  Train Perplexity: 1.4451\n",
      "  Val Perplexity: 1.4177\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:00<00:00, 34.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50:\n",
      "  Train Perplexity: 1.4368\n",
      "  Val Perplexity: 1.4154\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 30.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50:\n",
      "  Train Perplexity: 1.4359\n",
      "  Val Perplexity: 1.4133\n",
      "  *** Best model saved! ***\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50 [Train]: 100%|███████████████████████████████████████████████| 32/32 [00:01<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50:\n",
      "  Train Perplexity: 1.4332\n",
      "  Val Perplexity: 1.4112\n",
      "  *** Best model saved! ***\n",
      "\n",
      "\n",
      "Loading best model for testing...\n",
      "\n",
      "Test Perplexity: 1.3405\n",
      "Paper TMLM2L Perplexity: 4.598\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Dataset class\n",
    "class LaTeXDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for LaTeX sequences\n",
    "    \"\"\"\n",
    "    def __init__(self, csv_file, token2idx, max_seq_len=256):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to preprocessed CSV file\n",
    "            token2idx: Token to index mapping\n",
    "            max_seq_len: Maximum sequence length (256 as per paper)\n",
    "        \"\"\"\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.token2idx = token2idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.pad_idx = token2idx['<pad>']\n",
    "        self.eos_idx = token2idx['<eos>']\n",
    "    \n",
    "    def tokenize_latex(self, latex_str):\n",
    "        \"\"\"Tokenize LaTeX string into individual symbols\"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(latex_str):\n",
    "            if latex_str[i] == '\\\\':\n",
    "                j = i + 1\n",
    "                while j < len(latex_str) and latex_str[j].isalpha():\n",
    "                    j += 1\n",
    "                tokens.append(latex_str[i:j])\n",
    "                i = j\n",
    "            elif latex_str[i] in ['{', '}', '^', '_', '(', ')', '[', ']', '|', ',', '.', '=', '+', '-', '*', '/', '<', '>', '!', '&']:\n",
    "                tokens.append(latex_str[i])\n",
    "                i += 1\n",
    "            elif latex_str[i] == ' ':\n",
    "                i += 1\n",
    "            else:\n",
    "                tokens.append(latex_str[i])\n",
    "                i += 1\n",
    "        return tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        latex_str = self.df.iloc[idx]['preprocessed_label']\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenize_latex(latex_str)\n",
    "        \n",
    "        # Convert to indices and add <eos>\n",
    "        token_ids = [self.token2idx.get(token, self.pad_idx) for token in tokens]\n",
    "        token_ids.append(self.eos_idx)\n",
    "        \n",
    "        # Truncate or pad\n",
    "        if len(token_ids) > self.max_seq_len:\n",
    "            token_ids = token_ids[:self.max_seq_len]\n",
    "        else:\n",
    "            token_ids += [self.pad_idx] * (self.max_seq_len - len(token_ids))\n",
    "        \n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, dataloader, device,pad_idx):\n",
    "    \"\"\"\n",
    "    Calculate perplexity as per paper formula:\n",
    "    Perplexity = exp(-1/N * sum(log p(x_i | x_1, ..., x_{i-1})))\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Input: all tokens except last, Target: all tokens except first\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model(inputs, targets)\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            mask = (targets != pad_idx).float()\n",
    "            n_tokens = mask.sum().item()\n",
    "            \n",
    "            total_loss += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "\n",
    "def train_tmlm():\n",
    "    \"\"\"\n",
    "    Train TMLM model with exact settings from paper\n",
    "    \"\"\"\n",
    "    # Set device\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    # Load vocabulary\n",
    "    DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "    with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'rb') as f:\n",
    "        token2idx = pickle.load(f)\n",
    "    with open(os.path.join(DATA_PATH, 'idx2token.pkl'), 'rb') as f:\n",
    "        idx2token = pickle.load(f)\n",
    "    \n",
    "    vocab_size = len(token2idx)\n",
    "    pad_idx = token2idx['<pad>']\n",
    "\n",
    "    \n",
    "    # Hyperparameters from paper\n",
    "    num_layers = 2  # TMLM2L\n",
    "    d_model = 256\n",
    "    num_heads = 4\n",
    "    d_ff = 1024\n",
    "    max_seq_len = 256\n",
    "    dropout = 0.1\n",
    "    learning_rate = 1e-5  # 10^-5 as per paper\n",
    "    batch_size = 32\n",
    "    num_epochs = 50\n",
    "    \n",
    "    print(f\"Model Configuration:\")\n",
    "    print(f\"  - Number of layers: {num_layers} (TMLM2L)\")\n",
    "    print(f\"  - Vocabulary size: {vocab_size}\")\n",
    "    print(f\"  - d_model: {d_model}\")\n",
    "    print(f\"  - Number of heads: {num_heads}\")\n",
    "    print(f\"  - Feedforward hidden: {d_ff}\")\n",
    "    print(f\"  - Max sequence length: {max_seq_len}\")\n",
    "    print(f\"  - Dropout: {dropout}\")\n",
    "    print(f\"  - Learning rate: {learning_rate}\")\n",
    "    print(f\"  - Batch size: {batch_size}\")\n",
    "    print(f\"  - Epochs: {num_epochs}\\n\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = LaTeXDataset(\n",
    "        os.path.join(DATA_PATH, 'train_database_preprocessed.csv'),\n",
    "        token2idx, max_seq_len\n",
    "    )\n",
    "    val_dataset = LaTeXDataset(\n",
    "        os.path.join(DATA_PATH, 'val_database_preprocessed.csv'),\n",
    "        token2idx, max_seq_len\n",
    "    )\n",
    "    test_dataset = LaTeXDataset(\n",
    "        os.path.join(DATA_PATH, 'test_database_preprocessed.csv'),\n",
    "        token2idx, max_seq_len\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset sizes:\")\n",
    "    print(f\"  - Training: {len(train_dataset)}\")\n",
    "    print(f\"  - Validation: {len(val_dataset)}\")\n",
    "    print(f\"  - Testing: {len(test_dataset)}\\n\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Create model\n",
    "    model = TMLM(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        d_ff=d_ff,\n",
    "        max_seq_len=max_seq_len,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params / 1e6:.2f}M\\n\")\n",
    "    \n",
    "    # Optimizer: AdamW with learning rate 10^-5 as per paper\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"Starting training...\\n\")\n",
    "    best_val_perplexity = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_tokens = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            # Input: all tokens except last, Target: all tokens except first\n",
    "            inputs = batch[:, :-1]\n",
    "            targets = batch[:, 1:]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model(inputs, targets)\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            mask = (targets != token2idx['<pad>']).float()\n",
    "            n_tokens = mask.sum().item()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * n_tokens\n",
    "            train_tokens += n_tokens\n",
    "        \n",
    "        # Calculate training perplexity\n",
    "        train_perplexity = math.exp(train_loss / train_tokens)\n",
    "        \n",
    "        # Validation\n",
    "        val_perplexity = calculate_perplexity(model, val_loader, device,pad_idx)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Train Perplexity: {train_perplexity:.4f}\")\n",
    "        print(f\"  Val Perplexity: {val_perplexity:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_perplexity < best_val_perplexity:\n",
    "            best_val_perplexity = val_perplexity\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_perplexity': val_perplexity,\n",
    "            }, os.path.join(DATA_PATH, 'best_tmlm2l_model.pt'))\n",
    "            print(f\"  *** Best model saved! ***\")\n",
    "        print()\n",
    "    \n",
    "    # Load best model and evaluate on test set\n",
    "    print(\"\\nLoading best model for testing...\")\n",
    "    checkpoint = torch.load(os.path.join(DATA_PATH, 'best_tmlm2l_model.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    test_perplexity = calculate_perplexity(model, test_loader, device,pad_idx)\n",
    "    print(f\"\\nTest Perplexity: {test_perplexity:.4f}\")\n",
    "    print(f\"Paper TMLM2L Perplexity: 4.598\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = train_tmlm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650c32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def predict_latex(model, image_path, token2idx, idx2token, max_len=256, device='cuda:1'):\n",
    "    \"\"\"\n",
    "    Generate LaTeX prediction from a handwritten math image\n",
    "    (This requires an image encoder which the paper uses SRTC+SLP)\n",
    "    For now, we can test the language model on partial sequences\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # For demonstration: start with a partial LaTeX sequence and let model complete it\n",
    "    # In full system, this would come from the image recognizer\n",
    "    \n",
    "    partial_sequence = input(\"Enter partial LaTeX sequence (or press enter for empty): \")\n",
    "    \n",
    "    if not partial_sequence:\n",
    "        # Start with empty/start token\n",
    "        tokens = [token2idx['<pad>']]\n",
    "    else:\n",
    "        # Tokenize input\n",
    "        tokens = tokenize_partial(partial_sequence, token2idx)\n",
    "    \n",
    "    generated = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            # Prepare input\n",
    "            input_seq = torch.tensor([tokens]).to(device)\n",
    "            \n",
    "            # Get model output (log probabilities)\n",
    "            output = model.output_layer.log_prob(\n",
    "                model.input_layer(input_seq).view(-1, 256)\n",
    "            )\n",
    "            \n",
    "            # Get next token (greedy decoding)\n",
    "            next_token = output[-1].argmax().item()\n",
    "            \n",
    "            # Stop if EOS token\n",
    "            if idx2token[next_token] == '<eos>':\n",
    "                break\n",
    "                \n",
    "            generated.append(idx2token[next_token])\n",
    "            tokens.append(next_token)\n",
    "    \n",
    "    return ''.join(generated)\n",
    "\n",
    "def tokenize_partial(latex_str, token2idx):\n",
    "    \"\"\"Tokenize partial LaTeX string\"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(latex_str):\n",
    "        if latex_str[i] == '\\\\':\n",
    "            j = i + 1\n",
    "            while j < len(latex_str) and latex_str[j].isalpha():\n",
    "                j += 1\n",
    "            token = latex_str[i:j]\n",
    "            tokens.append(token2idx.get(token, token2idx['<pad>']))\n",
    "            i = j\n",
    "        elif latex_str[i] in ['{', '}', '^', '_', '(', ')', '[', ']', '|', ',', '.', '=', '+', '-', '*', '/', '<', '>', '!', '&']:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "        elif latex_str[i] == ' ':\n",
    "            i += 1\n",
    "        else:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29e8b5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "Image: c01aa8332aca1e8e.png\n",
      "Ground Truth: I=\\int Fdt\n",
      "Prediction: [Would require image encoder]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Image: 8e70d3a0a7a20e33.png\n",
      "Ground Truth: c>\\aleph_{0}\n",
      "Prediction: [Would require image encoder]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Image: 8a4aaa63197ac019.png\n",
      "Ground Truth: \\frac{\\partial r_{i}}{\\partial\\beta_{j}}\n",
      "Prediction: [Would require image encoder]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Image: 7d3986665243fc8a.png\n",
      "Ground Truth: \\{x\\}\\times\\mathbbR^{n}\n",
      "Prediction: [Would require image encoder]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Image: 41c6736570400100.png\n",
      "Ground Truth: d_K^{-p/2}\n",
      "Prediction: [Would require image encoder]\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def evaluate_on_test_images():\n",
    "    \"\"\"\n",
    "    Evaluate model predictions on test images\n",
    "    \"\"\"\n",
    "    DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_database_preprocessed.csv'))\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'rb') as f:\n",
    "        token2idx = pickle.load(f)\n",
    "    with open(os.path.join(DATA_PATH, 'idx2token.pkl'), 'rb') as f:\n",
    "        idx2token = pickle.load(f)\n",
    "    \n",
    "    # Load model\n",
    "    device = torch.device('cuda:1')\n",
    "    model = TMLM(\n",
    "        vocab_size=len(token2idx),\n",
    "        num_layers=2,\n",
    "        pad_idx=token2idx['<pad>']\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(os.path.join(DATA_PATH, 'best_tmlm2l_model.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Sample 5 random test cases\n",
    "    samples = test_df.sample(5)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST PREDICTIONS\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"Image: {row['filename']}\")\n",
    "        print(f\"Ground Truth: {row['preprocessed_label']}\")\n",
    "        print(f\"Prediction: [Would require image encoder]\")\n",
    "        print(\"-\"*80 + \"\\n\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_on_test_images()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51f3cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LANGUAGE MODEL COMPLETION TEST\n",
      "================================================================================\n",
      "\n",
      "Input:  \\frac{x\n",
      "Output: \\frac{x}}}}}}\n",
      "--------------------------------------------------------------------------------\n",
      "Input:  \\int\n",
      "Output: \\int}\n",
      "--------------------------------------------------------------------------------\n",
      "Input:  a^{2\n",
      "Output: a^{2}}}}\n",
      "--------------------------------------------------------------------------------\n",
      "Input:  \\sqrt{\n",
      "Output: \\sqrt{2}}}}}\n",
      "--------------------------------------------------------------------------------\n",
      "Input:  x=\n",
      "Output: x={2}}}}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "PERPLEXITY COMPARISON ON TEST SAMPLES\n",
      "================================================================================\n",
      "\n",
      "LaTeX: \\int dx\\psi_n^{*}(s)\n",
      "Loss: 0.2185\n",
      "Perplexity: 1.2442\n",
      "--------------------------------------------------------------------------------\n",
      "LaTeX: \\Gamma(\\frac{x}{y})\\rightarrow y\\Gamma(x)\n",
      "Loss: 0.2238\n",
      "Perplexity: 1.2508\n",
      "--------------------------------------------------------------------------------\n",
      "LaTeX: u:\\Omega\\rightarrow\\mathbb{C}\n",
      "Loss: 0.1557\n",
      "Perplexity: 1.1684\n",
      "--------------------------------------------------------------------------------\n",
      "LaTeX: {(\\beta^{\\chi})}^{\\eta}\n",
      "Loss: 0.1867\n",
      "Perplexity: 1.2053\n",
      "--------------------------------------------------------------------------------\n",
      "LaTeX: 4x+5y=32\n",
      "Loss: 0.1506\n",
      "Perplexity: 1.1625\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def generate_latex_completion(model, start_tokens, token2idx, idx2token, max_len=50, device='cuda:1'):\n",
    "    \"\"\"\n",
    "    Generate LaTeX completion using the trained language model\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = start_tokens.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            # Prepare input (pad if needed)\n",
    "            input_seq = tokens + [token2idx['<pad>']] * (256 - len(tokens))\n",
    "            input_seq = input_seq[:256]\n",
    "            input_tensor = torch.tensor([input_seq]).to(device)\n",
    "            \n",
    "            # Get embeddings through input layer\n",
    "            x = model.input_layer(input_tensor)\n",
    "            \n",
    "            # Pass through transformer layers\n",
    "            seq_len = input_tensor.size(1)\n",
    "            mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            \n",
    "            for transformer_layer in model.transformer_layers:\n",
    "                x = transformer_layer(x, mask)\n",
    "            \n",
    "            # Get logits for the last actual token position\n",
    "            last_pos = len(tokens) - 1\n",
    "            logits = model.output_layer.log_prob(x[:, last_pos, :])\n",
    "            \n",
    "            # Get next token (greedy decoding)\n",
    "            next_token = logits.argmax().item()\n",
    "            \n",
    "            # Stop if EOS or pad token\n",
    "            if idx2token[next_token] in ['<eos>', '<pad>']:\n",
    "                break\n",
    "            \n",
    "            tokens.append(next_token)\n",
    "    \n",
    "    # Convert tokens to string\n",
    "    result = []\n",
    "    for token_id in tokens:\n",
    "        if idx2token[token_id] not in ['<pad>', '<eos>']:\n",
    "            result.append(idx2token[token_id])\n",
    "    \n",
    "    return ''.join(result)\n",
    "\n",
    "\n",
    "def tokenize_latex(latex_str, token2idx):\n",
    "    \"\"\"Tokenize LaTeX string\"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(latex_str):\n",
    "        if latex_str[i] == '\\\\':\n",
    "            j = i + 1\n",
    "            while j < len(latex_str) and latex_str[j].isalpha():\n",
    "                j += 1\n",
    "            token = latex_str[i:j]\n",
    "            tokens.append(token2idx.get(token, token2idx['<pad>']))\n",
    "            i = j\n",
    "        elif latex_str[i] in ['{', '}', '^', '_', '(', ')', '[', ']', '|', ',', '.', '=', '+', '-', '*', '/', '<', '>', '!', '&']:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "        elif latex_str[i] == ' ':\n",
    "            i += 1\n",
    "        else:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def test_language_model_capabilities():\n",
    "    \"\"\"\n",
    "    Test the trained language model's capabilities\n",
    "    \"\"\"\n",
    "    DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'rb') as f:\n",
    "        token2idx = pickle.load(f)\n",
    "    with open(os.path.join(DATA_PATH, 'idx2token.pkl'), 'rb') as f:\n",
    "        idx2token = pickle.load(f)\n",
    "    \n",
    "    # Load model\n",
    "    device = torch.device('cuda:1')\n",
    "    model = TMLM(\n",
    "        vocab_size=len(token2idx),\n",
    "        num_layers=2,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        d_ff=1024,\n",
    "        max_seq_len=256,\n",
    "        dropout=0.1,\n",
    "        pad_idx=token2idx['<pad>']\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(os.path.join(DATA_PATH, 'best_tmlm2l_model.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LANGUAGE MODEL COMPLETION TEST\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Test cases: partial LaTeX sequences\n",
    "    test_cases = [\n",
    "        \"\\\\frac{x\",\n",
    "        \"\\\\int\",\n",
    "        \"a^{2\",\n",
    "        \"\\\\sqrt{\",\n",
    "        \"x=\"\n",
    "    ]\n",
    "    \n",
    "    for partial in test_cases:\n",
    "        tokens = tokenize_latex(partial, token2idx)\n",
    "        completed = generate_latex_completion(model, tokens, token2idx, idx2token, device=device)\n",
    "        print(f\"Input:  {partial}\")\n",
    "        print(f\"Output: {completed}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERPLEXITY COMPARISON ON TEST SAMPLES\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_database_preprocessed.csv'))\n",
    "    \n",
    "    # Test perplexity on individual samples\n",
    "    samples = test_df.sample(5)\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        latex = row['preprocessed_label']\n",
    "        tokens = tokenize_latex(latex, token2idx)\n",
    "        \n",
    "        # Calculate perplexity for this sequence\n",
    "        if len(tokens) > 1:\n",
    "            input_seq = tokens + [token2idx['<eos>']]\n",
    "            input_seq = input_seq + [token2idx['<pad>']] * (256 - len(input_seq))\n",
    "            input_seq = input_seq[:256]\n",
    "            \n",
    "            input_tensor = torch.tensor([input_seq]).to(device)\n",
    "            targets = input_tensor[:, 1:]\n",
    "            inputs = input_tensor[:, :-1]\n",
    "            \n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                loss = model(inputs, targets)\n",
    "            \n",
    "            print(f\"LaTeX: {latex}\")\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "            print(f\"Perplexity: {torch.exp(loss).item():.4f}\")\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_language_model_capabilities()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3d840e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Test Dataset Evaluation...\n",
      "\n",
      "====================================================================================================\n",
      "DETAILED TEST DATASET EVALUATION\n",
      "====================================================================================================\n",
      "\n",
      "Sample 1/100\n",
      "  File: 987b9c49d6879c44.png\n",
      "  LaTeX: M,w\\models I^{\\alpha}(e)\n",
      "  Tokens: 12\n",
      "  Loss: 0.2096\n",
      "  Perplexity: 1.2331\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 2/100\n",
      "  File: e8a00f63b814fdd6.png\n",
      "  LaTeX: dX=\\frac{\\partial X}{\\partial x}dx=F^{-1}dx=HdxordX_{M}=\\frac{\\partial X_{M}}{\\partial x_{n}}dx_{n}\n",
      "  Tokens: 59\n",
      "  Loss: 0.8801\n",
      "  Perplexity: 2.4111\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 3/100\n",
      "  File: 2ea09c1e42b328ee.png\n",
      "  LaTeX: TU=\\sqrt{\\frac{DU^{3}}{G*M}}\n",
      "  Tokens: 20\n",
      "  Loss: 0.3209\n",
      "  Perplexity: 1.3784\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 4/100\n",
      "  File: 0184526868aa528e.png\n",
      "  LaTeX: (\\frac{4}{7}-9)^{204\\cdot\\sqrt{5}}\n",
      "  Tokens: 22\n",
      "  Loss: 0.3005\n",
      "  Perplexity: 1.3505\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 5/100\n",
      "  File: e7e4de144a319f79.png\n",
      "  LaTeX: \\frac{1}{M^{n-1}\\cdot s}\n",
      "  Tokens: 15\n",
      "  Loss: 0.2009\n",
      "  Perplexity: 1.2225\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 6/100\n",
      "  File: 5d6263f93a735248.png\n",
      "  LaTeX: \\frac{1}{5}\n",
      "  Tokens: 7\n",
      "  Loss: 0.0611\n",
      "  Perplexity: 1.0630\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 7/100\n",
      "  File: c96c37e8df988c92.png\n",
      "  LaTeX: V=\\frac{1}{3}Ah\\rightarrow3V=Ah\\rightarrow Ah=3V\\rightarrow h=3\\frac{V}{A}\n",
      "  Tokens: 34\n",
      "  Loss: 0.5635\n",
      "  Perplexity: 1.7567\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 8/100\n",
      "  File: e0d7ede5a6423ea9.png\n",
      "  LaTeX: w_z^{\\psi}\n",
      "  Tokens: 7\n",
      "  Loss: 0.1063\n",
      "  Perplexity: 1.1121\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 9/100\n",
      "  File: c952c04b266bf300.png\n",
      "  LaTeX: \\alpha\\vee\\rho\n",
      "  Tokens: 3\n",
      "  Loss: 0.0664\n",
      "  Perplexity: 1.0687\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 10/100\n",
      "  File: ef085e4417130052.png\n",
      "  LaTeX: \\kappa:X\\times B\\rightarrow[0,1]\n",
      "  Tokens: 11\n",
      "  Loss: 0.2371\n",
      "  Perplexity: 1.2676\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 11/100\n",
      "  File: 9db83df95de0fbec.png\n",
      "  LaTeX: \\frac{c}{\\frac{y}{\\frac{1000000}{24.1451}}}\n",
      "  Tokens: 31\n",
      "  Loss: 0.3454\n",
      "  Perplexity: 1.4125\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 12/100\n",
      "  File: 41599f74f3ac2263.png\n",
      "  LaTeX: {6^{437}}^{\\frac{14}{64}}\n",
      "  Tokens: 21\n",
      "  Loss: 0.2729\n",
      "  Perplexity: 1.3137\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 13/100\n",
      "  File: a30ae78768f5d7db.png\n",
      "  LaTeX: (cos\\frac{\\theta}{2},sin\\frac{\\theta}{2})\n",
      "  Tokens: 23\n",
      "  Loss: 0.3173\n",
      "  Perplexity: 1.3734\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 14/100\n",
      "  File: bcf669c0b8d925ad.png\n",
      "  LaTeX: \\frac{\\frac{t}{8}-53}{7}\n",
      "  Tokens: 16\n",
      "  Loss: 0.1937\n",
      "  Perplexity: 1.2138\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 15/100\n",
      "  File: 0ac18f83421bfe26.png\n",
      "  LaTeX: 2log_{2}\\lambda\\approx1.523627086202492,\n",
      "  Tokens: 28\n",
      "  Loss: 0.4878\n",
      "  Perplexity: 1.6288\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 16/100\n",
      "  File: 2a8bd69b5c43cf6a.png\n",
      "  LaTeX: C_{3}=G_{2}+G_{1}\\cdot P_{2}+G_{0}\\cdot P_{1}\\cdot P_{2}+C_{0}\\cdot P_{0}\\cdot P_{1}\\cdot P_{2}\n",
      "  Tokens: 65\n",
      "  Loss: 0.8251\n",
      "  Perplexity: 2.2821\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 17/100\n",
      "  File: b1a847cd2357b47b.png\n",
      "  LaTeX: \\frac{t}{b}\n",
      "  Tokens: 7\n",
      "  Loss: 0.0787\n",
      "  Perplexity: 1.0819\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 18/100\n",
      "  File: 276377c46fcb0dc8.png\n",
      "  LaTeX: \\Gamma(\\frac{x}{y})\\rightarrow y\\Gamma(x)\n",
      "  Tokens: 16\n",
      "  Loss: 0.2238\n",
      "  Perplexity: 1.2508\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 19/100\n",
      "  File: 7eea39c8d89645fd.png\n",
      "  LaTeX: \\frac{-0}{|x|}=-0\n",
      "  Tokens: 13\n",
      "  Loss: 0.2024\n",
      "  Perplexity: 1.2243\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 20/100\n",
      "  File: c191c94a7e5fa425.png\n",
      "  LaTeX: r\\le\\mu(F_{r})<\\infty.\n",
      "  Tokens: 13\n",
      "  Loss: 0.2149\n",
      "  Perplexity: 1.2397\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 21/100\n",
      "  File: 638348ce6cf9a9ff.png\n",
      "  LaTeX: \\hat{=}\n",
      "  Tokens: 4\n",
      "  Loss: 0.0561\n",
      "  Perplexity: 1.0577\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 22/100\n",
      "  File: 5a9f917201621c25.png\n",
      "  LaTeX: C_N^{\\mu}\n",
      "  Tokens: 7\n",
      "  Loss: 0.1082\n",
      "  Perplexity: 1.1143\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 23/100\n",
      "  File: eb10a04acd63bef3.png\n",
      "  LaTeX: \\Delta E=B\\mu_{B}\\Delta m_{l}\n",
      "  Tokens: 15\n",
      "  Loss: 0.2366\n",
      "  Perplexity: 1.2669\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 24/100\n",
      "  File: 3098d7c1a4acdb54.png\n",
      "  LaTeX: |\\alpha-\\frac{p}{q}|>\\phi(q)\n",
      "  Tokens: 16\n",
      "  Loss: 0.2825\n",
      "  Perplexity: 1.3265\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 25/100\n",
      "  File: e9c006e585d7a1c7.png\n",
      "  LaTeX: \\frac{\\frac{\\frac{3}{23}}{48}*a}{\\frac{111111333992}{\\frac{444336666611}{67}}}\n",
      "  Tokens: 58\n",
      "  Loss: 0.7552\n",
      "  Perplexity: 2.1280\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 26/100\n",
      "  File: c01aa8332aca1e8e.png\n",
      "  LaTeX: I=\\int Fdt\n",
      "  Tokens: 6\n",
      "  Loss: 0.1250\n",
      "  Perplexity: 1.1331\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 27/100\n",
      "  File: 554d0b86705b4953.png\n",
      "  LaTeX: m_{j}\\subseteq\\{1,...,m\\}\n",
      "  Tokens: 17\n",
      "  Loss: 0.2475\n",
      "  Perplexity: 1.2808\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 28/100\n",
      "  File: 153b83ea1148423e.png\n",
      "  LaTeX: u:\\Omega\\rightarrow\\mathbb{C}\n",
      "  Tokens: 8\n",
      "  Loss: 0.1557\n",
      "  Perplexity: 1.1684\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 29/100\n",
      "  File: 9a5833d0916a8408.png\n",
      "  LaTeX: \\frac{2.138}{10000000000}\n",
      "  Tokens: 21\n",
      "  Loss: 0.2035\n",
      "  Perplexity: 1.2257\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 30/100\n",
      "  File: a2cd378ef6c258e9.png\n",
      "  LaTeX: \\beta=\\partial_{t}F_{t}|_{t=0}.\n",
      "  Tokens: 20\n",
      "  Loss: 0.2863\n",
      "  Perplexity: 1.3315\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 31/100\n",
      "  File: 93d8e67b58be2755.png\n",
      "  LaTeX: GPS_{q}=GPS(q_{1},...,q_{K})=\\frac{K\\cdot\\prod_{k=1}^{K}q_{k}}{\\sum_{k^{\\prime}=1}^{K}\\prod_{k=1,k\\ne k^{\\prime}}q_{k}}\n",
      "  Tokens: 87\n",
      "  Loss: 1.3231\n",
      "  Perplexity: 3.7552\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 32/100\n",
      "  File: b1487a6427de9657.png\n",
      "  LaTeX: O(rad(abc)\\Theta(abc)),\n",
      "  Tokens: 18\n",
      "  Loss: 0.3391\n",
      "  Perplexity: 1.4037\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 33/100\n",
      "  File: 017b1655f985a343.png\n",
      "  LaTeX: [-P^{T}|I_{n-k}]\n",
      "  Tokens: 16\n",
      "  Loss: 0.2989\n",
      "  Perplexity: 1.3484\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 34/100\n",
      "  File: 17dcd65dd46b87dd.png\n",
      "  LaTeX: x\\in\\tildeE^{+}\n",
      "  Tokens: 7\n",
      "  Loss: 0.1159\n",
      "  Perplexity: 1.1229\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 35/100\n",
      "  File: 973c56753803f764.png\n",
      "  LaTeX: (\\sigma^{2},\\nu,\\gamma)\n",
      "  Tokens: 11\n",
      "  Loss: 0.1655\n",
      "  Perplexity: 1.1800\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 36/100\n",
      "  File: bcd0302ec65f057b.png\n",
      "  LaTeX: \\mathbbP^{C}=\\mathbbR^{C}-\\{0\\}/\\mathbbR_{>0}\n",
      "  Tokens: 24\n",
      "  Loss: 0.3776\n",
      "  Perplexity: 1.4588\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 37/100\n",
      "  File: c0d28f955b760bcc.png\n",
      "  LaTeX: Q=\\int_{V}j^{0}dV\n",
      "  Tokens: 14\n",
      "  Loss: 0.2296\n",
      "  Perplexity: 1.2581\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 38/100\n",
      "  File: b32f843d177958b8.png\n",
      "  LaTeX: \\frac{t\\cdot13.774}{30.0177}\n",
      "  Tokens: 20\n",
      "  Loss: 0.2940\n",
      "  Perplexity: 1.3417\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 39/100\n",
      "  File: 808119f97254000a.png\n",
      "  LaTeX: Work=F_{av}\\times d=2Wd\n",
      "  Tokens: 17\n",
      "  Loss: 0.3084\n",
      "  Perplexity: 1.3612\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 40/100\n",
      "  File: 187614fb7d39aa9b.png\n",
      "  LaTeX: f(k_{1};\\lambda)\n",
      "  Tokens: 10\n",
      "  Loss: 0.1346\n",
      "  Perplexity: 1.1441\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 41/100\n",
      "  File: 36e7d543c30397eb.png\n",
      "  LaTeX: \\frac{110}{\\frac{118888887777}{9}\\cdot111111775555}\n",
      "  Tokens: 39\n",
      "  Loss: 0.5551\n",
      "  Perplexity: 1.7421\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 42/100\n",
      "  File: 652fb1bc6e0d0a56.png\n",
      "  LaTeX: \\sum_{j\\in\\mathbb{Z}}exp(-\\frac{\\pi}{cN}\\cdot(n+N\\cdot j)^{2})\n",
      "  Tokens: 36\n",
      "  Loss: 0.5861\n",
      "  Perplexity: 1.7970\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 43/100\n",
      "  File: adef36aa82e2ed59.png\n",
      "  LaTeX: \\Delta m_{body}\n",
      "  Tokens: 9\n",
      "  Loss: 0.1385\n",
      "  Perplexity: 1.1486\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 44/100\n",
      "  File: d306385e9891467f.png\n",
      "  LaTeX: y=m_{1}x+d_{1},y=m_{2}x+d_{2},m_{1}\\ne m_{2}\n",
      "  Tokens: 41\n",
      "  Loss: 0.4827\n",
      "  Perplexity: 1.6204\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 45/100\n",
      "  File: 609b585a17c7ae09.png\n",
      "  LaTeX: W(2,2)\n",
      "  Tokens: 6\n",
      "  Loss: 0.0936\n",
      "  Perplexity: 1.0981\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 46/100\n",
      "  File: 1542a034269a7ce3.png\n",
      "  LaTeX: -i\\frac{d}{dx}\n",
      "  Tokens: 10\n",
      "  Loss: 0.1279\n",
      "  Perplexity: 1.1365\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 47/100\n",
      "  File: 61be7bdcace662a2.png\n",
      "  LaTeX: |x\\cdot y|=|x|\\cdot|y|\n",
      "  Tokens: 13\n",
      "  Loss: 0.2915\n",
      "  Perplexity: 1.3385\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 48/100\n",
      "  File: daa5bf5cda41b8ed.png\n",
      "  LaTeX: \\frac{B_{21}}{B_{12}}=\\frac{g_{1}}{g_{2}}.\n",
      "  Tokens: 34\n",
      "  Loss: 0.3603\n",
      "  Perplexity: 1.4338\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 49/100\n",
      "  File: 329b76ecc3fba66e.png\n",
      "  LaTeX: \\gamma_{j,k}=e^{i(\\alpha_{j}+\\beta_{k})}\\int_{\\Omega_{2}}(T1_{A_{j}})1_{B_{k}}d\\mu_{2}\n",
      "  Tokens: 62\n",
      "  Loss: 0.8099\n",
      "  Perplexity: 2.2476\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 50/100\n",
      "  File: 03b0c850fd4d49ad.png\n",
      "  LaTeX: \\overline{x}(t)\n",
      "  Tokens: 7\n",
      "  Loss: 0.0853\n",
      "  Perplexity: 1.0890\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 51/100\n",
      "  File: fd488c33f5ed671a.png\n",
      "  LaTeX: \\beta_{n}=O(\\frac{1}{n})\n",
      "  Tokens: 16\n",
      "  Loss: 0.1974\n",
      "  Perplexity: 1.2182\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 52/100\n",
      "  File: f0609514cde610aa.png\n",
      "  LaTeX: {(\\beta^{\\chi})}^{\\eta}\n",
      "  Tokens: 13\n",
      "  Loss: 0.1867\n",
      "  Perplexity: 1.2053\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 53/100\n",
      "  File: bf4a1efa2d61ca50.png\n",
      "  LaTeX: i_{X}:\\Lambda^{k+1}(M)\\rightarrow\\Lambda^{k}(M)\n",
      "  Tokens: 25\n",
      "  Loss: 0.4030\n",
      "  Perplexity: 1.4963\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 54/100\n",
      "  File: 573be84ee0929393.png\n",
      "  LaTeX: S(0)=x,\n",
      "  Tokens: 7\n",
      "  Loss: 0.1052\n",
      "  Perplexity: 1.1110\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 55/100\n",
      "  File: 8e70d3a0a7a20e33.png\n",
      "  LaTeX: c>\\aleph_{0}\n",
      "  Tokens: 7\n",
      "  Loss: 0.1064\n",
      "  Perplexity: 1.1122\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 56/100\n",
      "  File: 51910edfaba495f8.png\n",
      "  LaTeX: R_{CF}\\approx R_{P}\n",
      "  Tokens: 12\n",
      "  Loss: 0.1952\n",
      "  Perplexity: 1.2155\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 57/100\n",
      "  File: ef5b8cca7ec6038d.png\n",
      "  LaTeX: h=\\frac{P_{a}-P_{o}}{g\\rho}\n",
      "  Tokens: 20\n",
      "  Loss: 0.2903\n",
      "  Perplexity: 1.3368\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 58/100\n",
      "  File: 3f473170a3e16352.png\n",
      "  LaTeX: \\upsilon_{c}=\\frac{8}{\\sqrt{L_{\\frac{8}{0}}C_{\\frac{8}{0}}}}\n",
      "  Tokens: 37\n",
      "  Loss: 0.4353\n",
      "  Perplexity: 1.5454\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 59/100\n",
      "  File: 3aed5d827fe96d19.png\n",
      "  LaTeX: s_{n}=z_{n}+k_{1}(n)r_1^{n}+k_{2}(n)r_2^{n}+\\cdot\\cdot\\cdot+k_{e}(n)r_e^{n},\n",
      "  Tokens: 64\n",
      "  Loss: 0.9702\n",
      "  Perplexity: 2.6385\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 60/100\n",
      "  File: bf54de3c8656b074.png\n",
      "  LaTeX: \\frac{dA}{dt}\n",
      "  Tokens: 9\n",
      "  Loss: 0.1209\n",
      "  Perplexity: 1.1285\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 61/100\n",
      "  File: 50834e349cc9ff51.png\n",
      "  LaTeX: \\frac{\\partial y}{\\partial x}\n",
      "  Tokens: 9\n",
      "  Loss: 0.1118\n",
      "  Perplexity: 1.1183\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 62/100\n",
      "  File: d726d0f1faf763c3.png\n",
      "  LaTeX: \\frac{\\eta(-1/z)}{\\sqrt{z/i}}\n",
      "  Tokens: 18\n",
      "  Loss: 0.2776\n",
      "  Perplexity: 1.3200\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 63/100\n",
      "  File: b43267a669e811a4.png\n",
      "  LaTeX: \\mathbb{C}[[t]][t^{-1}]=\\mathbb{C}((t))\n",
      "  Tokens: 27\n",
      "  Loss: 0.4873\n",
      "  Perplexity: 1.6279\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 64/100\n",
      "  File: 6fe8d1cd33b3ae18.png\n",
      "  LaTeX: \\Omega_{0}=\\frac{\\rho}{\\rho_{crit}},\n",
      "  Tokens: 21\n",
      "  Loss: 0.2881\n",
      "  Perplexity: 1.3339\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 65/100\n",
      "  File: 5c21473ced1e7a66.png\n",
      "  LaTeX: \\frac{5.0819556}{0}\n",
      "  Tokens: 15\n",
      "  Loss: 0.2061\n",
      "  Perplexity: 1.2289\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 66/100\n",
      "  File: 49ca2c8a518686c0.png\n",
      "  LaTeX: (i2\\pi\\xi)^{n}\n",
      "  Tokens: 10\n",
      "  Loss: 0.1757\n",
      "  Perplexity: 1.1921\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 67/100\n",
      "  File: 41c6736570400100.png\n",
      "  LaTeX: d_K^{-p/2}\n",
      "  Tokens: 10\n",
      "  Loss: 0.1632\n",
      "  Perplexity: 1.1772\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 68/100\n",
      "  File: 8e37bf1ba7228877.png\n",
      "  LaTeX: (c:a:a)\\cdot m:4_{2}\\odot\\tilde{a}\n",
      "  Tokens: 20\n",
      "  Loss: 0.3561\n",
      "  Perplexity: 1.4278\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 69/100\n",
      "  File: fcacba28ec5ed956.png\n",
      "  LaTeX: \\int dx\\psi_n^{*}(s)\n",
      "  Tokens: 13\n",
      "  Loss: 0.2185\n",
      "  Perplexity: 1.2442\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 70/100\n",
      "  File: 4da6a0b774d92996.png\n",
      "  LaTeX: \\Omega_u^{\\phi}\n",
      "  Tokens: 7\n",
      "  Loss: 0.1226\n",
      "  Perplexity: 1.1304\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 71/100\n",
      "  File: 7d3986665243fc8a.png\n",
      "  LaTeX: \\{x\\}\\times\\mathbbR^{n}\n",
      "  Tokens: 11\n",
      "  Loss: 0.1854\n",
      "  Perplexity: 1.2037\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 72/100\n",
      "  File: 2a7da749efc3264f.png\n",
      "  LaTeX: w=\\sqrt{1-\\frac{U}{E}}\n",
      "  Tokens: 14\n",
      "  Loss: 0.1922\n",
      "  Perplexity: 1.2119\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 73/100\n",
      "  File: 8475733c722d4332.png\n",
      "  LaTeX: \\overline{E_{0}}\n",
      "  Tokens: 8\n",
      "  Loss: 0.0899\n",
      "  Perplexity: 1.0941\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 74/100\n",
      "  File: 22764aaf20863cf9.png\n",
      "  LaTeX: \\hat{U}\n",
      "  Tokens: 4\n",
      "  Loss: 0.0604\n",
      "  Perplexity: 1.0622\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 75/100\n",
      "  File: a0d7a7141fdd3535.png\n",
      "  LaTeX: 2W\\le M\\le N\n",
      "  Tokens: 6\n",
      "  Loss: 0.1534\n",
      "  Perplexity: 1.1657\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 76/100\n",
      "  File: cd700d6d744064cd.png\n",
      "  LaTeX: ab^{2}c\n",
      "  Tokens: 7\n",
      "  Loss: 0.1085\n",
      "  Perplexity: 1.1146\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 77/100\n",
      "  File: 2a2f8e208e58715f.png\n",
      "  LaTeX: |\\frac{1}{2},\\mp\\frac{1}{2}\\rangle\n",
      "  Tokens: 18\n",
      "  Loss: 0.2007\n",
      "  Perplexity: 1.2222\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 78/100\n",
      "  File: 37f9932a30a9f714.png\n",
      "  LaTeX: T(x)=2xmod1\n",
      "  Tokens: 11\n",
      "  Loss: 0.1732\n",
      "  Perplexity: 1.1891\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 79/100\n",
      "  File: 6545965610daa407.png\n",
      "  LaTeX: v_{b}=\\frac{8}{\\frac{8}{v_{b_{0}}}+\\frac{ef}{m}}\n",
      "  Tokens: 36\n",
      "  Loss: 0.4254\n",
      "  Perplexity: 1.5302\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 80/100\n",
      "  File: 9c87ff5eaa80e0e0.png\n",
      "  LaTeX: \\frac{dS_{t}}{S_{t}}\n",
      "  Tokens: 16\n",
      "  Loss: 0.1874\n",
      "  Perplexity: 1.2061\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 81/100\n",
      "  File: 7f5ba0a5b752298a.png\n",
      "  LaTeX: Y=u(\\vec{X})\n",
      "  Tokens: 9\n",
      "  Loss: 0.1452\n",
      "  Perplexity: 1.1563\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 82/100\n",
      "  File: a591bbdfe77d59f7.png\n",
      "  LaTeX: \\gamma=lim_{a\\rightarrow1}(\\zeta(a)-\\frac{1}{a-1})\n",
      "  Tokens: 27\n",
      "  Loss: 0.3996\n",
      "  Perplexity: 1.4912\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 83/100\n",
      "  File: dcd2e89f9b370238.png\n",
      "  LaTeX: \\gamma/\\alpha.\n",
      "  Tokens: 4\n",
      "  Loss: 0.0740\n",
      "  Perplexity: 1.0768\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 84/100\n",
      "  File: 8a4aaa63197ac019.png\n",
      "  LaTeX: \\frac{\\partial r_{i}}{\\partial\\beta_{j}}\n",
      "  Tokens: 17\n",
      "  Loss: 0.2035\n",
      "  Perplexity: 1.2257\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 85/100\n",
      "  File: 91dd93a9402961e9.png\n",
      "  LaTeX: ((M,s)\\models AX\\phi)\\Leftrightarrow(\\forall\\langle s\\rightarrow s_{1}\\rangle((M,s_{1})\\models\\phi))\n",
      "  Tokens: 37\n",
      "  Loss: 0.6426\n",
      "  Perplexity: 1.9015\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 86/100\n",
      "  File: 5895f3a155a4c827.png\n",
      "  LaTeX: N_{i,n}(u)\n",
      "  Tokens: 10\n",
      "  Loss: 0.1679\n",
      "  Perplexity: 1.1828\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 87/100\n",
      "  File: a825dfe2466d0e9f.png\n",
      "  LaTeX: U_{k}(\\beta)-U_{k-2}(\\beta)=T_{k}(\\beta).\n",
      "  Tokens: 29\n",
      "  Loss: 0.3997\n",
      "  Perplexity: 1.4914\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 88/100\n",
      "  File: a53e46286be7c7b0.png\n",
      "  LaTeX: h_{ii}\\ge h_{ii}^{2}\\Rightarrow0\\le h_{ii}\\le1\n",
      "  Tokens: 28\n",
      "  Loss: 0.4595\n",
      "  Perplexity: 1.5833\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 89/100\n",
      "  File: 7c46c2354df9c72a.png\n",
      "  LaTeX: J-1\n",
      "  Tokens: 3\n",
      "  Loss: 0.0484\n",
      "  Perplexity: 1.0496\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 90/100\n",
      "  File: 0cd43059b92833d4.png\n",
      "  LaTeX: A=\\frac{1}{2}r^{2}\\theta.\n",
      "  Tokens: 16\n",
      "  Loss: 0.1819\n",
      "  Perplexity: 1.1995\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 91/100\n",
      "  File: 0352be6faca6a030.png\n",
      "  LaTeX: \\frac{100000000000}{69.1526}\n",
      "  Tokens: 24\n",
      "  Loss: 0.2435\n",
      "  Perplexity: 1.2757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 92/100\n",
      "  File: 5ebe600e118d046b.png\n",
      "  LaTeX: \\{\\nu_{1}(x)\\}\\sim\\{\\nu_{1}(y)\\}\n",
      "  Tokens: 25\n",
      "  Loss: 0.3225\n",
      "  Perplexity: 1.3806\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 93/100\n",
      "  File: 33a35fc0044c3c53.png\n",
      "  LaTeX: \\omega=90^{\\circ}\n",
      "  Tokens: 8\n",
      "  Loss: 0.1219\n",
      "  Perplexity: 1.1296\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 94/100\n",
      "  File: efd301eb630e129d.png\n",
      "  LaTeX: i^{-s}Li_{s}(e^{2\\pi ix})+i^{s}Li_{s}(e^{-2\\pi ix})=\\frac{(2\\pi)^{s}}{\\Gamma(s)}\\zeta(1-s,x),\n",
      "  Tokens: 72\n",
      "  Loss: 1.1277\n",
      "  Perplexity: 3.0884\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 95/100\n",
      "  File: b1df8ab07e7b98d5.png\n",
      "  LaTeX: \\frac{c^{\\prime}}{\\sqrt{1-\\frac{c^{6}}{9}}}\\ge1\n",
      "  Tokens: 28\n",
      "  Loss: 0.3613\n",
      "  Perplexity: 1.4352\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 96/100\n",
      "  File: 6a2c643271c0f7ef.png\n",
      "  LaTeX: (\\frac{1^{10}}{9})^{\\frac{74^{\\sqrt{6}}}{6}}\n",
      "  Tokens: 32\n",
      "  Loss: 0.3826\n",
      "  Perplexity: 1.4662\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 97/100\n",
      "  File: 5e37001823ea0aaf.png\n",
      "  LaTeX: H\\cdot t^{i}\n",
      "  Tokens: 7\n",
      "  Loss: 0.1072\n",
      "  Perplexity: 1.1132\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 98/100\n",
      "  File: d0070ee72da60d82.png\n",
      "  LaTeX: 4x+5y=32\n",
      "  Tokens: 8\n",
      "  Loss: 0.1506\n",
      "  Perplexity: 1.1625\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 99/100\n",
      "  File: 5eb9590efd141eb2.png\n",
      "  LaTeX: P(r)\n",
      "  Tokens: 4\n",
      "  Loss: 0.0543\n",
      "  Perplexity: 1.0558\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 100/100\n",
      "  File: 0d39c59d66bb3a73.png\n",
      "  LaTeX: Var(S_{t})=S_0^{2}e^{2\\mu t}(e^{\\sigma^{2}t}-1).\n",
      "  Tokens: 40\n",
      "  Loss: 0.5539\n",
      "  Perplexity: 1.7400\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "TEST DATASET SUMMARY\n",
      "====================================================================================================\n",
      "Total samples: 100\n",
      "Average loss: 0.4722\n",
      "Overall perplexity: 1.6035\n",
      "Total tokens: 1995.0\n",
      "\n",
      "Perplexity Statistics:\n",
      "  Min: 1.0496\n",
      "  Max: 3.7552\n",
      "  Mean: 1.3796\n",
      "  Median: 1.2397\n",
      "\n",
      "Detailed results saved to: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test_evaluation_results.csv\n",
      "\n",
      "====================================================================================================\n",
      "TOP 10 BEST PREDICTIONS (Lowest Perplexity)\n",
      "====================================================================================================\n",
      "\n",
      "Perplexity: 1.0496\n",
      "LaTeX: J-1\n",
      "File: 7c46c2354df9c72a.png\n",
      "\n",
      "Perplexity: 1.0558\n",
      "LaTeX: P(r)\n",
      "File: 5eb9590efd141eb2.png\n",
      "\n",
      "Perplexity: 1.0577\n",
      "LaTeX: \\hat{=}\n",
      "File: 638348ce6cf9a9ff.png\n",
      "\n",
      "Perplexity: 1.0622\n",
      "LaTeX: \\hat{U}\n",
      "File: 22764aaf20863cf9.png\n",
      "\n",
      "Perplexity: 1.0630\n",
      "LaTeX: \\frac{1}{5}\n",
      "File: 5d6263f93a735248.png\n",
      "\n",
      "Perplexity: 1.0687\n",
      "LaTeX: \\alpha\\vee\\rho\n",
      "File: c952c04b266bf300.png\n",
      "\n",
      "Perplexity: 1.0768\n",
      "LaTeX: \\gamma/\\alpha.\n",
      "File: dcd2e89f9b370238.png\n",
      "\n",
      "Perplexity: 1.0819\n",
      "LaTeX: \\frac{t}{b}\n",
      "File: b1a847cd2357b47b.png\n",
      "\n",
      "Perplexity: 1.0890\n",
      "LaTeX: \\overline{x}(t)\n",
      "File: 03b0c850fd4d49ad.png\n",
      "\n",
      "Perplexity: 1.0941\n",
      "LaTeX: \\overline{E_{0}}\n",
      "File: 8475733c722d4332.png\n",
      "\n",
      "====================================================================================================\n",
      "TOP 10 WORST PREDICTIONS (Highest Perplexity)\n",
      "====================================================================================================\n",
      "\n",
      "Perplexity: 1.7567\n",
      "LaTeX: V=\\frac{1}{3}Ah\\rightarrow3V=Ah\\rightarrow Ah=3V\\rightarrow h=3\\frac{V}{A}\n",
      "File: c96c37e8df988c92.png\n",
      "\n",
      "Perplexity: 1.7970\n",
      "LaTeX: \\sum_{j\\in\\mathbb{Z}}exp(-\\frac{\\pi}{cN}\\cdot(n+N\\cdot j)^{2})\n",
      "File: 652fb1bc6e0d0a56.png\n",
      "\n",
      "Perplexity: 1.9015\n",
      "LaTeX: ((M,s)\\models AX\\phi)\\Leftrightarrow(\\forall\\langle s\\rightarrow s_{1}\\rangle((M,s_{1})\\models\\phi))\n",
      "File: 91dd93a9402961e9.png\n",
      "\n",
      "Perplexity: 2.1280\n",
      "LaTeX: \\frac{\\frac{\\frac{3}{23}}{48}*a}{\\frac{111111333992}{\\frac{444336666611}{67}}}\n",
      "File: e9c006e585d7a1c7.png\n",
      "\n",
      "Perplexity: 2.2476\n",
      "LaTeX: \\gamma_{j,k}=e^{i(\\alpha_{j}+\\beta_{k})}\\int_{\\Omega_{2}}(T1_{A_{j}})1_{B_{k}}d\\mu_{2}\n",
      "File: 329b76ecc3fba66e.png\n",
      "\n",
      "Perplexity: 2.2821\n",
      "LaTeX: C_{3}=G_{2}+G_{1}\\cdot P_{2}+G_{0}\\cdot P_{1}\\cdot P_{2}+C_{0}\\cdot P_{0}\\cdot P_{1}\\cdot P_{2}\n",
      "File: 2a8bd69b5c43cf6a.png\n",
      "\n",
      "Perplexity: 2.4111\n",
      "LaTeX: dX=\\frac{\\partial X}{\\partial x}dx=F^{-1}dx=HdxordX_{M}=\\frac{\\partial X_{M}}{\\partial x_{n}}dx_{n}\n",
      "File: e8a00f63b814fdd6.png\n",
      "\n",
      "Perplexity: 2.6385\n",
      "LaTeX: s_{n}=z_{n}+k_{1}(n)r_1^{n}+k_{2}(n)r_2^{n}+\\cdot\\cdot\\cdot+k_{e}(n)r_e^{n},\n",
      "File: 3aed5d827fe96d19.png\n",
      "\n",
      "Perplexity: 3.0884\n",
      "LaTeX: i^{-s}Li_{s}(e^{2\\pi ix})+i^{s}Li_{s}(e^{-2\\pi ix})=\\frac{(2\\pi)^{s}}{\\Gamma(s)}\\zeta(1-s,x),\n",
      "File: efd301eb630e129d.png\n",
      "\n",
      "Perplexity: 3.7552\n",
      "LaTeX: GPS_{q}=GPS(q_{1},...,q_{K})=\\frac{K\\cdot\\prod_{k=1}^{K}q_{k}}{\\sum_{k^{\\prime}=1}^{K}\\prod_{k=1,k\\ne k^{\\prime}}q_{k}}\n",
      "File: 93d8e67b58be2755.png\n",
      "\n",
      "\n",
      "\n",
      "Starting Validation Dataset Evaluation...\n",
      "\n",
      "====================================================================================================\n",
      "DETAILED VAL DATASET EVALUATION\n",
      "====================================================================================================\n",
      "\n",
      "Sample 1/100\n",
      "  File: 5864eff9d2def194.png\n",
      "  LaTeX: f_{\\omega+1}(f_{\\omega}(3))-2\n",
      "  Tokens: 19\n",
      "  Loss: 0.2577\n",
      "  Perplexity: 1.2939\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 2/100\n",
      "  File: 5d83e2eb369cf7c8.png\n",
      "  LaTeX: s\\notin\\alpha,t\\notin\\gamma\n",
      "  Tokens: 7\n",
      "  Loss: 0.1484\n",
      "  Perplexity: 1.1600\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 3/100\n",
      "  File: c3da2e0d8d2a7986.png\n",
      "  LaTeX: \\int f(x)dx\n",
      "  Tokens: 7\n",
      "  Loss: 0.1112\n",
      "  Perplexity: 1.1176\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 4/100\n",
      "  File: 7831867da9afb7bc.png\n",
      "  LaTeX: \\hat{y}(f)\n",
      "  Tokens: 7\n",
      "  Loss: 0.0962\n",
      "  Perplexity: 1.1010\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 5/100\n",
      "  File: bddccbd312c7aca3.png\n",
      "  LaTeX: g(n)=\\prod_{k=1}^{n}f(k)\n",
      "  Tokens: 20\n",
      "  Loss: 0.3010\n",
      "  Perplexity: 1.3512\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 6/100\n",
      "  File: 463ffc2a3cb2d667.png\n",
      "  LaTeX: PPI(monitor)=\\frac{NumberofPixels}{SizeinInches}=\\frac{1920}{20}=96ppi\n",
      "  Tokens: 62\n",
      "  Loss: 1.1585\n",
      "  Perplexity: 3.1852\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 7/100\n",
      "  File: 4857166d1211fcdb.png\n",
      "  LaTeX: [1-\\lambda R(\\underline{k},\\omega)]^{n}u(n)\n",
      "  Tokens: 22\n",
      "  Loss: 0.3911\n",
      "  Perplexity: 1.4786\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 8/100\n",
      "  File: c9c57dfb019d6026.png\n",
      "  LaTeX: \\frac{1372}{3}\\pi\n",
      "  Tokens: 11\n",
      "  Loss: 0.1412\n",
      "  Perplexity: 1.1517\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 9/100\n",
      "  File: ad03a481dc24af59.png\n",
      "  LaTeX: \\{e_{\\mu}\\}\n",
      "  Tokens: 9\n",
      "  Loss: 0.1098\n",
      "  Perplexity: 1.1161\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 10/100\n",
      "  File: 09d3a17570c20e15.png\n",
      "  LaTeX: \\langle(\\delta u(r))^{n}\\rangle=C_{n}\\langle(\\epsilon r)^{\\frac{n}{3}}\\rangle,\n",
      "  Tokens: 36\n",
      "  Loss: 0.5833\n",
      "  Perplexity: 1.7919\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 11/100\n",
      "  File: 456f96a1b217e0f3.png\n",
      "  LaTeX: 128cos\\frac{\\pi}{29}cos\\frac{4\\pi}{29}cos\\frac{5\\pi}{29}cos\\frac{6\\pi}{29}cos\\frac{7\\pi}{29}cos\\frac{9\\pi}{29}cos\\frac{13\\pi}{29}\n",
      "  Tokens: 87\n",
      "  Loss: 1.3862\n",
      "  Perplexity: 3.9996\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 12/100\n",
      "  File: 4677b76acec23465.png\n",
      "  LaTeX: \\frac{37.13006145}{14.630468}\n",
      "  Tokens: 25\n",
      "  Loss: 0.3478\n",
      "  Perplexity: 1.4159\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 13/100\n",
      "  File: 2ead5d17759653c7.png\n",
      "  LaTeX: A(u)\n",
      "  Tokens: 4\n",
      "  Loss: 0.0708\n",
      "  Perplexity: 1.0734\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 14/100\n",
      "  File: 0ae709dc69df8399.png\n",
      "  LaTeX: R(i,a)\n",
      "  Tokens: 6\n",
      "  Loss: 0.1029\n",
      "  Perplexity: 1.1084\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 15/100\n",
      "  File: ec891dbd997afbdc.png\n",
      "  LaTeX: M_{3}\n",
      "  Tokens: 5\n",
      "  Loss: 0.0520\n",
      "  Perplexity: 1.0533\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 16/100\n",
      "  File: bb7bb8c8097285ea.png\n",
      "  LaTeX: \\lambda_{1}/\\lambda_{2}=0.15,\n",
      "  Tokens: 17\n",
      "  Loss: 0.2030\n",
      "  Perplexity: 1.2251\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 17/100\n",
      "  File: b27ed2625259ea08.png\n",
      "  LaTeX: d\\approx\\sqrt{\\frac{3}{8}l_{1}(l_{2}-l_{1})}\n",
      "  Tokens: 30\n",
      "  Loss: 0.3655\n",
      "  Perplexity: 1.4412\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 18/100\n",
      "  File: 89ab0edef6a6cbf3.png\n",
      "  LaTeX: V(r)\\sim\\sigma r\n",
      "  Tokens: 7\n",
      "  Loss: 0.1116\n",
      "  Perplexity: 1.1181\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 19/100\n",
      "  File: ea75c59034fa0130.png\n",
      "  LaTeX: \\frac{1}{T}\\frac{d^{2}T}{dt^{2}}=-k^{2}\n",
      "  Tokens: 31\n",
      "  Loss: 0.3632\n",
      "  Perplexity: 1.4379\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 20/100\n",
      "  File: 1517a95764083a78.png\n",
      "  LaTeX: \\Delta\\langle2\\rangle\n",
      "  Tokens: 4\n",
      "  Loss: 0.0881\n",
      "  Perplexity: 1.0921\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 21/100\n",
      "  File: 60bcbdb972965e63.png\n",
      "  LaTeX: \\tilde{O}(\\theta dlog\\frac{1}{d})\n",
      "  Tokens: 18\n",
      "  Loss: 0.2776\n",
      "  Perplexity: 1.3200\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 22/100\n",
      "  File: 713389699c2621cd.png\n",
      "  LaTeX: I_{b}b\n",
      "  Tokens: 6\n",
      "  Loss: 0.0902\n",
      "  Perplexity: 1.0944\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 23/100\n",
      "  File: 11a36cea4d76f4c7.png\n",
      "  LaTeX: f(a_{0},...,a_{n-1},s)=0\\leftrightarrow\\forall i<n(\\beta(s,i)=a_{i})\n",
      "  Tokens: 43\n",
      "  Loss: 0.6762\n",
      "  Perplexity: 1.9663\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 24/100\n",
      "  File: 636f2de3a9cb2614.png\n",
      "  LaTeX: G_{6}(\\vec{r})\\propto e^{-r/\\xi_{6}}\n",
      "  Tokens: 24\n",
      "  Loss: 0.3406\n",
      "  Perplexity: 1.4058\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 25/100\n",
      "  File: 45f47244f6648322.png\n",
      "  LaTeX: \\frac{\\frac{444443366666}{x}}{35}\n",
      "  Tokens: 25\n",
      "  Loss: 0.3273\n",
      "  Perplexity: 1.3873\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 26/100\n",
      "  File: a81edb7a18da75b9.png\n",
      "  LaTeX: U_{n+1}\\equiv0(modn).\n",
      "  Tokens: 16\n",
      "  Loss: 0.2970\n",
      "  Perplexity: 1.3458\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 27/100\n",
      "  File: aec83b64b1ce28bc.png\n",
      "  LaTeX: \\frac{19.53299}{881112222288}\n",
      "  Tokens: 25\n",
      "  Loss: 0.3577\n",
      "  Perplexity: 1.4300\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 28/100\n",
      "  File: 0397af8eed76b603.png\n",
      "  LaTeX: \\int_{E}f(x)\\mu(dx)\n",
      "  Tokens: 14\n",
      "  Loss: 0.2000\n",
      "  Perplexity: 1.2214\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 29/100\n",
      "  File: 56fc9b1d30cbe6a9.png\n",
      "  LaTeX: (\\frac{\\partial x}{\\partial r})^{2}=cos^{2}\\phi\n",
      "  Tokens: 24\n",
      "  Loss: 0.3261\n",
      "  Perplexity: 1.3855\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 30/100\n",
      "  File: 21b4df2b6c3482d0.png\n",
      "  LaTeX: a_{30}=30heqat\n",
      "  Tokens: 14\n",
      "  Loss: 0.2434\n",
      "  Perplexity: 1.2756\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 31/100\n",
      "  File: 28fd080753cf2be6.png\n",
      "  LaTeX: 0=F(t,x,\\dot{x})\n",
      "  Tokens: 13\n",
      "  Loss: 0.1986\n",
      "  Perplexity: 1.2197\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 32/100\n",
      "  File: bc1da8b86aa95119.png\n",
      "  LaTeX: (\\frac{8}{10})^{7}-2^{6}\n",
      "  Tokens: 20\n",
      "  Loss: 0.2379\n",
      "  Perplexity: 1.2686\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 33/100\n",
      "  File: b8d78ba3b203183a.png\n",
      "  LaTeX: \\Phi_{e,\\nu}^{i}\n",
      "  Tokens: 11\n",
      "  Loss: 0.1645\n",
      "  Perplexity: 1.1788\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 34/100\n",
      "  File: 1995c29d6d7fa81a.png\n",
      "  LaTeX: \\Sigma_{YY}^{1/2}=V_{Y}D_Y^{1/2}V_Y^{\\top},V_{Y}D_{Y}V_Y^{\\top}=\\Sigma_{YY}.\n",
      "  Tokens: 60\n",
      "  Loss: 0.9913\n",
      "  Perplexity: 2.6947\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 35/100\n",
      "  File: 0b278cf862e90215.png\n",
      "  LaTeX: [a]=[a]_{1}\\cdot[a]_{2}\n",
      "  Tokens: 19\n",
      "  Loss: 0.3089\n",
      "  Perplexity: 1.3619\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 36/100\n",
      "  File: f1429a78812c08a3.png\n",
      "  LaTeX: \\gamma=\\frac{\\omega F}{\\sqrt{2E_{i}}}\n",
      "  Tokens: 18\n",
      "  Loss: 0.2396\n",
      "  Perplexity: 1.2707\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 37/100\n",
      "  File: 06507e7ba0a60629.png\n",
      "  LaTeX: 0.0000012\\%\n",
      "  Tokens: 11\n",
      "  Loss: 0.1442\n",
      "  Perplexity: 1.1551\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 38/100\n",
      "  File: fab7f6c81eda2c0d.png\n",
      "  LaTeX: \\theta_{1}=\\frac{\\mu}{\\sigma^{2}}\n",
      "  Tokens: 17\n",
      "  Loss: 0.1757\n",
      "  Perplexity: 1.1921\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 39/100\n",
      "  File: 065db3a5a1e4ed03.png\n",
      "  LaTeX: \\hatf_{k}(t)\n",
      "  Tokens: 8\n",
      "  Loss: 0.0952\n",
      "  Perplexity: 1.0999\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 40/100\n",
      "  File: 64d9386defa685f1.png\n",
      "  LaTeX: f\n",
      "  Tokens: 1\n",
      "  Loss: 0.0175\n",
      "  Perplexity: 1.0177\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 41/100\n",
      "  File: 02b82014174f683b.png\n",
      "  LaTeX: F=\\frac{\\partial F_{i}}{\\partial x_{j}}(x_{0})\n",
      "  Tokens: 26\n",
      "  Loss: 0.3055\n",
      "  Perplexity: 1.3573\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 42/100\n",
      "  File: 16ce99adcf654a58.png\n",
      "  LaTeX: X_{0},X_{1},X_{2},...,X_{m}\n",
      "  Tokens: 27\n",
      "  Loss: 0.3204\n",
      "  Perplexity: 1.3777\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 43/100\n",
      "  File: 6c1fee59934b6686.png\n",
      "  LaTeX: \\frac{\\partial lnL(\\alpha,\\beta,a,c|Y)}{\\partial\\alpha}=\\sum_{i=1}^{N}ln(Y_{i}-a)-N(-\\psi(\\alpha+\\beta)+\\psi(\\alpha))-Nln(c-a)=0\n",
      "  Tokens: 72\n",
      "  Loss: 1.3129\n",
      "  Perplexity: 3.7168\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 44/100\n",
      "  File: a4d73f7620faa203.png\n",
      "  LaTeX: \\frac{\\frac{555555666661}{111999444442}*76}{\\frac{92.870346505}{c+\\frac{17.04976473}{1000000}}}\n",
      "  Tokens: 79\n",
      "  Loss: 1.0901\n",
      "  Perplexity: 2.9745\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 45/100\n",
      "  File: 5e26108ab1712406.png\n",
      "  LaTeX: X:=\\prod_{i\\in I}X_{i}\n",
      "  Tokens: 15\n",
      "  Loss: 0.2353\n",
      "  Perplexity: 1.2653\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 46/100\n",
      "  File: 9af7b420632583af.png\n",
      "  LaTeX: (Sch/T)_{fppf}\\times_{t,Y}X_{T}\n",
      "  Tokens: 26\n",
      "  Loss: 0.4430\n",
      "  Perplexity: 1.5574\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 47/100\n",
      "  File: 72882fc186ab0ade.png\n",
      "  LaTeX: k_{2}/K_{M}\n",
      "  Tokens: 11\n",
      "  Loss: 0.1344\n",
      "  Perplexity: 1.1438\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 48/100\n",
      "  File: 68e1c1867e294c42.png\n",
      "  LaTeX: H_{0}:Y_{1}=Y_{2}=0\n",
      "  Tokens: 19\n",
      "  Loss: 0.2252\n",
      "  Perplexity: 1.2525\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 49/100\n",
      "  File: 85209c3cc1238f91.png\n",
      "  LaTeX: \\frac{666666888883}{333311111111}\n",
      "  Tokens: 29\n",
      "  Loss: 0.3891\n",
      "  Perplexity: 1.4757\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 50/100\n",
      "  File: 58662549e29146e2.png\n",
      "  LaTeX: |_{corr}\n",
      "  Tokens: 8\n",
      "  Loss: 0.1150\n",
      "  Perplexity: 1.1219\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 51/100\n",
      "  File: 9e5ca5e96e5e5f7f.png\n",
      "  LaTeX: \\frac{10000}{22}\n",
      "  Tokens: 12\n",
      "  Loss: 0.1036\n",
      "  Perplexity: 1.1092\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 52/100\n",
      "  File: 638404356cb86a15.png\n",
      "  LaTeX: (\\frac{5+311}{\\sqrt{255}}\\cdot\\frac{2}{6}\\cdot445)\n",
      "  Tokens: 30\n",
      "  Loss: 0.4006\n",
      "  Perplexity: 1.4927\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 53/100\n",
      "  File: 6b1a49c3bea431f9.png\n",
      "  LaTeX: U_{\\alpha}\\times G\n",
      "  Tokens: 7\n",
      "  Loss: 0.1111\n",
      "  Perplexity: 1.1175\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 54/100\n",
      "  File: 8823d6b5980abd6f.png\n",
      "  LaTeX: n=341=11\\cdot31\n",
      "  Tokens: 11\n",
      "  Loss: 0.1773\n",
      "  Perplexity: 1.1940\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 55/100\n",
      "  File: 4ebe81536c73a110.png\n",
      "  LaTeX: F(x)=\\frac{1}{2x}\n",
      "  Tokens: 13\n",
      "  Loss: 0.1298\n",
      "  Perplexity: 1.1386\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 56/100\n",
      "  File: 34676f71940f44ee.png\n",
      "  LaTeX: R=\\sum_{i=1toN}p_{i}x_{i}\n",
      "  Tokens: 22\n",
      "  Loss: 0.3274\n",
      "  Perplexity: 1.3874\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 57/100\n",
      "  File: b622a8a92f7b114d.png\n",
      "  LaTeX: w+hg+l\n",
      "  Tokens: 6\n",
      "  Loss: 0.1310\n",
      "  Perplexity: 1.1400\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 58/100\n",
      "  File: 66bde127b748940a.png\n",
      "  LaTeX: D=\\frac{1}{2}|\\hat{r}-\\hat{p}|_{1}\n",
      "  Tokens: 24\n",
      "  Loss: 0.3267\n",
      "  Perplexity: 1.3863\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 59/100\n",
      "  File: 48e9e680f2bcf21d.png\n",
      "  LaTeX: \\hat{H}(\\lambda)|\\psi_{n}(\\lambda)\\rangle=E_{n}(\\lambda)|\\psi_{n}(\\lambda)\\rangle\n",
      "  Tokens: 36\n",
      "  Loss: 0.5950\n",
      "  Perplexity: 1.8130\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 60/100\n",
      "  File: cb98009c621f6874.png\n",
      "  LaTeX: 7^{7^{\\cdot^{\\cdot^{\\cdot^{u}}}}}\n",
      "  Tokens: 21\n",
      "  Loss: 0.3042\n",
      "  Perplexity: 1.3556\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 61/100\n",
      "  File: 6fafa4781653a5a3.png\n",
      "  LaTeX: w=1-\\frac{2}{i\\pi}Log(z)\n",
      "  Tokens: 18\n",
      "  Loss: 0.2818\n",
      "  Perplexity: 1.3255\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 62/100\n",
      "  File: 6fd3a9adb7f928d4.png\n",
      "  LaTeX: K\\otimes_{\\mathbb{Q}}\\mathbbQ_{p},\n",
      "  Tokens: 15\n",
      "  Loss: 0.2346\n",
      "  Perplexity: 1.2644\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 63/100\n",
      "  File: b5794f1f282ef50f.png\n",
      "  LaTeX: B(u,v)=(Q(u+v)-Q(u)-Q(v))/2.\n",
      "  Tokens: 28\n",
      "  Loss: 0.5076\n",
      "  Perplexity: 1.6613\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 64/100\n",
      "  File: c0e617ef9a2f074f.png\n",
      "  LaTeX: F(T_{0},\\eta_{0}(T_{0}))<0\n",
      "  Tokens: 23\n",
      "  Loss: 0.3033\n",
      "  Perplexity: 1.3543\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 65/100\n",
      "  File: 104659b4c988fcf2.png\n",
      "  LaTeX: \\theta=arctan(\\frac{x}{b})\n",
      "  Tokens: 17\n",
      "  Loss: 0.2688\n",
      "  Perplexity: 1.3084\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 66/100\n",
      "  File: f1e6c6a590bab6f8.png\n",
      "  LaTeX: (\\begin{matrix}u&0\\\\ az&u\\end{matrix})\n",
      "  Tokens: 29\n",
      "  Loss: 0.5266\n",
      "  Perplexity: 1.6932\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 67/100\n",
      "  File: ce68b0aeda395d49.png\n",
      "  LaTeX: Row_{n}=\\sum_{i=1}^{n-1}a_{i}\\cdot Row_{i}\n",
      "  Tokens: 34\n",
      "  Loss: 0.5195\n",
      "  Perplexity: 1.6813\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 68/100\n",
      "  File: 55d78b113fe2b7b7.png\n",
      "  LaTeX: T_{1/2}=1s\n",
      "  Tokens: 10\n",
      "  Loss: 0.1326\n",
      "  Perplexity: 1.1418\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 69/100\n",
      "  File: 006d598025642c4d.png\n",
      "  LaTeX: L:V\\rightarrow V\n",
      "  Tokens: 5\n",
      "  Loss: 0.1109\n",
      "  Perplexity: 1.1173\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 70/100\n",
      "  File: 1e9d2b0ded9966c3.png\n",
      "  LaTeX: \\overline{(\\Delta x)^{2}}\n",
      "  Tokens: 11\n",
      "  Loss: 0.1337\n",
      "  Perplexity: 1.1431\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 71/100\n",
      "  File: a7a1e37dc592094b.png\n",
      "  LaTeX: +\\sqrt{2}\n",
      "  Tokens: 5\n",
      "  Loss: 0.0587\n",
      "  Perplexity: 1.0604\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 72/100\n",
      "  File: c991af105fbdbc61.png\n",
      "  LaTeX: ...,p_{-2}-p_{-1},p_{-1}-p_{0},p_{0}-p_{1},p_{1}-p_{2},...\n",
      "  Tokens: 58\n",
      "  Loss: 0.7752\n",
      "  Perplexity: 2.1710\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 73/100\n",
      "  File: 7330cd16c5bb6961.png\n",
      "  LaTeX: \\frac{\\partial y}{\\partial\\varphi}\\cdot\\frac{\\partial x}{\\partial\\lambda}-\\frac{\\partial y}{\\partial\\lambda}\\cdot\\frac{\\partial x}{\\partial\\varphi}=s\\cdot cos\\varphi\\cdot\\frac{(1-e^{2})}{(1-e^{2}sin^{2}\\varphi)^{2}}\n",
      "  Tokens: 82\n",
      "  Loss: 1.1888\n",
      "  Perplexity: 3.2831\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 74/100\n",
      "  File: 44441f064e2a5860.png\n",
      "  LaTeX: g(z)=z+cz^{2}+\\cdot\\cdot\\cdot\n",
      "  Tokens: 17\n",
      "  Loss: 0.2784\n",
      "  Perplexity: 1.3210\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 75/100\n",
      "  File: 4234fac220a7c440.png\n",
      "  LaTeX: \\prod_{x}\\frac{x+1}{x}=Cx\n",
      "  Tokens: 17\n",
      "  Loss: 0.2142\n",
      "  Perplexity: 1.2389\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 76/100\n",
      "  File: ed845941a162e795.png\n",
      "  LaTeX: T^{1}(A)\\cong\\frac{A^{m}}{df\\cdot A^{n}}\n",
      "  Tokens: 27\n",
      "  Loss: 0.3990\n",
      "  Perplexity: 1.4903\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 77/100\n",
      "  File: 024ef0b45f820c36.png\n",
      "  LaTeX: P\\rightarrow Q\\Leftrightarrow\\neg P\\vee Q\n",
      "  Tokens: 8\n",
      "  Loss: 0.2064\n",
      "  Perplexity: 1.2292\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 78/100\n",
      "  File: 8c10b8c21dffd779.png\n",
      "  LaTeX: 2-log_{2}(\\sqrt{2})=\\frac{3}{2}\n",
      "  Tokens: 23\n",
      "  Loss: 0.2729\n",
      "  Perplexity: 1.3137\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 79/100\n",
      "  File: 7fb2f8bcdc1f212e.png\n",
      "  LaTeX: p=\\gamma m_{0}v,\n",
      "  Tokens: 10\n",
      "  Loss: 0.1501\n",
      "  Perplexity: 1.1620\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 80/100\n",
      "  File: 690fea03f7e830b6.png\n",
      "  LaTeX: x=b(X-vT)\n",
      "  Tokens: 9\n",
      "  Loss: 0.1482\n",
      "  Perplexity: 1.1598\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 81/100\n",
      "  File: fd7f474daed2a396.png\n",
      "  LaTeX: Prob(m|t)\n",
      "  Tokens: 9\n",
      "  Loss: 0.1608\n",
      "  Perplexity: 1.1744\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 82/100\n",
      "  File: 86c18809b27fce1f.png\n",
      "  LaTeX: p=\\prod_{i=1}^{n}p_{i}\n",
      "  Tokens: 18\n",
      "  Loss: 0.2557\n",
      "  Perplexity: 1.2914\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 83/100\n",
      "  File: 4355f1dea7dc4ce2.png\n",
      "  LaTeX: \\int_{r_{0}}^{\\infty}r^{-|k|+1}w_{k}(r)dr=0for|k|>1.\n",
      "  Tokens: 44\n",
      "  Loss: 0.7320\n",
      "  Perplexity: 2.0791\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 84/100\n",
      "  File: 7281dfac21bc6a0d.png\n",
      "  LaTeX: \\mu(X)=\\frac{a+4b+c}{6}\n",
      "  Tokens: 17\n",
      "  Loss: 0.2535\n",
      "  Perplexity: 1.2886\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 85/100\n",
      "  File: b6da9773cce27aa9.png\n",
      "  LaTeX: f^{A_{i}}(a_{1}(i),...,a_{n}(i)).\n",
      "  Tokens: 33\n",
      "  Loss: 0.4490\n",
      "  Perplexity: 1.5667\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 86/100\n",
      "  File: 5c0818d1a85bfca9.png\n",
      "  LaTeX: \\epsilon\\le1/2^{k-1}\n",
      "  Tokens: 11\n",
      "  Loss: 0.1770\n",
      "  Perplexity: 1.1936\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 87/100\n",
      "  File: b17266867c7bfea9.png\n",
      "  LaTeX: \\frac{z}{\\frac{\\frac{35.63576407}{b}*18\\cdot\\frac{16.338469}{v}\\cdot\\frac{19.786}{c}}{100000\\cdot556666667777\\cdot t}}\n",
      "  Tokens: 81\n",
      "  Loss: 1.1852\n",
      "  Perplexity: 3.2714\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 88/100\n",
      "  File: 6742a14d70bbcfa6.png\n",
      "  LaTeX: \\phi=cos^{-1}\\sqrt{\\frac{2}{3}}\n",
      "  Tokens: 20\n",
      "  Loss: 0.2720\n",
      "  Perplexity: 1.3125\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 89/100\n",
      "  File: 99878c9f3684251f.png\n",
      "  LaTeX: u_{tt}+sinu-(a_s^{2}g)u_{xx}=\\epsilon f(u),\n",
      "  Tokens: 35\n",
      "  Loss: 0.5910\n",
      "  Perplexity: 1.8057\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 90/100\n",
      "  File: ccb3140eb1780fc1.png\n",
      "  LaTeX: \\phi(\\alpha_{i})\n",
      "  Tokens: 8\n",
      "  Loss: 0.0942\n",
      "  Perplexity: 1.0987\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 91/100\n",
      "  File: da282f7cc58625f5.png\n",
      "  LaTeX: M\\sqrt{v}\n",
      "  Tokens: 5\n",
      "  Loss: 0.0772\n",
      "  Perplexity: 1.0802\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 92/100\n",
      "  File: 07955cb884833a69.png\n",
      "  LaTeX: p(c)=d\n",
      "  Tokens: 6\n",
      "  Loss: 0.1021\n",
      "  Perplexity: 1.1075\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 93/100\n",
      "  File: 9145c2e40f29180b.png\n",
      "  LaTeX: X_{\\epsilon}\n",
      "  Tokens: 5\n",
      "  Loss: 0.0552\n",
      "  Perplexity: 1.0567\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 94/100\n",
      "  File: b7cb20c5e392d47b.png\n",
      "  LaTeX: \\frac{\\sigma^{2}}{2\\theta}e^{-\\theta|t-s|}\n",
      "  Tokens: 23\n",
      "  Loss: 0.3599\n",
      "  Perplexity: 1.4332\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 95/100\n",
      "  File: 1dc18e394535c3ae.png\n",
      "  LaTeX: \\frac{95}{\\frac{75}{7.776612688}}\n",
      "  Tokens: 25\n",
      "  Loss: 0.3412\n",
      "  Perplexity: 1.4066\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 96/100\n",
      "  File: 8741bceb4e573546.png\n",
      "  LaTeX: 3:1.\n",
      "  Tokens: 4\n",
      "  Loss: 0.0664\n",
      "  Perplexity: 1.0687\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 97/100\n",
      "  File: b831f4a286a2e92b.png\n",
      "  LaTeX: |\\Delta x/\\Delta t|<c,\n",
      "  Tokens: 10\n",
      "  Loss: 0.2228\n",
      "  Perplexity: 1.2496\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 98/100\n",
      "  File: 361d261511c58aaa.png\n",
      "  LaTeX: (\\begin{matrix}k\\\\ 2\\end{matrix})-m\n",
      "  Tokens: 26\n",
      "  Loss: 0.4260\n",
      "  Perplexity: 1.5311\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 99/100\n",
      "  File: a0a0d8ee02374f34.png\n",
      "  LaTeX: M_{1,k}=\\int_{-\\infty}^{+\\infty}x^{k}\\varphi(x)dx=-\\int_{-\\infty}^{+\\infty}x^{k-1}\\varphi^{\\prime}(x)dx=(k-1)\\int_{-\\infty}^{+\\infty}x^{k-2}\\varphi(x)dx=(k-1)M_{1,k-2}\n",
      "  Tokens: 105\n",
      "  Loss: 1.5569\n",
      "  Perplexity: 4.7440\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Sample 100/100\n",
      "  File: 615d8f0f6f283ea3.png\n",
      "  LaTeX: \\frac{\\partial^{2}v}{\\partial t^{2}}\n",
      "  Tokens: 17\n",
      "  Loss: 0.2026\n",
      "  Perplexity: 1.2246\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "====================================================================================================\n",
      "VAL DATASET SUMMARY\n",
      "====================================================================================================\n",
      "Total samples: 100\n",
      "Average loss: 0.6027\n",
      "Overall perplexity: 1.8271\n",
      "Total tokens: 2245.0\n",
      "\n",
      "Perplexity Statistics:\n",
      "  Min: 1.0177\n",
      "  Max: 4.7440\n",
      "  Mean: 1.4849\n",
      "  Median: 1.2914\n",
      "\n",
      "Detailed results saved to: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/val_evaluation_results.csv\n",
      "\n",
      "====================================================================================================\n",
      "TOP 10 BEST PREDICTIONS (Lowest Perplexity)\n",
      "====================================================================================================\n",
      "\n",
      "Perplexity: 1.0177\n",
      "LaTeX: f\n",
      "File: 64d9386defa685f1.png\n",
      "\n",
      "Perplexity: 1.0533\n",
      "LaTeX: M_{3}\n",
      "File: ec891dbd997afbdc.png\n",
      "\n",
      "Perplexity: 1.0567\n",
      "LaTeX: X_{\\epsilon}\n",
      "File: 9145c2e40f29180b.png\n",
      "\n",
      "Perplexity: 1.0604\n",
      "LaTeX: +\\sqrt{2}\n",
      "File: a7a1e37dc592094b.png\n",
      "\n",
      "Perplexity: 1.0687\n",
      "LaTeX: 3:1.\n",
      "File: 8741bceb4e573546.png\n",
      "\n",
      "Perplexity: 1.0734\n",
      "LaTeX: A(u)\n",
      "File: 2ead5d17759653c7.png\n",
      "\n",
      "Perplexity: 1.0802\n",
      "LaTeX: M\\sqrt{v}\n",
      "File: da282f7cc58625f5.png\n",
      "\n",
      "Perplexity: 1.0921\n",
      "LaTeX: \\Delta\\langle2\\rangle\n",
      "File: 1517a95764083a78.png\n",
      "\n",
      "Perplexity: 1.0944\n",
      "LaTeX: I_{b}b\n",
      "File: 713389699c2621cd.png\n",
      "\n",
      "Perplexity: 1.0987\n",
      "LaTeX: \\phi(\\alpha_{i})\n",
      "File: ccb3140eb1780fc1.png\n",
      "\n",
      "====================================================================================================\n",
      "TOP 10 WORST PREDICTIONS (Highest Perplexity)\n",
      "====================================================================================================\n",
      "\n",
      "Perplexity: 2.0791\n",
      "LaTeX: \\int_{r_{0}}^{\\infty}r^{-|k|+1}w_{k}(r)dr=0for|k|>1.\n",
      "File: 4355f1dea7dc4ce2.png\n",
      "\n",
      "Perplexity: 2.1710\n",
      "LaTeX: ...,p_{-2}-p_{-1},p_{-1}-p_{0},p_{0}-p_{1},p_{1}-p_{2},...\n",
      "File: c991af105fbdbc61.png\n",
      "\n",
      "Perplexity: 2.6947\n",
      "LaTeX: \\Sigma_{YY}^{1/2}=V_{Y}D_Y^{1/2}V_Y^{\\top},V_{Y}D_{Y}V_Y^{\\top}=\\Sigma_{YY}.\n",
      "File: 1995c29d6d7fa81a.png\n",
      "\n",
      "Perplexity: 2.9745\n",
      "LaTeX: \\frac{\\frac{555555666661}{111999444442}*76}{\\frac{92.870346505}{c+\\frac{17.04976473}{1000000}}}\n",
      "File: a4d73f7620faa203.png\n",
      "\n",
      "Perplexity: 3.1852\n",
      "LaTeX: PPI(monitor)=\\frac{NumberofPixels}{SizeinInches}=\\frac{1920}{20}=96ppi\n",
      "File: 463ffc2a3cb2d667.png\n",
      "\n",
      "Perplexity: 3.2714\n",
      "LaTeX: \\frac{z}{\\frac{\\frac{35.63576407}{b}*18\\cdot\\frac{16.338469}{v}\\cdot\\frac{19.786}{c}}{100000\\cdot556666667777\\cdot t}}\n",
      "File: b17266867c7bfea9.png\n",
      "\n",
      "Perplexity: 3.2831\n",
      "LaTeX: \\frac{\\partial y}{\\partial\\varphi}\\cdot\\frac{\\partial x}{\\partial\\lambda}-\\frac{\\partial y}{\\partial\\lambda}\\cdot\\frac{\\partial x}{\\partial\\varphi}=s\\cdot cos\\varphi\\cdot\\frac{(1-e^{2})}{(1-e^{2}sin^{2}\\varphi)^{2}}\n",
      "File: 7330cd16c5bb6961.png\n",
      "\n",
      "Perplexity: 3.7168\n",
      "LaTeX: \\frac{\\partial lnL(\\alpha,\\beta,a,c|Y)}{\\partial\\alpha}=\\sum_{i=1}^{N}ln(Y_{i}-a)-N(-\\psi(\\alpha+\\beta)+\\psi(\\alpha))-Nln(c-a)=0\n",
      "File: 6c1fee59934b6686.png\n",
      "\n",
      "Perplexity: 3.9996\n",
      "LaTeX: 128cos\\frac{\\pi}{29}cos\\frac{4\\pi}{29}cos\\frac{5\\pi}{29}cos\\frac{6\\pi}{29}cos\\frac{7\\pi}{29}cos\\frac{9\\pi}{29}cos\\frac{13\\pi}{29}\n",
      "File: 456f96a1b217e0f3.png\n",
      "\n",
      "Perplexity: 4.7440\n",
      "LaTeX: M_{1,k}=\\int_{-\\infty}^{+\\infty}x^{k}\\varphi(x)dx=-\\int_{-\\infty}^{+\\infty}x^{k-1}\\varphi^{\\prime}(x)dx=(k-1)\\int_{-\\infty}^{+\\infty}x^{k-2}\\varphi(x)dx=(k-1)M_{1,k-2}\n",
      "File: a0a0d8ee02374f34.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "def evaluate_full_dataset(dataset_name='test'):\n",
    "    \"\"\"\n",
    "    Evaluate model on entire test or validation dataset\n",
    "    Generate detailed results with perplexity for each sample\n",
    "    \"\"\"\n",
    "    DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'rb') as f:\n",
    "        token2idx = pickle.load(f)\n",
    "    with open(os.path.join(DATA_PATH, 'idx2token.pkl'), 'rb') as f:\n",
    "        idx2token = pickle.load(f)\n",
    "    \n",
    "    # Load model\n",
    "    device = torch.device('cuda:1')\n",
    "    model = TMLM(\n",
    "        vocab_size=len(token2idx),\n",
    "        num_layers=2,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        d_ff=1024,\n",
    "        max_seq_len=256,\n",
    "        dropout=0.1,\n",
    "        pad_idx=token2idx['<pad>']\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(os.path.join(DATA_PATH, 'best_tmlm2l_model.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Load dataset\n",
    "    csv_file = os.path.join(DATA_PATH, f'{dataset_name}_database_preprocessed.csv')\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"DETAILED {dataset_name.upper()} DATASET EVALUATION\")\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    results = []\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    pad_idx = token2idx['<pad>']\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        latex = row['preprocessed_label']\n",
    "        filename = row['filename']\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenize_latex(latex, token2idx)\n",
    "        \n",
    "        if len(tokens) > 0:\n",
    "            # Prepare input\n",
    "            input_seq = tokens + [token2idx['<eos>']]\n",
    "            input_seq = input_seq + [pad_idx] * (256 - len(input_seq))\n",
    "            input_seq = input_seq[:256]\n",
    "            \n",
    "            input_tensor = torch.tensor([input_seq]).to(device)\n",
    "            targets = input_tensor[:, 1:]\n",
    "            inputs = input_tensor[:, :-1]\n",
    "            \n",
    "            # Calculate loss\n",
    "            with torch.no_grad():\n",
    "                loss = model(inputs, targets)\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            mask = (targets != pad_idx).float()\n",
    "            n_tokens = mask.sum().item()\n",
    "            \n",
    "            # Per-sample perplexity\n",
    "            sample_perplexity = math.exp(loss.item())\n",
    "            \n",
    "            # Accumulate\n",
    "            total_loss += loss.item() * n_tokens\n",
    "            total_tokens += n_tokens\n",
    "            \n",
    "            results.append({\n",
    "                'filename': filename,\n",
    "                'latex': latex,\n",
    "                'loss': loss.item(),\n",
    "                'perplexity': sample_perplexity,\n",
    "                'num_tokens': len(tokens)\n",
    "            })\n",
    "            \n",
    "            # Print every sample\n",
    "            print(f\"Sample {idx+1}/{len(df)}\")\n",
    "            print(f\"  File: {filename}\")\n",
    "            print(f\"  LaTeX: {latex}\")\n",
    "            print(f\"  Tokens: {len(tokens)}\")\n",
    "            print(f\"  Loss: {loss.item():.4f}\")\n",
    "            print(f\"  Perplexity: {sample_perplexity:.4f}\")\n",
    "            print(\"-\" * 100)\n",
    "    \n",
    "    # Overall statistics\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    overall_perplexity = math.exp(avg_loss)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(f\"{dataset_name.upper()} DATASET SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")\n",
    "    print(f\"Overall perplexity: {overall_perplexity:.4f}\")\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    \n",
    "    # Statistics\n",
    "    perplexities = [r['perplexity'] for r in results]\n",
    "    print(f\"\\nPerplexity Statistics:\")\n",
    "    print(f\"  Min: {min(perplexities):.4f}\")\n",
    "    print(f\"  Max: {max(perplexities):.4f}\")\n",
    "    print(f\"  Mean: {sum(perplexities)/len(perplexities):.4f}\")\n",
    "    print(f\"  Median: {sorted(perplexities)[len(perplexities)//2]:.4f}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    output_file = os.path.join(DATA_PATH, f'{dataset_name}_evaluation_results.csv')\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nDetailed results saved to: {output_file}\")\n",
    "    \n",
    "    # Show top 10 best and worst predictions\n",
    "    results_df = results_df.sort_values('perplexity')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TOP 10 BEST PREDICTIONS (Lowest Perplexity)\")\n",
    "    print(\"=\"*100)\n",
    "    for idx, row in results_df.head(10).iterrows():\n",
    "        print(f\"\\nPerplexity: {row['perplexity']:.4f}\")\n",
    "        print(f\"LaTeX: {row['latex']}\")\n",
    "        print(f\"File: {row['filename']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TOP 10 WORST PREDICTIONS (Highest Perplexity)\")\n",
    "    print(\"=\"*100)\n",
    "    for idx, row in results_df.tail(10).iterrows():\n",
    "        print(f\"\\nPerplexity: {row['perplexity']:.4f}\")\n",
    "        print(f\"LaTeX: {row['latex']}\")\n",
    "        print(f\"File: {row['filename']}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "def tokenize_latex(latex_str, token2idx):\n",
    "    \"\"\"Tokenize LaTeX string\"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(latex_str):\n",
    "        if latex_str[i] == '\\\\':\n",
    "            j = i + 1\n",
    "            while j < len(latex_str) and latex_str[j].isalpha():\n",
    "                j += 1\n",
    "            token = latex_str[i:j]\n",
    "            tokens.append(token2idx.get(token, token2idx['<pad>']))\n",
    "            i = j\n",
    "        elif latex_str[i] in ['{', '}', '^', '_', '(', ')', '[', ']', '|', ',', '.', '=', '+', '-', '*', '/', '<', '>', '!', '&']:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "        elif latex_str[i] == ' ':\n",
    "            i += 1\n",
    "        else:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Run evaluations\n",
    "print(\"Starting Test Dataset Evaluation...\")\n",
    "test_results = evaluate_full_dataset('test')\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"Starting Validation Dataset Evaluation...\")\n",
    "val_results = evaluate_full_dataset('val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165a32e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "MODEL UNDERSTANDING VISUALIZATION\n",
      "====================================================================================================\n",
      "\n",
      "This shows how CONFIDENT the model is about the CORRECT LaTeX for each image\n",
      "Lower perplexity = Higher confidence that this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 41c6736570400100.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/41c6736570400100.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: d_K^{-p/2}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.1772\n",
      "  Loss: 0.1632\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: d0070ee72da60d82.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/d0070ee72da60d82.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: 4x+5y=32\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.1625\n",
      "  Loss: 0.1506\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 9a5833d0916a8408.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/9a5833d0916a8408.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\frac{2.138}{10000000000}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.2257\n",
      "  Loss: 0.2035\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 2a8bd69b5c43cf6a.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/2a8bd69b5c43cf6a.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: C_{3}=G_{2}+G_{1}\\cdot P_{2}+G_{0}\\cdot P_{1}\\cdot P_{2}+C_{0}\\cdot P_{0}\\cdot P_{1}\\cdot P_{2}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 2.2821\n",
      "  Loss: 0.8251\n",
      "  Assessment: ⚠ MODERATE confidence - Model is somewhat uncertain\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 2ea09c1e42b328ee.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/2ea09c1e42b328ee.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: TU=\\sqrt{\\frac{DU^{3}}{G*M}}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.3784\n",
      "  Loss: 0.3209\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 573be84ee0929393.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/573be84ee0929393.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: S(0)=x,\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.1110\n",
      "  Loss: 0.1052\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: c191c94a7e5fa425.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/c191c94a7e5fa425.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: r\\le\\mu(F_{r})<\\infty.\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.2397\n",
      "  Loss: 0.2149\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 41599f74f3ac2263.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/41599f74f3ac2263.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: {6^{437}}^{\\frac{14}{64}}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.3137\n",
      "  Loss: 0.2729\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: b1df8ab07e7b98d5.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/b1df8ab07e7b98d5.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\frac{c^{\\prime}}{\\sqrt{1-\\frac{c^{6}}{9}}}\\ge1\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.4352\n",
      "  Loss: 0.3613\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: a30ae78768f5d7db.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/a30ae78768f5d7db.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: (cos\\frac{\\theta}{2},sin\\frac{\\theta}{2})\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.3734\n",
      "  Loss: 0.3173\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 7f5ba0a5b752298a.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/7f5ba0a5b752298a.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: Y=u(\\vec{X})\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.1563\n",
      "  Loss: 0.1452\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: efd301eb630e129d.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/efd301eb630e129d.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: i^{-s}Li_{s}(e^{2\\pi ix})+i^{s}Li_{s}(e^{-2\\pi ix})=\\frac{(2\\pi)^{s}}{\\Gamma(s)}\\zeta(1-s,x),\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 3.0884\n",
      "  Loss: 1.1277\n",
      "  Assessment: ✗ LOW confidence - Model thinks this LaTeX might be unusual\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: eb10a04acd63bef3.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/eb10a04acd63bef3.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\Delta E=B\\mu_{B}\\Delta m_{l}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.2669\n",
      "  Loss: 0.2366\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: dcd2e89f9b370238.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/dcd2e89f9b370238.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\gamma/\\alpha.\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.0768\n",
      "  Loss: 0.0740\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: b1487a6427de9657.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/b1487a6427de9657.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: O(rad(abc)\\Theta(abc)),\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.4037\n",
      "  Loss: 0.3391\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: bcd0302ec65f057b.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/bcd0302ec65f057b.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\mathbbP^{C}=\\mathbbR^{C}-\\{0\\}/\\mathbbR_{>0}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.4588\n",
      "  Loss: 0.3776\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: bcf669c0b8d925ad.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/bcf669c0b8d925ad.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\frac{\\frac{t}{8}-53}{7}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.2138\n",
      "  Loss: 0.1937\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: cd700d6d744064cd.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/cd700d6d744064cd.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: ab^{2}c\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.1146\n",
      "  Loss: 0.1085\n",
      "  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: e9c006e585d7a1c7.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/e9c006e585d7a1c7.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\frac{\\frac{\\frac{3}{23}}{48}*a}{\\frac{111111333992}{\\frac{444336666611}{67}}}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 2.1280\n",
      "  Loss: 0.7552\n",
      "  Assessment: ⚠ MODERATE confidence - Model is somewhat uncertain\n",
      "====================================================================================================\n",
      "\n",
      "====================================================================================================\n",
      "Image File: 3f473170a3e16352.png\n",
      "Image Path: /home/ie643_errorcode500/errorcode500-working/Mathwritting-1000/test/3f473170a3e16352.png\n",
      "✓ Image found\n",
      "\n",
      "Ground Truth LaTeX: \\upsilon_{c}=\\frac{8}{\\sqrt{L_{\\frac{8}{0}}C_{\\frac{8}{0}}}}\n",
      "\n",
      "Model Confidence:\n",
      "  Perplexity: 1.5454\n",
      "  Loss: 0.4353\n",
      "  Assessment: ✓ HIGH confidence - Model thinks this LaTeX is likely correct\n",
      "====================================================================================================\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "SUMMARY\n",
      "====================================================================================================\n",
      "Average Perplexity: 1.4576\n",
      "Best (Lowest): 1.0768\n",
      "Worst (Highest): 3.0884\n",
      "\n",
      "High Confidence Predictions: 16/20 (80.0%)\n",
      "\n",
      "====================================================================================================\n",
      "WHAT THIS MEANS:\n",
      "====================================================================================================\n",
      "✓ Your Language Model has learned mathematical LaTeX patterns very well!\n",
      "✓ It can identify correct vs incorrect LaTeX with high confidence\n",
      "✓ To predict FROM images, you need an image encoder (like SRTC+SLP in the paper)\n",
      "\n",
      "✗ Currently showing: How well the model understands the CORRECT LaTeX\n",
      "✗ NOT showing: Model predictions from scratch (needs image encoder)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_model_understanding():\n",
    "    \"\"\"\n",
    "    Show images alongside ground truth and model confidence\n",
    "    This shows HOW WELL the model understands the correct LaTeX\n",
    "    \"\"\"\n",
    "    DATA_PATH = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(os.path.join(DATA_PATH, 'token2idx.pkl'), 'rb') as f:\n",
    "        token2idx = pickle.load(f)\n",
    "    with open(os.path.join(DATA_PATH, 'idx2token.pkl'), 'rb') as f:\n",
    "        idx2token = pickle.load(f)\n",
    "    \n",
    "    # Load model\n",
    "    device = torch.device('cuda:1')\n",
    "    model = TMLM(\n",
    "        vocab_size=len(token2idx),\n",
    "        num_layers=2,\n",
    "        d_model=256,\n",
    "        num_heads=4,\n",
    "        d_ff=1024,\n",
    "        max_seq_len=256,\n",
    "        dropout=0.1,\n",
    "        pad_idx=token2idx['<pad>']\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(os.path.join(DATA_PATH, 'best_tmlm2l_model.pt'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_df = pd.read_csv(os.path.join(DATA_PATH, 'test_database_preprocessed.csv'))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MODEL UNDERSTANDING VISUALIZATION\")\n",
    "    print(\"=\"*100)\n",
    "    print(\"\\nThis shows how CONFIDENT the model is about the CORRECT LaTeX for each image\")\n",
    "    print(\"Lower perplexity = Higher confidence that this LaTeX is correct\")\n",
    "    print(\"=\"*100 + \"\\n\")\n",
    "    \n",
    "    # Sample 20 test cases\n",
    "    samples = test_df.sample(20)\n",
    "    \n",
    "    results = []\n",
    "    pad_idx = token2idx['<pad>']\n",
    "    \n",
    "    for idx, row in samples.iterrows():\n",
    "        latex = row['preprocessed_label']\n",
    "        filename = row['filename']\n",
    "        image_path = os.path.join(DATA_PATH, 'test', filename)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = tokenize_latex(latex, token2idx)\n",
    "        \n",
    "        if len(tokens) > 0:\n",
    "            # Prepare input\n",
    "            input_seq = tokens + [token2idx['<eos>']]\n",
    "            input_seq = input_seq + [pad_idx] * (256 - len(input_seq))\n",
    "            input_seq = input_seq[:256]\n",
    "            \n",
    "            input_tensor = torch.tensor([input_seq]).to(device)\n",
    "            targets = input_tensor[:, 1:]\n",
    "            inputs = input_tensor[:, :-1]\n",
    "            \n",
    "            # Calculate loss\n",
    "            with torch.no_grad():\n",
    "                loss = model(inputs, targets)\n",
    "            \n",
    "            sample_perplexity = math.exp(loss.item())\n",
    "            \n",
    "            # Check if image exists\n",
    "            image_exists = os.path.exists(image_path)\n",
    "            \n",
    "            print(f\"{'='*100}\")\n",
    "            print(f\"Image File: {filename}\")\n",
    "            if image_exists:\n",
    "                print(f\"Image Path: {image_path}\")\n",
    "                print(f\"✓ Image found\")\n",
    "            else:\n",
    "                print(f\"✗ Image not found at: {image_path}\")\n",
    "            print(f\"\\nGround Truth LaTeX: {latex}\")\n",
    "            print(f\"\\nModel Confidence:\")\n",
    "            print(f\"  Perplexity: {sample_perplexity:.4f}\")\n",
    "            print(f\"  Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            if sample_perplexity < 1.5:\n",
    "                print(f\"  Assessment: ✓ VERY HIGH confidence - Model strongly agrees this LaTeX is correct\")\n",
    "            elif sample_perplexity < 2.0:\n",
    "                print(f\"  Assessment: ✓ HIGH confidence - Model thinks this LaTeX is likely correct\")\n",
    "            elif sample_perplexity < 3.0:\n",
    "                print(f\"  Assessment: ⚠ MODERATE confidence - Model is somewhat uncertain\")\n",
    "            else:\n",
    "                print(f\"  Assessment: ✗ LOW confidence - Model thinks this LaTeX might be unusual\")\n",
    "            \n",
    "            print(f\"{'='*100}\\n\")\n",
    "            \n",
    "            results.append({\n",
    "                'filename': filename,\n",
    "                'latex': latex,\n",
    "                'perplexity': sample_perplexity,\n",
    "                'image_exists': image_exists\n",
    "            })\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    perplexities = [r['perplexity'] for r in results]\n",
    "    print(f\"Average Perplexity: {sum(perplexities)/len(perplexities):.4f}\")\n",
    "    print(f\"Best (Lowest): {min(perplexities):.4f}\")\n",
    "    print(f\"Worst (Highest): {max(perplexities):.4f}\")\n",
    "    \n",
    "    high_conf = sum(1 for p in perplexities if p < 1.5)\n",
    "    print(f\"\\nHigh Confidence Predictions: {high_conf}/{len(perplexities)} ({100*high_conf/len(perplexities):.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"WHAT THIS MEANS:\")\n",
    "    print(\"=\"*100)\n",
    "    print(\"✓ Your Language Model has learned mathematical LaTeX patterns very well!\")\n",
    "    print(\"✓ It can identify correct vs incorrect LaTeX with high confidence\")\n",
    "    print(\"✓ To predict FROM images, you need an image encoder (like SRTC+SLP in the paper)\")\n",
    "    print(\"\\n✗ Currently showing: How well the model understands the CORRECT LaTeX\")\n",
    "    print(\"✗ NOT showing: Model predictions from scratch (needs image encoder)\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n",
    "def tokenize_latex(latex_str, token2idx):\n",
    "    \"\"\"Tokenize LaTeX string\"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(latex_str):\n",
    "        if latex_str[i] == '\\\\':\n",
    "            j = i + 1\n",
    "            while j < len(latex_str) and latex_str[j].isalpha():\n",
    "                j += 1\n",
    "            token = latex_str[i:j]\n",
    "            tokens.append(token2idx.get(token, token2idx['<pad>']))\n",
    "            i = j\n",
    "        elif latex_str[i] in ['{', '}', '^', '_', '(', ')', '[', ']', '|', ',', '.', '=', '+', '-', '*', '/', '<', '>', '!', '&']:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "        elif latex_str[i] == ' ':\n",
    "            i += 1\n",
    "        else:\n",
    "            tokens.append(token2idx.get(latex_str[i], token2idx['<pad>']))\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Run visualization\n",
    "visualize_model_understanding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80a2932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
