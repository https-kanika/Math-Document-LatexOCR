{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5e35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CSV DATASET INFORMATION\n",
      "============================================================\n",
      "Columns: ['filename', 'sample_id', 'label', 'normalized_label', 'is_symbol']\n",
      "Total training samples available: 10,000\n",
      "Will use: 10,000 samples\n",
      "============================================================\n",
      "\n",
      "First few filenames:\n",
      "0    a9fa242701017325.png\n",
      "1    78f6cbf89778f0aa.png\n",
      "2    d167b3da5fc40cc2.png\n",
      "3    a1fbae100abe06bd.png\n",
      "4    5b06d5ab01e8019b.png\n",
      "Name: filename, dtype: object\n",
      "\n",
      "============================================================\n",
      "TRAINING DATASET READY\n",
      "============================================================\n",
      "Dataset size: 10,000 samples\n",
      "Batch size: 32\n",
      "Batches per epoch: 313\n",
      "============================================================\n",
      "\n",
      "✓ Batch loaded successfully\n",
      "  Image tensor shape: torch.Size([32, 9, 480, 1600])\n",
      "  First 3 labels: ['S_{b}|b+\\\\rangle=\\\\frac{\\\\hbar}{2}|b+\\\\rangle', '[\\\\begin{matrix}1&a\\\\\\\\ 0&0\\\\end{matrix}]', '\\\\int pdq']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 1: Dataset Loading with SUBSET SAMPLING\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'  \n",
    "\n",
    "\n",
    "def get_directional_kernels():\n",
    "    # 8 edge detection kernels: N, NE, E, SE, S, SW, W, NW\n",
    "    k = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]) # Vertical\n",
    "    kernels = [\n",
    "        k,                              # S\n",
    "        np.rot90(k, 1),                 # W\n",
    "        np.rot90(k, 2),                 # N\n",
    "        np.rot90(k, 3),                 # E\n",
    "        np.fliplr(k),                   # SW\n",
    "        np.flipud(k),                   # NE\n",
    "        np.fliplr(np.rot90(k, 1)),      # NW\n",
    "        np.flipud(np.rot90(k, 3)),      # SE\n",
    "    ]\n",
    "    return kernels\n",
    "\n",
    "def get_directional_maps(image):\n",
    "    kernels = get_directional_kernels()\n",
    "    edge_maps = []\n",
    "    for kern in kernels:    \n",
    "        em = cv2.filter2D(image, cv2.CV_32F, kern, borderType=cv2.BORDER_REPLICATE)\n",
    "        em = np.abs(em)\n",
    "        maxv = em.max()\n",
    "        if maxv > 1e-8:\n",
    "            em = em / maxv \n",
    "        else:\n",
    "            em = np.zeros_like(em, dtype=np.float32)\n",
    "        \n",
    "        edge_maps.append(em.astype(np.float32))\n",
    "    return np.stack(edge_maps, axis=0)  # [8, H, W]\n",
    "\n",
    "\n",
    "class MathEquation9ChDataset(Dataset):\n",
    "    def __init__(self, csv_file, dataset_root, split='train', transform=None, max_samples=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file: Path to CSV file with 'filename' column\n",
    "            dataset_root: Root directory of dataset\n",
    "            split: 'train', 'val', or 'test' - determines subdirectory\n",
    "            transform: Optional transforms\n",
    "            max_samples: Maximum number of samples to use (None = use all)\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # SAMPLE SUBSET if max_samples is specified\n",
    "        if max_samples is not None and max_samples < len(df):\n",
    "            print(f\"Sampling {max_samples:,} from {len(df):,} available samples...\")\n",
    "            # Use random sampling with fixed seed for reproducibility\n",
    "            df = df.sample(n=max_samples, random_state=42).reset_index(drop=True)\n",
    "            print(f\"Sampled dataset size: {len(df):,}\")\n",
    "        \n",
    "        self.data_frame = df\n",
    "        self.dataset_root = dataset_root\n",
    "        self.split = split  # train, val, or test\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get filename from CSV\n",
    "        filename = self.data_frame.iloc[idx]['filename']\n",
    "        \n",
    "        # Construct full path: dataset_root/split/filename\n",
    "        img_full_path = os.path.join(self.dataset_root, self.split, filename)\n",
    "        img_full_path = os.path.normpath(img_full_path).replace('\\\\', '/')\n",
    "        \n",
    "        image = cv2.imread(img_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_full_path}\")\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        H, W = image.shape\n",
    "        \n",
    "        # 9 channel construction\n",
    "        channels = np.zeros((9, H, W), dtype=np.float32)\n",
    "        channels[0] = image  # Greyscale base\n",
    "        channels[1:] = get_directional_maps(image)  # 8 directions\n",
    "        label = self.data_frame.iloc[idx]['normalized_label']\n",
    "        sample = {'image': torch.tensor(channels, dtype=torch.float32), 'label': label}\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        return sample\n",
    "\n",
    "# ==============================\n",
    "# CONFIGURE SUBSET SIZES HERE\n",
    "# ==============================\n",
    "TRAIN_SAMPLES = 10000\n",
    "VAL_SAMPLES = 2000\n",
    "TEST_SAMPLES = 2000\n",
    "\n",
    "# Usage:\n",
    "DATASET_ROOT ='/home/ie643_errorcode500/errorcode500-working/Mathwritting-10k'\n",
    "TRAIN_CSV = os.path.join(DATASET_ROOT, 'train_database.csv')\n",
    "\n",
    "# Let's first check if the CSV exists and print its contents\n",
    "if os.path.exists(TRAIN_CSV):\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    print(\"=\"*60)\n",
    "    print(\"CSV DATASET INFORMATION\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(f\"Total training samples available: {len(df):,}\")\n",
    "    print(f\"Will use: {TRAIN_SAMPLES:,} samples\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nFirst few filenames:\")\n",
    "    print(df['filename'].head())\n",
    "else:\n",
    "    print(f\"CSV file not found at {TRAIN_CSV}\")\n",
    "\n",
    "# Create dataset with SUBSET\n",
    "train_dataset = MathEquation9ChDataset(\n",
    "    TRAIN_CSV, \n",
    "    DATASET_ROOT, \n",
    "    split='train',\n",
    "    max_samples=TRAIN_SAMPLES  # KEY PARAMETER\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32,  # Increased batch size\n",
    "    shuffle=True,\n",
    "    num_workers=8,  # Parallel data loading\n",
    "    pin_memory=True,  # Faster GPU transfer\n",
    "    persistent_workers=True  # Keep workers alive\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING DATASET READY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Dataset size: {len(train_dataset):,} samples\")\n",
    "print(f\"Batch size: 32\")\n",
    "print(f\"Batches per epoch: {len(train_loader):,}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "try:\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        print(f\"✓ Batch loaded successfully\")\n",
    "        print(f\"  Image tensor shape: {images.shape}\")\n",
    "        print(f\"  First 3 labels: {labels[:3]}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Error loading batch: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699bf695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 92\n",
      "First 20 tokens: ['<PAD>', '<SOS>', '<EOS>', ' ', '!', '#', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3']\n",
      "Original label: \\frac{54.88043}{x}\n",
      "Encoded: [1, 59, 68, 80, 63, 65, 89, 21, 20, 14, 24, 24, 16, 20, 19, 91, 89, 86, 91, 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load all labels from train/val/test CSVs\n",
    "csv_files = [\n",
    "    'train_database.csv',\n",
    "    'val_database.csv',\n",
    "    'test_database.csv'\n",
    "]\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(DATASET_ROOT, csv_file))\n",
    "    all_labels.extend(df['normalized_label'].astype(str).tolist())\n",
    "\n",
    "# Build character-level vocabulary\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>']\n",
    "char_counter = Counter()\n",
    "for label in all_labels:\n",
    "    char_counter.update(list(label))\n",
    "\n",
    "vocab = special_tokens + sorted(char_counter.keys())\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"First 20 tokens:\", vocab[:20])\n",
    "\n",
    "# Encode a label string to indices\n",
    "def encode_label(label, max_len=128):\n",
    "    tokens = [char2idx['<SOS>']] + [char2idx[ch] for ch in label] + [char2idx['<EOS>']]\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [char2idx['<PAD>']] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sample_label = all_labels[0]\n",
    "encoded = encode_label(sample_label)\n",
    "print(\"Original label:\", sample_label)\n",
    "print(\"Encoded:\", encoded[:20])\n",
    "\n",
    "# For your dataset class, you can add:\n",
    "# label_indices = encode_label(label)\n",
    "# sample = {'image': image_tensor, 'label': label_indices}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9403e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=4, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Conv2d(\n",
    "                in_channels if i == 0 else out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout_p > 0:\n",
    "                layers.append(nn.Dropout2d(p=dropout_p))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class WatcherFCN(nn.Module):\n",
    "    def __init__(self, in_channels=9):\n",
    "        super().__init__()\n",
    "        # First blocks without dropout\n",
    "        self.block1 = ConvBlock(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.block2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        # Last blocks with 20% dropout\n",
    "        self.block3 = ConvBlock(64, 64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.block4 = ConvBlock(64, 128, dropout_p=0.2)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool4(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "model = WatcherFCN(in_channels=9)\n",
    "dummy_input = torch.randn(2, 9, 480, 1600)\n",
    "output = model(dummy_input)\n",
    "print(output.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a66471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3000, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, channels, height, width = output.shape\n",
    "encoder_outputs = output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "# encoder_outputs: [batch, 3000, 512]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoverageAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim, coverage_dim):\n",
    "        super().__init__()\n",
    "        self.W_a = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U_a = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.U_f = nn.Linear(coverage_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden, coverage):\n",
    "        # encoder_outputs: [batch, L, encoder_dim]\n",
    "        # decoder_hidden: [batch, decoder_dim]\n",
    "        # coverage: [batch, L, coverage_dim]\n",
    "        Wh = self.W_a(decoder_hidden).unsqueeze(1)  # [batch, 1, att_dim]\n",
    "        Ua = self.U_a(encoder_outputs)              # [batch, L, att_dim]\n",
    "        Uf = self.U_f(coverage)                     # [batch, L, att_dim]\n",
    "        att = torch.tanh(Wh + Ua + Uf)              # [batch, L, att_dim]\n",
    "        scores = self.v(att).squeeze(-1)            # [batch, L]\n",
    "        alpha = F.softmax(scores, dim=1)            # [batch, L]\n",
    "        context = torch.sum(encoder_outputs * alpha.unsqueeze(-1), dim=1)  # [batch, encoder_dim]\n",
    "        return context, alpha\n",
    "\n",
    "# class ParserGRUDecoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, encoder_dim=128, embed_dim=256, decoder_dim=256, attention_dim=256, coverage_dim=1):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "#         self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "#         self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "#         self.fc = nn.Linear(decoder_dim + encoder_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, encoder_outputs, targets, max_len):\n",
    "#         batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "#         device = encoder_outputs.device\n",
    "#         coverage = torch.zeros(batch_size, L, 1, device=device)\n",
    "#         inputs = torch.full((batch_size,), 1, dtype=torch.long, device=device)  # <SOS> token index\n",
    "#         hidden = torch.zeros(batch_size, 256, device=device)\n",
    "#         outputs = []\n",
    "#         for t in range(max_len):\n",
    "#             embedded = self.embedding(inputs)  # [batch, embed_dim]\n",
    "#             context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "#             gru_input = torch.cat([embedded, context], dim=1)\n",
    "#             hidden = self.gru(gru_input, hidden)\n",
    "#             output = self.fc(torch.cat([hidden, context], dim=1))\n",
    "#             outputs.append(output)\n",
    "#             # Teacher forcing: use ground truth if available\n",
    "#             if targets is not None and t < targets.size(1):\n",
    "#                 inputs = targets[:, t]\n",
    "#             else:\n",
    "#                 inputs = output.argmax(dim=1)\n",
    "#             coverage = coverage + alpha.unsqueeze(-1)\n",
    "#         outputs = torch.stack(outputs, dim=1)  # [batch, max_len, vocab_size]\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "#Modified ParserGRUDecoder\n",
    "\n",
    "class ParserGRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_dim=128, embed_dim=256, decoder_dim=256, attention_dim=256, coverage_dim=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Modify input size to include context vector\n",
    "        self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "        # Change output layer to use all available information\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(decoder_dim + encoder_dim + embed_dim, decoder_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(decoder_dim, vocab_size)\n",
    "        )\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "    def forward(self, encoder_outputs, targets, max_len):\n",
    "        batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "        device = encoder_outputs.device\n",
    "        coverage = torch.zeros(batch_size, L, 1, device=device)\n",
    "        inputs = torch.full((batch_size,), 1, dtype=torch.long, device=device)  # <SOS> token index\n",
    "        hidden = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(max_len):\n",
    "            # 1. Get current input embedding\n",
    "            embedded = self.embedding(inputs)  # [batch, embed_dim]\n",
    "            \n",
    "            # 2. Calculate attention and context\n",
    "            context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "            \n",
    "            # 3. Update GRU hidden state with concatenated input\n",
    "            gru_input = torch.cat([embedded, context], dim=1)\n",
    "            hidden = self.gru(gru_input, hidden)\n",
    "            \n",
    "            # 4. Generate output using all available information\n",
    "            # Concatenate current embedding, hidden state, and context\n",
    "            output = self.out(torch.cat([embedded, hidden, context], dim=1))\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # 5. Teacher forcing or use own predictions\n",
    "            if targets is not None and t < targets.size(1):\n",
    "                inputs = targets[:, t]\n",
    "            else:\n",
    "                inputs = output.argmax(dim=1)\n",
    "            \n",
    "            # 6. Update coverage vector\n",
    "            coverage = coverage + alpha.unsqueeze(-1)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch, max_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "# encoder_outputs: [batch, L, encoder_dim] (flatten FCN output to [batch, L, 512])\n",
    "# targets: [batch, max_len] (token indices)\n",
    "# decoder = ParserGRUDecoder(vocab_size=len(vocab))\n",
    "# outputs = decoder(encoder_outputs, targets, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea65ca4",
   "metadata": {},
   "source": [
    "This is the main training code which we were using before. Now I am using a better code which is below this commented cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU Name: NVIDIA RTX A6000\n",
      "GPU Memory: 50.91 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  10%|██▊                        | 32/313 [01:02<09:05,  1.94s/it, loss=2.6135, gpu_mem=1.0GB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user\n",
      "Final model saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Training with GPU A6000 optimization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================\n",
    "# Training Configuration for A6000\n",
    "# ==============================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    # Enable TF32 for faster training on A6000\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    # Use cudnn benchmarking for optimal performance\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "num_epochs = 10\n",
    "max_len = 128\n",
    "best_loss = float('inf')\n",
    "\n",
    "watcher = WatcherFCN(in_channels=9).to(device)\n",
    "decoder = ParserGRUDecoder(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "pad_idx = vocab.index('<PAD>')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "# optimizer = optim.Adadelta(list(watcher.parameters()) + list(decoder.parameters()))\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    list(watcher.parameters()) + list(decoder.parameters()),\n",
    "    lr=1e-4,  # Much lower learning rate\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# FIXED: Better scheduler\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.1\n",
    ")\n",
    "\n",
    "# Mixed precision training for A6000\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# ==============================\n",
    "# Helper: Apply Weight Noise Regularization\n",
    "# ==============================\n",
    "# def apply_weight_noise(model, std=0.01):\n",
    "#     \"\"\"Adds Gaussian noise to model weights for regularization.\"\"\"\n",
    "#     with torch.no_grad():\n",
    "#         for p in model.parameters():\n",
    "#             if p.requires_grad:\n",
    "#                 p.add_(torch.randn_like(p) * std)\n",
    "\n",
    "################################## Modifying the code here. ##################################\n",
    "def apply_weight_noise(model, std=0.01):\n",
    "    \"\"\"Adds Gaussian noise to model weights for regularization.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.add_(torch.randn_like(p) * std)\n",
    "\n",
    "# ADDED: Quick validation function\n",
    "@torch.no_grad()\n",
    "def quick_validate(watcher, decoder, val_loader, num_samples=5):\n",
    "    \"\"\"Quick validation check during training\"\"\"\n",
    "    watcher.eval()\n",
    "    decoder.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        if total >= num_samples:\n",
    "            break\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            watcher_output = watcher(images)\n",
    "            batch_size, channels, height, width = watcher_output.shape\n",
    "            encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, height * width, channels\n",
    "            )\n",
    "            outputs = decoder(encoder_outputs, None, max_len=128)\n",
    "        \n",
    "        for i in range(min(len(outputs), num_samples - total)):\n",
    "            pred_indices = outputs[i].argmax(dim=-1)\n",
    "            pred_text = ''.join([idx2char[idx.item()] for idx in pred_indices \n",
    "                               if idx2char[idx.item()] not in ['<PAD>', '<SOS>', '<EOS>']])\n",
    "            true_text = labels[i]\n",
    "            \n",
    "            if pred_text == true_text:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "            if total <= 3:  # Print first 3 examples\n",
    "                print(f\"  Pred: {pred_text[:50]}...\")\n",
    "                print(f\"  True: {true_text[:50]}...\")\n",
    "                print(\"  \" + \"-\" * 40)\n",
    "    \n",
    "    watcher.train()\n",
    "    decoder.train()\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Create a small validation loader for quick checks\n",
    "VAL_CSV = os.path.join(DATASET_ROOT, 'val_database.csv')\n",
    "val_dataset_quick = MathEquation9ChDataset(\n",
    "    VAL_CSV, \n",
    "    DATASET_ROOT, \n",
    "    split='val',\n",
    "    max_samples=100  # Small subset for quick validation\n",
    ")\n",
    "val_loader_quick = DataLoader(\n",
    "    val_dataset_quick, \n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "# ==============================\n",
    "# Training Loop with GPU Optimization\n",
    "# ==============================\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        watcher.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # for batch in pbar:\n",
    "        #     # Non-blocking GPU transfer\n",
    "        #     images = batch['image'].to(device, non_blocking=True)\n",
    "        #     labels = [encode_label(lbl, max_len) for lbl in batch['label']]\n",
    "        #     labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "        #     optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "        #     try:\n",
    "        #         # Mixed precision training\n",
    "        #         with torch.cuda.amp.autocast():\n",
    "        #             watcher_output = watcher(images)\n",
    "        #             batch_size, channels, height, width = watcher_output.shape\n",
    "        #             encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "        #                 batch_size, height * width, channels\n",
    "        #             )\n",
    "\n",
    "        #             outputs = decoder(encoder_outputs, labels, max_len)\n",
    "        #             outputs = outputs.view(-1, outputs.size(-1))\n",
    "        #             labels_flat = labels.view(-1)\n",
    "\n",
    "        #             loss = criterion(outputs, labels_flat)\n",
    "\n",
    "        #         # Scaled backpropagation\n",
    "        #         scaler.scale(loss).backward()\n",
    "        #         scaler.unscale_(optimizer)\n",
    "        #         torch.nn.utils.clip_grad_norm_(\n",
    "        #             list(watcher.parameters()) + list(decoder.parameters()), \n",
    "        #             max_norm=5.0\n",
    "        #         )\n",
    "        #         scaler.step(optimizer)\n",
    "        #         scaler.update()\n",
    "\n",
    "        #         total_loss += loss.item()\n",
    "        #         batch_count += 1\n",
    "        #         pbar.set_postfix({\n",
    "        #             'loss': f'{loss.item():.4f}',\n",
    "        #             'gpu_mem': f'{torch.cuda.memory_allocated(0)/1e9:.1f}GB'\n",
    "        #         })\n",
    "                \n",
    "        #     except RuntimeError as e:\n",
    "        #         print(f\"Error in batch: {str(e)}\")\n",
    "        #         torch.cuda.empty_cache()\n",
    "        #         continue\n",
    "        ###################################### Modifying the code here. ######################################\n",
    "        for batch in pbar:\n",
    "            # Non-blocking GPU transfer\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            labels = [encode_label(lbl, max_len) for lbl in batch['label']]\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            try:\n",
    "                # Mixed precision training\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    watcher_output = watcher(images)\n",
    "                    batch_size, channels, height, width = watcher_output.shape\n",
    "                    encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "                        batch_size, height * width, channels\n",
    "                    )\n",
    "\n",
    "                    outputs = decoder(encoder_outputs, labels, max_len)\n",
    "                    outputs = outputs.view(-1, outputs.size(-1))\n",
    "                    labels_flat = labels.view(-1)\n",
    "\n",
    "                    loss = criterion(outputs, labels_flat)\n",
    "\n",
    "                # Scaled backpropagation\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                \n",
    "                # ADDED: Gradient monitoring\n",
    "                total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    list(watcher.parameters()) + list(decoder.parameters()), \n",
    "                    max_norm=1.0  # Reduced from 5.0 to 1.0\n",
    "                )\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()  # Step scheduler after each batch\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # ADDED: Better monitoring every 50 batches\n",
    "                if batch_count % 50 == 0:\n",
    "                    print(f\"\\nBatch {batch_count}, Loss: {loss.item():.4f}, Grad Norm: {total_norm:.4f}\")\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'grad_norm': f'{total_norm:.3f}',  # Added gradient norm\n",
    "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}',  # Added learning rate\n",
    "                    'gpu_mem': f'{torch.cuda.memory_allocated(0)/1e9:.1f}GB'\n",
    "                })\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {str(e)}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        avg_loss = total_loss / batch_count\n",
    "        print(f\"\\nEpoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB / {torch.cuda.memory_reserved(0)/1e9:.2f}GB\")\n",
    "\n",
    "        # Apply weight noise (annealing regularization)\n",
    "\n",
    "        ###########################Modifying the code here. ###########################\n",
    "        # apply_weight_noise(watcher, std=0.01)\n",
    "        # apply_weight_noise(decoder, std=0.01)\n",
    "        # ADDED: Quick validation check every 2 epochs\n",
    "        if epoch % 2 == 0:\n",
    "            print(f\"\\nQuick validation check (Epoch {epoch+1}):\")\n",
    "            val_acc = quick_validate(watcher, decoder, val_loader_quick, num_samples=10)\n",
    "            print(f\"Quick validation accuracy: {val_acc:.2%}\\n\")\n",
    "\n",
    "        #scheduler.step(avg_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'watcher_state_dict': watcher.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                 'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"Saved best model with loss: {best_loss:.4f}\")\n",
    "\n",
    "        # Clear GPU cache after each epoch\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    torch.save({\n",
    "        'watcher_state_dict': watcher.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss / batch_count if 'total_loss' in locals() and batch_count > 0 else None,\n",
    "    }, 'final_model.pth')\n",
    "    print(\"Final model saved.\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48364d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m decoder \u001b[38;5;241m=\u001b[39m ParserGRUDecoder(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vocab))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m watcher\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwatcher_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     13\u001b[0m decoder\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.10/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.10/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.10/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model.pth'"
     ]
    }
   ],
   "source": [
    "# Cell 5: Evaluation with SUBSET SAMPLING\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load models\n",
    "watcher = WatcherFCN(in_channels=9)\n",
    "decoder = ParserGRUDecoder(vocab_size=len(vocab))\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "watcher.load_state_dict(checkpoint['watcher_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "# Move to device and set to eval mode\n",
    "watcher = watcher.to(device)\n",
    "decoder = decoder.to(device)\n",
    "watcher.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Best training loss: {checkpoint['loss']:.4f}\")\n",
    "\n",
    "# Evaluation function with GPU optimization\n",
    "@torch.no_grad()\n",
    "def evaluate_model(watcher, decoder, data_loader, max_samples=50):\n",
    "    \"\"\"Evaluate model and show sample predictions\"\"\"\n",
    "    watcher.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    samples = []\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.cuda.amp.autocast():\n",
    "            watcher_output = watcher(images)\n",
    "            batch_size, channels, height, width = watcher_output.shape\n",
    "            encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, height * width, channels\n",
    "            )\n",
    "            \n",
    "            # Generate predictions (no teacher forcing)\n",
    "            outputs = decoder(encoder_outputs, None, max_len=128)\n",
    "        \n",
    "        # Convert to text\n",
    "        for i in range(len(outputs)):\n",
    "            pred_indices = outputs[i].argmax(dim=-1)\n",
    "            pred_text = ''.join([idx2char[idx.item()] for idx in pred_indices \n",
    "                               if idx2char[idx.item()] not in ['<PAD>', '<SOS>', '<EOS>']])\n",
    "            true_text = labels[i]\n",
    "            \n",
    "            is_correct = (pred_text == true_text)\n",
    "            total_correct += is_correct\n",
    "            total_samples += 1\n",
    "            \n",
    "            # Save samples\n",
    "            if len(samples) < max_samples:\n",
    "                samples.append({\n",
    "                    'pred': pred_text,\n",
    "                    'true': true_text,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if total_samples % 200 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if total_samples >= max_samples:\n",
    "            break\n",
    "    \n",
    "    # Print results\n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({total_correct}/{total_samples})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Print samples\n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        status = '✓' if sample['correct'] else '✗'\n",
    "        print(f\"Sample {i} {status}\")\n",
    "        print(f\"Predicted: {sample['pred']}\")\n",
    "        print(f\"True     : {sample['true']}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return accuracy, samples\n",
    "\n",
    "# Evaluate on validation set with SUBSET\n",
    "VAL_CSV = os.path.join(DATASET_ROOT, 'val_database.csv')\n",
    "val_dataset = MathEquation9ChDataset(\n",
    "    VAL_CSV, \n",
    "    DATASET_ROOT, \n",
    "    split='val',\n",
    "    max_samples=VAL_SAMPLES  # Use subset\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Validation dataset: {len(val_dataset):,} samples\")\n",
    "print(\"Evaluating on Validation Set...\")\n",
    "val_accuracy, val_samples = evaluate_model(watcher, decoder, val_loader, max_samples=50)\n",
    "\n",
    "# Evaluate on test set with SUBSET\n",
    "TEST_CSV = os.path.join(DATASET_ROOT, 'test_database.csv')\n",
    "test_dataset = MathEquation9ChDataset(\n",
    "    TEST_CSV, \n",
    "    DATASET_ROOT, \n",
    "    split='test',\n",
    "    max_samples=TEST_SAMPLES  # Use subset\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTest dataset: {len(test_dataset):,} samples\")\n",
    "print(\"Evaluating on Test Set...\")\n",
    "test_accuracy, test_samples = evaluate_model(watcher, decoder, test_loader, max_samples=50)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2%}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2%}\")\n",
    "print(f\"GPU Memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
