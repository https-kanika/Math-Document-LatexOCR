{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e441ed26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully\n",
      "Columns: ['image_path', 'sample_id', 'label', 'normalized_label', 'split', 'ink_creation_method', 'label_creation_method', 'original_path', 'is_symbol']\n",
      "\n",
      "First few image paths:\n",
      "0    train\\000aa4c444cba3f2.png\n",
      "1    train\\004970a2ad0fcb27.png\n",
      "2    train\\0050464363a7d02d.png\n",
      "3    train\\0053f4751a1d9065.png\n",
      "4    train\\005f0a6b379cc5db.png\n",
      "Name: image_path, dtype: object\n",
      "\n",
      "Batch loaded successfully\n",
      "Image tensor shape: torch.Size([8, 9, 480, 1600])\n",
      "First few labels: ['\\\\overline{K}_{n}', 'E\\\\mapsto\\\\int_{E}fd\\\\mu', 'C=P\\\\times\\\\frac{F_{L}}{F_{n}}']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "def get_directional_kernels():\n",
    "    # 8 edge detection kernels: N, NE, E, SE, S, SW, W, NW\n",
    "    k = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]) # Vertical\n",
    "    kernels = [\n",
    "        k,                              # S\n",
    "        np.rot90(k, 1),                 # W\n",
    "        np.rot90(k, 2),                 # N\n",
    "        np.rot90(k, 3),                 # E\n",
    "        np.fliplr(k),                   # SW\n",
    "        np.flipud(k),                   # NE\n",
    "        np.fliplr(np.rot90(k, 1)),      # NW\n",
    "        np.flipud(np.rot90(k, 3)),      # SE\n",
    "    ]\n",
    "    return kernels\n",
    "\n",
    "def get_directional_maps(image):\n",
    "    kernels = get_directional_kernels()\n",
    "    edge_maps = [cv2.filter2D(image, -1, kern) for kern in kernels]  # 8 edge maps\n",
    "    # Normalize each map to [0, 1] and clip negative values\n",
    "    edge_maps = [(em.astype(np.float32) / 255.0) for em in edge_maps]\n",
    "    edge_maps = [np.clip(em, 0, 1) for em in edge_maps]\n",
    "    return np.stack(edge_maps, axis=0)  # shape [8, H, W]\n",
    "\n",
    "class MathEquation9ChDataset(Dataset):\n",
    "    def __init__(self, csv_file, dataset_root, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.dataset_root = dataset_root\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Normalize image paths in the dataframe\n",
    "        self.data_frame['image_path'] = self.data_frame['image_path'].apply(\n",
    "            lambda x: os.path.normpath(x).replace('\\\\', '/')\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        relative_img_path = self.data_frame.iloc[idx]['image_path']\n",
    "        img_full_path = os.path.join(self.dataset_root, relative_img_path)\n",
    "        # Normalize the full path as well\n",
    "        img_full_path = os.path.normpath(img_full_path).replace('\\\\', '/')\n",
    "        \n",
    "        image = cv2.imread(img_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_full_path}\")\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        H, W = image.shape\n",
    "        # 9 channel construction\n",
    "        channels = np.zeros((9, H, W), dtype=np.float32)\n",
    "        channels[0] = image  # Greyscale base\n",
    "        channels[1:] = get_directional_maps(image)  # 8 directions\n",
    "        label = self.data_frame.iloc[idx]['normalized_label']\n",
    "        sample = {'image': torch.tensor(channels, dtype=torch.float32), 'label': label}\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        return sample\n",
    "\n",
    "# Usage:\n",
    "DATASET_ROOT = r'C:\\Users\\kani1\\Desktop\\IE643\\custom-dataset\\ProccessMathwritting-exercpt'\n",
    "TRAIN_CSV = os.path.join(DATASET_ROOT, 'train_database.csv')\n",
    "\n",
    "# Let's first check if the CSV exists and print its contents\n",
    "if os.path.exists(TRAIN_CSV):\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    print(\"CSV file loaded successfully\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst few image paths:\")\n",
    "    print(df['image_path'].head())\n",
    "else:\n",
    "    print(f\"CSV file not found at {TRAIN_CSV}\")\n",
    "\n",
    "train_dataset = MathEquation9ChDataset(TRAIN_CSV, DATASET_ROOT)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "try:\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        print(f\"\\nBatch loaded successfully\")\n",
    "        print(f\"Image tensor shape: {images.shape}\")\n",
    "        print(\"First few labels:\", labels[:3])\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading batch: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb249f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 91\n",
      "First 20 tokens: ['<PAD>', '<SOS>', '<EOS>', ' ', '!', '#', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4']\n",
      "Original label: \\vartheta=-\\frac{log\\frac{\\phi_{\\varsigma_{1}}}{\\phi_{\\varsigma_{2}}}}{log\\frac{\\varsigma_{1}}{\\varsigma_{2}}}\n",
      "Encoded: [1, 58, 83, 62, 79, 81, 69, 66, 81, 62, 28, 12, 58, 67, 79, 62, 64, 88, 73, 76]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load all labels from train/val/test CSVs\n",
    "csv_files = [\n",
    "    'train_database.csv',\n",
    "    'val_database.csv',\n",
    "    'test_database.csv'\n",
    "]\n",
    "DATASET_ROOT = r'C:\\Users\\kani1\\Desktop\\IE643\\custom-dataset\\ProccessMathwritting-exercpt'\n",
    "\n",
    "all_labels = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(DATASET_ROOT, csv_file))\n",
    "    all_labels.extend(df['normalized_label'].astype(str).tolist())\n",
    "\n",
    "# Build character-level vocabulary\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>']\n",
    "char_counter = Counter()\n",
    "for label in all_labels:\n",
    "    char_counter.update(list(label))\n",
    "\n",
    "vocab = special_tokens + sorted(char_counter.keys())\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"First 20 tokens:\", vocab[:20])\n",
    "\n",
    "# Encode a label string to indices\n",
    "def encode_label(label, max_len=128):\n",
    "    tokens = [char2idx['<SOS>']] + [char2idx[ch] for ch in label] + [char2idx['<EOS>']]\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [char2idx['<PAD>']] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sample_label = all_labels[0]\n",
    "encoded = encode_label(sample_label)\n",
    "print(\"Original label:\", sample_label)\n",
    "print(\"Encoded:\", encoded[:20])\n",
    "\n",
    "# For your dataset class, you can add:\n",
    "# label_indices = encode_label(label)\n",
    "# sample = {'image': image_tensor, 'label': label_indices}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc5b87e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Conv2d(\n",
    "                in_channels if i == 0 else out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class WatcherFCN(nn.Module):\n",
    "    def __init__(self, in_channels=9):\n",
    "        super().__init__()\n",
    "        self.block1 = ConvBlock(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.block2 = ConvBlock(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.block3 = ConvBlock(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.block4 = ConvBlock(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        # Output: [batch, 512, H/16, W/16]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool4(x)\n",
    "        return x  # [batch, 512, H/16, W/16]\n",
    "\n",
    "# Example usage:\n",
    "model = WatcherFCN(in_channels=9)\n",
    "dummy_input = torch.randn(2, 9, 480, 1600)\n",
    "output = model(dummy_input)\n",
    "print(output.shape)  # Should be [2, 512, 30, 100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b5b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, channels, height, width = output.shape\n",
    "encoder_outputs = output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "# encoder_outputs: [batch, 3000, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf4ab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoverageAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim, coverage_dim):\n",
    "        super().__init__()\n",
    "        self.W_a = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U_a = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.U_f = nn.Linear(coverage_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden, coverage):\n",
    "        # encoder_outputs: [batch, L, encoder_dim]\n",
    "        # decoder_hidden: [batch, decoder_dim]\n",
    "        # coverage: [batch, L, coverage_dim]\n",
    "        Wh = self.W_a(decoder_hidden).unsqueeze(1)  # [batch, 1, att_dim]\n",
    "        Ua = self.U_a(encoder_outputs)              # [batch, L, att_dim]\n",
    "        Uf = self.U_f(coverage)                     # [batch, L, att_dim]\n",
    "        att = torch.tanh(Wh + Ua + Uf)              # [batch, L, att_dim]\n",
    "        scores = self.v(att).squeeze(-1)            # [batch, L]\n",
    "        alpha = F.softmax(scores, dim=1)            # [batch, L]\n",
    "        context = torch.sum(encoder_outputs * alpha.unsqueeze(-1), dim=1)  # [batch, encoder_dim]\n",
    "        return context, alpha\n",
    "\n",
    "class ParserGRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_dim=512, embed_dim=256, decoder_dim=256, attention_dim=256, coverage_dim=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "        self.fc = nn.Linear(decoder_dim + encoder_dim, vocab_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, targets, max_len):\n",
    "        batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "        device = encoder_outputs.device\n",
    "        coverage = torch.zeros(batch_size, L, 1, device=device)\n",
    "        inputs = torch.full((batch_size,), 1, dtype=torch.long, device=device)  # <SOS> token index\n",
    "        hidden = torch.zeros(batch_size, 256, device=device)\n",
    "        outputs = []\n",
    "        for t in range(max_len):\n",
    "            embedded = self.embedding(inputs)  # [batch, embed_dim]\n",
    "            context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "            gru_input = torch.cat([embedded, context], dim=1)\n",
    "            hidden = self.gru(gru_input, hidden)\n",
    "            output = self.fc(torch.cat([hidden, context], dim=1))\n",
    "            outputs.append(output)\n",
    "            # Teacher forcing: use ground truth if available\n",
    "            if targets is not None and t < targets.size(1):\n",
    "                inputs = targets[:, t]\n",
    "            else:\n",
    "                inputs = output.argmax(dim=1)\n",
    "            coverage = coverage + alpha.unsqueeze(-1)\n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch, max_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "# encoder_outputs: [batch, L, encoder_dim] (flatten FCN output to [batch, L, 512])\n",
    "# targets: [batch, max_len] (token indices)\n",
    "# decoder = ParserGRUDecoder(vocab_size=len(vocab))\n",
    "# outputs = decoder(encoder_outputs, targets, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 13/13 [1:28:48<00:00, 409.90s/it, loss=3.6374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1, Average Loss: 3.6220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 13/13 [1:13:35<00:00, 339.63s/it, loss=2.5843]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2, Average Loss: 2.9170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:  54%|█████▍    | 7/13 [41:10<35:11, 351.87s/it, loss=2.5177]  "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize models with proper configuration\n",
    "watcher = WatcherFCN(in_channels=9)  # 9-channel input as defined in dataset\n",
    "decoder = ParserGRUDecoder(vocab_size=len(vocab))  # vocab was defined in previous cell\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move models to device\n",
    "watcher = watcher.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "pad_idx = vocab.index('<PAD>')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adadelta(list(watcher.parameters()) + list(decoder.parameters()))\n",
    "\n",
    "num_epochs = 5\n",
    "max_len = 128\n",
    "\n",
    "# Learning rate scheduler for better convergence\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Rest of the training code remains the same...\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        watcher.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Add progress bar\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            # Move batch to device\n",
    "            images = batch['image'].to(device)\n",
    "            labels = [encode_label(lbl, max_len) for lbl in batch['label']]\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "            \n",
    "            try:\n",
    "                watcher_output = watcher(images)\n",
    "                batch_size, channels, height, width = watcher_output.shape\n",
    "                encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "\n",
    "                outputs = decoder(encoder_outputs, labels, max_len)\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(list(watcher.parameters()) + list(decoder.parameters()), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Update metrics\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average loss\n",
    "        avg_loss = total_loss / batch_count\n",
    "        print(f\"\\nEpoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'watcher_state_dict': watcher.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_model.pth')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {str(e)}\")\n",
    "finally:\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'watcher_state_dict': watcher.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss / len(train_loader) if 'total_loss' in locals() else None,\n",
    "    }, 'final_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
