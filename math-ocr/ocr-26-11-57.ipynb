{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913f518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file loaded successfully\n",
      "Columns: ['image_path', 'sample_id', 'label', 'normalized_label', 'split', 'ink_creation_method', 'label_creation_method', 'original_path', 'is_symbol']\n",
      "\n",
      "First few image paths:\n",
      "0    train\\000aa4c444cba3f2.png\n",
      "1    train\\004970a2ad0fcb27.png\n",
      "2    train\\0050464363a7d02d.png\n",
      "3    train\\0053f4751a1d9065.png\n",
      "4    train\\005f0a6b379cc5db.png\n",
      "Name: image_path, dtype: object\n",
      "\n",
      "Batch loaded successfully\n",
      "Image tensor shape: torch.Size([8, 9, 480, 1600])\n",
      "First few labels: ['\\\\vec{p}_{0}=(0,1,0)', '\\\\underline{P}X=\\\\emptyset', '\\\\Rightarrow ln\\\\frac{x(t)}{x(0)}=kt']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "def get_directional_kernels():\n",
    "    # 8 edge detection kernels: N, NE, E, SE, S, SW, W, NW\n",
    "    k = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]) # Vertical\n",
    "    kernels = [\n",
    "        k,                              # S\n",
    "        np.rot90(k, 1),                 # W\n",
    "        np.rot90(k, 2),                 # N\n",
    "        np.rot90(k, 3),                 # E\n",
    "        np.fliplr(k),                   # SW\n",
    "        np.flipud(k),                   # NE\n",
    "        np.fliplr(np.rot90(k, 1)),      # NW\n",
    "        np.flipud(np.rot90(k, 3)),      # SE\n",
    "    ]\n",
    "    return kernels\n",
    "\n",
    "def get_directional_maps(image):\n",
    "    kernels = get_directional_kernels()\n",
    "    edge_maps = []\n",
    "    for kern in kernels:    \n",
    "        em = cv2.filter2D(image, cv2.CV_32F, kern, borderType=cv2.BORDER_REPLICATE)\n",
    "        em = np.abs(em)\n",
    "        maxv = em.max()\n",
    "        if maxv > 1e-8:\n",
    "            em = em / maxv \n",
    "        else:\n",
    "            em = np.zeros_like(em, dtype=np.float32)\n",
    "        \n",
    "        edge_maps.append(em.astype(np.float32))\n",
    "    return np.stack(edge_maps, axis=0)\n",
    "\n",
    "class MathEquation9ChDataset(Dataset):\n",
    "    def __init__(self, csv_file, dataset_root, transform=None):\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.dataset_root = dataset_root\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Normalize image paths in the dataframe\n",
    "        self.data_frame['image_path'] = self.data_frame['image_path'].apply(\n",
    "            lambda x: os.path.normpath(x).replace('\\\\', '/')\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        relative_img_path = self.data_frame.iloc[idx]['image_path']\n",
    "        img_full_path = os.path.join(self.dataset_root, relative_img_path)\n",
    "        # Normalize the full path as well\n",
    "        img_full_path = os.path.normpath(img_full_path).replace('\\\\', '/')\n",
    "        \n",
    "        image = cv2.imread(img_full_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_full_path}\")\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        H, W = image.shape\n",
    "        #print(H, W)\n",
    "        # 9 channel construction\n",
    "        channels = np.zeros((9, H, W), dtype=np.float32)\n",
    "        channels[0] = image  # Greyscale base\n",
    "        channels[1:] = get_directional_maps(image)  # 8 directions\n",
    "        label = self.data_frame.iloc[idx]['normalized_label']\n",
    "        sample = {'image': torch.tensor(channels, dtype=torch.float32), 'label': label}\n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        return sample\n",
    "\n",
    "# Usage:\n",
    "DATASET_ROOT = '/Users/parvjain/Downloads/ProccessMathwritting-exercpt'\n",
    "TRAIN_CSV = os.path.join(DATASET_ROOT, 'train_database.csv')\n",
    "\n",
    "# Let's first check if the CSV exists and print its contents\n",
    "if os.path.exists(TRAIN_CSV):\n",
    "    df = pd.read_csv(TRAIN_CSV)\n",
    "    print(\"CSV file loaded successfully\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"\\nFirst few image paths:\")\n",
    "    print(df['image_path'].head())\n",
    "else:\n",
    "    print(f\"CSV file not found at {TRAIN_CSV}\")\n",
    "\n",
    "train_dataset = MathEquation9ChDataset(TRAIN_CSV, DATASET_ROOT)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "try:\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        print(f\"\\nBatch loaded successfully\")\n",
    "        print(f\"Image tensor shape: {images.shape}\")\n",
    "        print(\"First few labels:\", labels[:3])\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading batch: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "699bf695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 91\n",
      "First 20 tokens: ['<PAD>', '<SOS>', '<EOS>', ' ', '!', '#', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4']\n",
      "Original label: \\vartheta=-\\frac{log\\frac{\\phi_{\\varsigma_{1}}}{\\phi_{\\varsigma_{2}}}}{log\\frac{\\varsigma_{1}}{\\varsigma_{2}}}\n",
      "Encoded: [1, 58, 83, 62, 79, 81, 69, 66, 81, 62, 28, 12, 58, 67, 79, 62, 64, 88, 73, 76]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load all labels from train/val/test CSVs\n",
    "csv_files = [\n",
    "    'train_database.csv',\n",
    "    'val_database.csv',\n",
    "    'test_database.csv'\n",
    "]\n",
    "DATASET_ROOT = '/Users/parvjain/Downloads/ProccessMathwritting-exercpt'\n",
    "\n",
    "all_labels = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(DATASET_ROOT, csv_file))\n",
    "    all_labels.extend(df['normalized_label'].astype(str).tolist())\n",
    "\n",
    "# Build character-level vocabulary\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>']\n",
    "char_counter = Counter()\n",
    "for label in all_labels:\n",
    "    char_counter.update(list(label))\n",
    "\n",
    "vocab = special_tokens + sorted(char_counter.keys())\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"First 20 tokens:\", vocab[:20])\n",
    "\n",
    "# Encode a label string to indices\n",
    "def encode_label(label, max_len=128):\n",
    "    tokens = [char2idx['<SOS>']] + [char2idx[ch] for ch in label] + [char2idx['<EOS>']]\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [char2idx['<PAD>']] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sample_label = all_labels[0]\n",
    "encoded = encode_label(sample_label)\n",
    "print(\"Original label:\", sample_label)\n",
    "print(\"Encoded:\", encoded[:20])\n",
    "\n",
    "# For your dataset class, you can add:\n",
    "# label_indices = encode_label(label)\n",
    "# sample = {'image': image_tensor, 'label': label_indices}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b9403e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=4, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Conv2d(\n",
    "                in_channels if i == 0 else out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout_p > 0:\n",
    "                layers.append(nn.Dropout2d(p=dropout_p))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class WatcherFCN(nn.Module):\n",
    "    def __init__(self, in_channels=9):\n",
    "        super().__init__()\n",
    "        # First blocks without dropout\n",
    "        self.block1 = ConvBlock(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.block2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        # Last blocks with 20% dropout\n",
    "        self.block3 = ConvBlock(64, 64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.block4 = ConvBlock(64, 128, dropout_p=0.2)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool4(x)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "model = WatcherFCN(in_channels=9)\n",
    "dummy_input = torch.randn(2, 9, 480, 1600)\n",
    "output = model(dummy_input)\n",
    "print(output.shape) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f9a66471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3000, 128])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, channels, height, width = output.shape\n",
    "encoder_outputs = output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "# encoder_outputs: [batch, 3000, 512]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a64fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoverageAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim, coverage_dim):\n",
    "        super().__init__()\n",
    "        self.W_a = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U_a = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.U_f = nn.Linear(coverage_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden, coverage):\n",
    "        # encoder_outputs: [batch, L, encoder_dim]\n",
    "        # decoder_hidden: [batch, decoder_dim]\n",
    "        # coverage: [batch, L, coverage_dim]\n",
    "        Wh = self.W_a(decoder_hidden).unsqueeze(1)  # [batch, 1, att_dim]\n",
    "        Ua = self.U_a(encoder_outputs)              # [batch, L, att_dim]\n",
    "        Uf = self.U_f(coverage)                     # [batch, L, att_dim]\n",
    "        att = torch.tanh(Wh + Ua + Uf)              # [batch, L, att_dim]\n",
    "        scores = self.v(att).squeeze(-1)            # [batch, L]\n",
    "        alpha = F.softmax(scores, dim=1)            # [batch, L]\n",
    "        context = torch.sum(encoder_outputs * alpha.unsqueeze(-1), dim=1)  # [batch, encoder_dim]\n",
    "        return context, alpha\n",
    "\n",
    "# class ParserGRUDecoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, encoder_dim=128, embed_dim=256, decoder_dim=256, attention_dim=256, coverage_dim=1):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "#         self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "#         self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "#         self.fc = nn.Linear(decoder_dim + encoder_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, encoder_outputs, targets, max_len):\n",
    "#         batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "#         device = encoder_outputs.device\n",
    "#         coverage = torch.zeros(batch_size, L, 1, device=device)\n",
    "#         inputs = torch.full((batch_size,), 1, dtype=torch.long, device=device)  # <SOS> token index\n",
    "#         hidden = torch.zeros(batch_size, 256, device=device)\n",
    "#         outputs = []\n",
    "#         for t in range(max_len):\n",
    "#             embedded = self.embedding(inputs)  # [batch, embed_dim]\n",
    "#             context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "#             gru_input = torch.cat([embedded, context], dim=1)\n",
    "#             hidden = self.gru(gru_input, hidden)\n",
    "#             output = self.fc(torch.cat([hidden, context], dim=1))\n",
    "#             outputs.append(output)\n",
    "#             # Teacher forcing: use ground truth if available\n",
    "#             if targets is not None and t < targets.size(1):\n",
    "#                 inputs = targets[:, t]\n",
    "#             else:\n",
    "#                 inputs = output.argmax(dim=1)\n",
    "#             coverage = coverage + alpha.unsqueeze(-1)\n",
    "#         outputs = torch.stack(outputs, dim=1)  # [batch, max_len, vocab_size]\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "#Modified ParserGRUDecoder\n",
    "\n",
    "class ParserGRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_dim=128, embed_dim=256, decoder_dim=256, attention_dim=256, coverage_dim=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Modify input size to include context vector\n",
    "        self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "        # Change output layer to use all available information\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(decoder_dim + encoder_dim + embed_dim, decoder_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(decoder_dim, vocab_size)\n",
    "        )\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "    def forward(self, encoder_outputs, targets, max_len):\n",
    "        batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "        device = encoder_outputs.device\n",
    "        coverage = torch.zeros(batch_size, L, 1, device=device)\n",
    "        inputs = torch.full((batch_size,), 1, dtype=torch.long, device=device)  # <SOS> token index\n",
    "        hidden = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(max_len):\n",
    "            # 1. Get current input embedding\n",
    "            embedded = self.embedding(inputs)  # [batch, embed_dim]\n",
    "            \n",
    "            # 2. Calculate attention and context\n",
    "            context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "            \n",
    "            # 3. Update GRU hidden state with concatenated input\n",
    "            gru_input = torch.cat([embedded, context], dim=1)\n",
    "            hidden = self.gru(gru_input, hidden)\n",
    "            \n",
    "            # 4. Generate output using all available information\n",
    "            # Concatenate current embedding, hidden state, and context\n",
    "            output = self.out(torch.cat([embedded, hidden, context], dim=1))\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # 5. Teacher forcing or use own predictions\n",
    "            if targets is not None and t < targets.size(1):\n",
    "                inputs = targets[:, t]\n",
    "            else:\n",
    "                inputs = output.argmax(dim=1)\n",
    "            \n",
    "            # 6. Update coverage vector\n",
    "            coverage = coverage + alpha.unsqueeze(-1)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch, max_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "# encoder_outputs: [batch, L, encoder_dim] (flatten FCN output to [batch, L, 512])\n",
    "# targets: [batch, max_len] (token indices)\n",
    "# decoder = ParserGRUDecoder(vocab_size=len(vocab))\n",
    "# outputs = decoder(encoder_outputs, targets, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea65ca4",
   "metadata": {},
   "source": [
    "This is the main training code which we were using before. Now I am using a better code which is below this commented cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606cf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/13 [00:06<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Initialize models with proper configuration\n",
    "# watcher = WatcherFCN(in_channels=9)  # 9-channel input as defined in dataset\n",
    "# decoder = ParserGRUDecoder(vocab_size=len(vocab))  # vocab was defined in previous cell\n",
    "\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Move models to device\n",
    "# watcher = watcher.to(device)\n",
    "# decoder = decoder.to(device)\n",
    "\n",
    "# pad_idx = vocab.index('<PAD>')\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "# optimizer = optim.Adadelta(list(watcher.parameters()) + list(decoder.parameters()))\n",
    "\n",
    "# num_epochs = 10\n",
    "# max_len = 128\n",
    "\n",
    "# # Learning rate scheduler for better convergence\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "# best_loss = float('inf')\n",
    "\n",
    "# # Rest of the training code remains the same...\n",
    "\n",
    "# try:\n",
    "#     for epoch in range(num_epochs):\n",
    "#         watcher.train()\n",
    "#         decoder.train()\n",
    "#         total_loss = 0\n",
    "#         batch_count = 0\n",
    "        \n",
    "#         # Add progress bar\n",
    "#         pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "#         for batch in pbar:\n",
    "#             # Move batch to device\n",
    "#             images = batch['image'].to(device)\n",
    "#             labels = [encode_label(lbl, max_len) for lbl in batch['label']]\n",
    "#             labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "#             optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "            \n",
    "#             try:\n",
    "#                 watcher_output = watcher(images)\n",
    "#                 batch_size, channels, height, width = watcher_output.shape\n",
    "#                 encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "\n",
    "#                 outputs = decoder(encoder_outputs, labels, max_len)\n",
    "#                 outputs = outputs.view(-1, outputs.size(-1))\n",
    "#                 labels = labels.view(-1)\n",
    "\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "                \n",
    "#                 # Gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(list(watcher.parameters()) + list(decoder.parameters()), max_norm=5.0)\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 # Update metrics\n",
    "#                 total_loss += loss.item()\n",
    "#                 batch_count += 1\n",
    "                \n",
    "#                 # Update progress bar\n",
    "#                 pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "#             except RuntimeError as e:\n",
    "#                 print(f\"Error in batch: {str(e)}\")\n",
    "#                 continue\n",
    "\n",
    "#         # Calculate average loss\n",
    "#         avg_loss = total_loss / batch_count\n",
    "#         print(f\"\\nEpoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "#         # Learning rate scheduling\n",
    "#         scheduler.step(avg_loss)\n",
    "        \n",
    "#         # Save best model\n",
    "#         if avg_loss < best_loss:\n",
    "#             best_loss = avg_loss\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'watcher_state_dict': watcher.state_dict(),\n",
    "#                 'decoder_state_dict': decoder.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss': best_loss,\n",
    "#             }, 'best_model.pth')\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nTraining interrupted by user\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nError during training: {str(e)}\")\n",
    "# finally:\n",
    "#     # Save final model\n",
    "#     torch.save({\n",
    "#         'watcher_state_dict': watcher.state_dict(),\n",
    "#         'decoder_state_dict': decoder.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'loss': total_loss / len(train_loader) if 'total_loss' in locals() else None,\n",
    "#     }, 'final_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "768d234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   8%|▊         | 1/13 [00:08<01:36,  8.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch: MPS backend out of memory (MPS allocated: 8.28 GB, other allocations: 750.66 MB, max allowed: 9.07 GB). Tried to allocate 375.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  15%|█▌        | 2/13 [00:10<00:53,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch: MPS backend out of memory (MPS allocated: 8.28 GB, other allocations: 750.66 MB, max allowed: 9.07 GB). Tried to allocate 375.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  23%|██▎       | 3/13 [00:12<00:36,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch: MPS backend out of memory (MPS allocated: 8.28 GB, other allocations: 750.66 MB, max allowed: 9.07 GB). Tried to allocate 375.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  31%|███       | 4/13 [00:16<00:32,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch: MPS backend out of memory (MPS allocated: 8.28 GB, other allocations: 750.66 MB, max allowed: 9.07 GB). Tried to allocate 375.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  31%|███       | 4/13 [00:17<00:38,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user\n",
      "Final model saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================\n",
    "# Training Configuration\n",
    "# ==============================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() \n",
    "                      else 'mps' if torch.backends.mps.is_available() \n",
    "                      else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "watcher = WatcherFCN(in_channels=9).to(device)\n",
    "decoder = ParserGRUDecoder(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "pad_idx = vocab.index('<PAD>')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adadelta(list(watcher.parameters()) + list(decoder.parameters()))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "\n",
    "num_epochs = 10\n",
    "max_len = 128\n",
    "best_loss = float('inf')\n",
    "\n",
    "# ==============================\n",
    "# Helper: Apply Weight Noise Regularization\n",
    "# ==============================\n",
    "def apply_weight_noise(model, std=0.01):\n",
    "    \"\"\"Adds Gaussian noise to model weights for regularization.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.add_(torch.randn_like(p) * std)\n",
    "\n",
    "# ==============================\n",
    "# Training Loop\n",
    "# ==============================\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        watcher.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch in pbar:\n",
    "            images = batch['image'].to(device)\n",
    "            labels = [encode_label(lbl, max_len) for lbl in batch['label']]\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            try:\n",
    "                watcher_output = watcher(images)\n",
    "                batch_size, channels, height, width = watcher_output.shape\n",
    "                encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "\n",
    "                outputs = decoder(encoder_outputs, labels, max_len)\n",
    "                outputs = outputs.view(-1, outputs.size(-1))\n",
    "                labels = labels.view(-1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(list(watcher.parameters()) + list(decoder.parameters()), max_norm=5.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"Error in batch: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        avg_loss = total_loss / batch_count\n",
    "        print(f\"\\nEpoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Apply weight noise (annealing regularization)\n",
    "        apply_weight_noise(watcher, std=0.01)\n",
    "        apply_weight_noise(decoder, std=0.01)\n",
    "\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'watcher_state_dict': watcher.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_model.pth')\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError during training: {str(e)}\")\n",
    "finally:\n",
    "    torch.save({\n",
    "        'watcher_state_dict': watcher.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss / len(train_loader) if 'total_loss' in locals() else None,\n",
    "    }, 'final_model.pth')\n",
    "    print(\"Final model saved.\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Beam Search Decoding\n",
    "# ==============================\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(watcher, decoder, image, beam_width=10, max_len=128):\n",
    "    \"\"\"\n",
    "    Performs beam search decoding as per paper:\n",
    "    - Start from <SOS>\n",
    "    - Expand top-k hypotheses at each step\n",
    "    - Stop at <EOS>\n",
    "    \"\"\"\n",
    "    watcher.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    # Encode image\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    encoder_out = watcher(image)\n",
    "    batch_size, channels, height, width = encoder_out.shape\n",
    "    encoder_outputs = encoder_out.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "\n",
    "    # Initial setup\n",
    "    start_token = torch.tensor([vocab.index('<SOS>')], device=device)\n",
    "    end_token = vocab.index('<EOS>')\n",
    "    coverage = torch.zeros(1, encoder_outputs.size(1), 1, device=device)\n",
    "    hidden = torch.zeros(1, 256, device=device)\n",
    "\n",
    "    beams = [(start_token, hidden, 0.0, coverage)]  # (sequence, hidden, score, coverage)\n",
    "    completed = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, h, score, cov in beams:\n",
    "            if seq[-1].item() == end_token:\n",
    "                completed.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            embedded = decoder.embedding(seq[-1].unsqueeze(0))\n",
    "            context, alpha = decoder.attention(encoder_outputs, h, cov)\n",
    "            gru_input = torch.cat([embedded, context], dim=1)\n",
    "            new_hidden = decoder.gru(gru_input, h)\n",
    "            output = decoder.fc(torch.cat([new_hidden, context], dim=1))\n",
    "            log_probs = F.log_softmax(output, dim=1)\n",
    "\n",
    "            topk_probs, topk_idx = log_probs.topk(beam_width, dim=1)\n",
    "            for k in range(beam_width):\n",
    "                next_seq = torch.cat([seq, topk_idx[0, k].unsqueeze(0)])\n",
    "                new_beams.append((next_seq, new_hidden, score + topk_probs[0, k].item(), cov + alpha.unsqueeze(-1)))\n",
    "\n",
    "        # Keep top-k beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[2], reverse=True)[:beam_width]\n",
    "\n",
    "        # If all beams ended\n",
    "        if all(seq[-1].item() == end_token for seq, _, _, _ in beams):\n",
    "            break\n",
    "\n",
    "    if not completed:\n",
    "        completed = beams\n",
    "\n",
    "    best_seq = max(completed, key=lambda x: x[1])[0]\n",
    "    return [vocab[idx.item()] for idx in best_seq if idx.item() not in (start_token, end_token)]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# image = dataset[0]['image']\n",
    "# predicted_seq = beam_search_decode(watcher, decoder, image)\n",
    "# print(\"Predicted LaTeX:\", \" \".join(predicted_seq))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea35d39",
   "metadata": {},
   "source": [
    "---------------------------------------------Kana TRY TO RUN TILL HERE-------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152b2a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   8%|▊         | 1/13 [00:03<00:36,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 0: MPS backend out of memory (MPS allocated: 6.88 GB, other allocations: 1.47 GB, max allowed: 9.07 GB). Tried to allocate 1.46 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  15%|█▌        | 2/13 [00:05<00:31,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 1: MPS backend out of memory (MPS allocated: 6.88 GB, other allocations: 1.47 GB, max allowed: 9.07 GB). Tried to allocate 1.46 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  23%|██▎       | 3/13 [00:07<00:25,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 2: MPS backend out of memory (MPS allocated: 6.88 GB, other allocations: 1.47 GB, max allowed: 9.07 GB). Tried to allocate 1.46 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  31%|███       | 4/13 [00:10<00:23,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 3: MPS backend out of memory (MPS allocated: 6.88 GB, other allocations: 1.47 GB, max allowed: 9.07 GB). Tried to allocate 1.46 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  38%|███▊      | 5/13 [00:14<00:24,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 4: MPS backend out of memory (MPS allocated: 6.88 GB, other allocations: 1.47 GB, max allowed: 9.07 GB). Tried to allocate 1.46 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:  38%|███▊      | 5/13 [00:19<00:31,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user\n",
      "Saved final model to checkpoints/model_final_20251022_161245.pth\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "# from tqdm import tqdm\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Initialize models with proper configuration\n",
    "# watcher = WatcherFCN(in_channels=9)\n",
    "# decoder = ParserGRUDecoder(vocab_size=len(vocab))\n",
    "\n",
    "# # Device configuration\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "# # Move models to device\n",
    "# watcher = watcher.to(device)\n",
    "# decoder = decoder.to(device)\n",
    "\n",
    "# # Training configuration\n",
    "# pad_idx = vocab.index('<PAD>')\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "# optimizer = optim.Adadelta(list(watcher.parameters()) + list(decoder.parameters()), rho=0.95)\n",
    "\n",
    "# num_epochs = 10\n",
    "# max_len = 128\n",
    "# save_dir = 'checkpoints'\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Learning rate scheduler (without verbose parameter)\n",
    "# scheduler = ReduceLROnPlateau(\n",
    "#     optimizer,\n",
    "#     mode='min',\n",
    "#     patience=2,\n",
    "#     factor=0.5,\n",
    "#     min_lr=1e-6\n",
    "# )\n",
    "# best_loss = float('inf')\n",
    "\n",
    "# try:\n",
    "#     for epoch in range(num_epochs):\n",
    "#         watcher.train()\n",
    "#         decoder.train()\n",
    "#         total_loss = 0\n",
    "#         batch_count = 0\n",
    "        \n",
    "#         pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "#         for batch_idx, batch in enumerate(pbar):\n",
    "#             images = batch['image'].to(device)\n",
    "#             labels = [encode_label(lbl, max_len) for lbl in batch['label']]\n",
    "#             labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "#             try:\n",
    "#                 # Forward pass\n",
    "#                 watcher_output = watcher(images)\n",
    "#                 batch_size, channels, height, width = watcher_output.shape\n",
    "#                 encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "#                     batch_size, height * width, channels\n",
    "#                 )\n",
    "\n",
    "#                 outputs = decoder(encoder_outputs, labels, max_len)\n",
    "#                 outputs = outputs.view(-1, outputs.size(-1))\n",
    "#                 labels = labels.view(-1)\n",
    "\n",
    "#                 # Calculate loss and backprop\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "                \n",
    "#                 # Gradient clipping\n",
    "#                 torch.nn.utils.clip_grad_norm_(\n",
    "#                     list(watcher.parameters()) + list(decoder.parameters()), \n",
    "#                     max_norm=5.0\n",
    "#                 )\n",
    "#                 optimizer.step()\n",
    "\n",
    "#                 # Update metrics\n",
    "#                 total_loss += loss.item()\n",
    "#                 batch_count += 1\n",
    "                \n",
    "#                 # Update progress bar with more metrics\n",
    "#                 pbar.set_postfix({\n",
    "#                     'loss': f'{loss.item():.4f}',\n",
    "#                     'avg_loss': f'{total_loss/batch_count:.4f}',\n",
    "#                     'lr': f'{optimizer.param_groups[0][\"lr\"]:.6f}'\n",
    "#                 })\n",
    "                \n",
    "#             except RuntimeError as e:\n",
    "#                 print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "#                 continue\n",
    "\n",
    "#         # Calculate average loss\n",
    "#         avg_loss = total_loss / batch_count\n",
    "        \n",
    "#         # Track learning rate changes\n",
    "#         old_lr = optimizer.param_groups[0]['lr']\n",
    "#         scheduler.step(avg_loss)\n",
    "#         new_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "#         print(f\"\\nEpoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "#         if old_lr != new_lr:\n",
    "#             print(f\"Learning rate changed: {old_lr:.2e} -> {new_lr:.2e}\")\n",
    "        \n",
    "#         # Save best model with timestamp\n",
    "#         if avg_loss < best_loss:\n",
    "#             best_loss = avg_loss\n",
    "#             timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#             save_path = os.path.join(save_dir, f'model_epoch{epoch+1}_{timestamp}.pth')\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'watcher_state_dict': watcher.state_dict(),\n",
    "#                 'decoder_state_dict': decoder.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'scheduler_state_dict': scheduler.state_dict(),\n",
    "#                 'loss': best_loss,\n",
    "#                 'vocab': vocab,\n",
    "#                 'config': {\n",
    "#                     'max_len': max_len,\n",
    "#                     'vocab_size': len(vocab)\n",
    "#                 }\n",
    "#             }, save_path)\n",
    "#             print(f\"Saved best model to {save_path}\")\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\nTraining interrupted by user\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nError during training: {str(e)}\")\n",
    "# finally:\n",
    "#     # Save final model\n",
    "#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#     final_path = os.path.join(save_dir, f'model_final_{timestamp}.pth')\n",
    "#     torch.save({\n",
    "#         'watcher_state_dict': watcher.state_dict(),\n",
    "#         'decoder_state_dict': decoder.state_dict(),\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'scheduler_state_dict': scheduler.state_dict(),\n",
    "#         'loss': total_loss / len(train_loader) if 'total_loss' in locals() else None,\n",
    "#         'vocab': vocab,\n",
    "#         'config': {\n",
    "#             'max_len': max_len,\n",
    "#             'vocab_size': len(vocab)\n",
    "#         }\n",
    "#     }, final_path)\n",
    "#     print(f\"Saved final model to {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92ca30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParserGRUDecoder(\n",
       "  (embedding): Embedding(91, 256)\n",
       "  (gru): GRUCell(768, 256)\n",
       "  (attention): CoverageAttention(\n",
       "    (W_a): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (U_a): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (U_f): Linear(in_features=1, out_features=256, bias=True)\n",
       "    (v): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=91, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Initialize models with same architecture\n",
    "watcher = WatcherFCN(in_channels=9)  # 9-channel input as per your dataset\n",
    "decoder = ParserGRUDecoder(vocab_size=len(vocab))  # Using vocab from previous setup\n",
    "\n",
    "# Device configuration - use CPU if memory is constrained\n",
    "device = torch.device('cpu')  # Change to 'cuda' or 'mps' if you have enough memory\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the pre-trained weights\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "watcher.load_state_dict(checkpoint['watcher_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "# Set models to evaluation mode\n",
    "watcher.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9224c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(watcher, decoder, test_csv, batch_size=4):\n",
    "    # Load test dataset\n",
    "    test_dataset = MathEquation9ChDataset(test_csv, DATASET_ROOT)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation for inference\n",
    "        for batch in tqdm(test_loader, desc='Testing'):\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label']\n",
    "            \n",
    "            # Forward pass through watcher\n",
    "            watcher_output = watcher(images)\n",
    "            batch_size, channels, height, width = watcher_output.shape\n",
    "            encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, height * width, channels\n",
    "            )\n",
    "            \n",
    "            # Decode without teacher forcing\n",
    "            outputs = decoder(encoder_outputs, None, max_len=128)\n",
    "            \n",
    "            # Convert outputs to text\n",
    "            predictions = decode_predictions(outputs, idx2char)\n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(labels)\n",
    "            \n",
    "            # Free up memory\n",
    "            del watcher_output, encoder_outputs, outputs\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return all_predictions, all_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af26a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 25/25 [08:07<00:00, 19.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "Total samples: 100\n",
      "Correct predictions: 0\n",
      "Accuracy: 0.0000\n",
      "\n",
      "Sample Predictions:\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : (\\begin{matrix}j\\\\ m+1\\end{matrix})\n",
      "\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : \\int_{E}f(x)\\mu(dx)\n",
      "\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : \\overline{z}=1-i\\sqrt{\\nu}\n",
      "\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : S=\\int_{M}BF\n",
      "\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : Q:=\\bigcup_{\\sigma\\in\\sigma(T_{0})}Q_{\\sigma}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test on validation set\n",
    "VAL_CSV = os.path.join(DATASET_ROOT, 'val_database.csv')\n",
    "predictions, targets = test_model(watcher, decoder, VAL_CSV, batch_size=4)\n",
    "\n",
    "# Calculate metrics\n",
    "correct = sum(1 for p, t in zip(predictions, targets) if p == t)\n",
    "accuracy = correct / len(predictions)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"Total samples: {len(predictions)}\")\n",
    "print(f\"Correct predictions: {correct}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Print sample predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "for pred, target in zip(predictions[:5], targets[:5]):\n",
    "    print(f\"Predicted: {pred}\")\n",
    "    print(f\"Target   : {target}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4976e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error Analysis:\n",
      "Total errors: 100\n",
      "\n",
      "Sample errors:\n",
      "\n",
      "Index: 0\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : (\\begin{matrix}j\\\\ m+1\\end{matrix})\n",
      "Length difference: 92\n",
      "\n",
      "Index: 1\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : \\int_{E}f(x)\\mu(dx)\n",
      "Length difference: 108\n",
      "\n",
      "Index: 2\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : \\overline{z}=1-i\\sqrt{\\nu}\n",
      "Length difference: 101\n",
      "\n",
      "Index: 3\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : S=\\int_{M}BF\n",
      "Length difference: 115\n",
      "\n",
      "Index: 4\n",
      "Predicted: \\frac{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\partial R}{\\\n",
      "Target   : Q:=\\bigcup_{\\sigma\\in\\sigma(T_{0})}Q_{\\sigma}\n",
      "Length difference: 82\n"
     ]
    }
   ],
   "source": [
    "def analyze_errors(predictions, targets):\n",
    "    errors = []\n",
    "    for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
    "        if pred != target:\n",
    "            errors.append({\n",
    "                'index': i,\n",
    "                'predicted': pred,\n",
    "                'target': target,\n",
    "                'length_diff': len(pred) - len(target)\n",
    "            })\n",
    "    \n",
    "    print(\"\\nError Analysis:\")\n",
    "    print(f\"Total errors: {len(errors)}\")\n",
    "    print(\"\\nSample errors:\")\n",
    "    for error in errors[:5]:\n",
    "        print(f\"\\nIndex: {error['index']}\")\n",
    "        print(f\"Predicted: {error['predicted']}\")\n",
    "        print(f\"Target   : {error['target']}\")\n",
    "        print(f\"Length difference: {error['length_diff']}\")\n",
    "\n",
    "# Run error analysis\n",
    "analyze_errors(predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27577d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
