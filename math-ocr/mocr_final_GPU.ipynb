{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d77f0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PARALLEL dataset preprocessing...\n",
      "Source: /home/ie643_errorcode500/errorcode500-working/ProcessedFullMathwriting\n",
      "Output: /home/ie643_errorcode500/errorcode500-working/FinalMath\n",
      "Available CPUs: 32\n",
      "\n",
      "This will use multiprocessing for 10-20x speedup!\n",
      "Estimated time for 454k images: 2-3 hours (vs 20+ hours sequential)\n",
      "\n",
      "Press Enter to continue...\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING TRAIN SPLIT (PARALLEL)\n",
      "============================================================\n",
      "Found 454,437 images to process\n",
      "Using 30 CPU cores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:   0%|▏                                                        | 1758/454437 [01:01<4:23:06, 28.67it/s]Process ForkPoolWorker-29:\n",
      "Process ForkPoolWorker-25:\n",
      "Process ForkPoolWorker-34:\n",
      "Process ForkPoolWorker-26:\n",
      "Process ForkPoolWorker-41:\n",
      "Process ForkPoolWorker-39:\n",
      "Process ForkPoolWorker-38:\n",
      "Process ForkPoolWorker-43:\n",
      "Process ForkPoolWorker-37:\n",
      "Process ForkPoolWorker-31:\n",
      "Process ForkPoolWorker-22:\n",
      "Process ForkPoolWorker-23:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-24:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-42:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-20:\n",
      "Process ForkPoolWorker-40:\n",
      "  File \"/opt/anaconda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/anaconda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-32:\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_ROOT, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_database.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(csv_file):\n\u001b[0;32m--> 159\u001b[0m     success, failed \u001b[38;5;241m=\u001b[39m \u001b[43mprecompute_9channel_dataset_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATASET_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROCESSED_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_WORKERS\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found, skipping \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m split\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 101\u001b[0m, in \u001b[0;36mprecompute_9channel_dataset_parallel\u001b[0;34m(csv_file, dataset_root, split, output_dir, num_workers)\u001b[0m\n\u001b[1;32m     98\u001b[0m failed_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(processes\u001b[38;5;241m=\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 101\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_single_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessing \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Collect results\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result, error \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # Cell 0: FAST MULTIPROCESSING PREPROCESSING\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import pickle\n",
    "\n",
    "def get_directional_kernels():\n",
    "    k = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
    "    kernels = [\n",
    "        k,\n",
    "        np.rot90(k, 1),\n",
    "        np.rot90(k, 2),\n",
    "        np.rot90(k, 3),\n",
    "        np.fliplr(k),\n",
    "        np.flipud(k),\n",
    "        np.fliplr(np.rot90(k, 1)),\n",
    "        np.flipud(np.rot90(k, 3)),\n",
    "    ]\n",
    "    return kernels\n",
    "\n",
    "def get_directional_maps(image):\n",
    "    kernels = get_directional_kernels()\n",
    "    edge_maps = [cv2.filter2D(image, -1, kern) for kern in kernels]\n",
    "    edge_maps = [(em.astype(np.float32) / 255.0) for em in edge_maps]\n",
    "    edge_maps = [np.clip(em, 0, 1) for em in edge_maps]\n",
    "    return np.stack(edge_maps, axis=0)\n",
    "\n",
    "def process_single_image(args):\n",
    "    \"\"\"Process a single image - used for multiprocessing\"\"\"\n",
    "    idx, filename, label, dataset_root, split, output_dir = args\n",
    "    \n",
    "    try:\n",
    "        # Construct paths\n",
    "        img_path = os.path.join(dataset_root, split, filename)\n",
    "        output_filename = filename.replace('.png', '.npz').replace('.jpg', '.npz')\n",
    "        output_path = os.path.join(output_dir, split, output_filename)\n",
    "        \n",
    "        # Read and process image\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            return None, f\"Could not read {img_path}\"\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        H, W = image.shape\n",
    "        \n",
    "        # Compute 9 channels\n",
    "        channels = np.zeros((9, H, W), dtype=np.float32)\n",
    "        channels[0] = image\n",
    "        channels[1:] = get_directional_maps(image)\n",
    "        \n",
    "        # Create subdirectories and save\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        np.savez_compressed(output_path, channels=channels)\n",
    "        \n",
    "        return {\n",
    "            'filename': output_filename,\n",
    "            'normalized_label': label,\n",
    "            'original_filename': filename\n",
    "        }, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None, f\"Error processing {filename}: {str(e)}\"\n",
    "\n",
    "def precompute_9channel_dataset_parallel(csv_file, dataset_root, split, output_dir, num_workers=30):\n",
    "    \"\"\"\n",
    "    Precompute dataset using multiprocessing - MUCH FASTER!\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PREPROCESSING {split.upper()} SPLIT (PARALLEL)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    split_output_dir = os.path.join(output_dir, split)\n",
    "    os.makedirs(split_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    total_images = len(df)\n",
    "    print(f\"Found {total_images:,} images to process\")\n",
    "    \n",
    "    # Determine number of workers\n",
    "    if num_workers is None:\n",
    "        num_workers = max(1, cpu_count() - 2)  # Leave 2 cores free\n",
    "    print(f\"Using {num_workers} CPU cores\")\n",
    "    \n",
    "    # Prepare arguments for multiprocessing\n",
    "    args_list = [\n",
    "        (idx, df.iloc[idx]['filename'], df.iloc[idx]['normalized_label'], \n",
    "         dataset_root, split, output_dir)\n",
    "        for idx in range(total_images)\n",
    "    ]\n",
    "    \n",
    "    # Process in parallel with progress bar\n",
    "    processed_data = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(process_single_image, args_list),\n",
    "            total=total_images,\n",
    "            desc=f\"Processing {split}\"\n",
    "        ))\n",
    "    \n",
    "    # Collect results\n",
    "    for result, error in results:\n",
    "        if result is not None:\n",
    "            processed_data.append(result)\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            if error and failed_count <= 10:  # Print first 10 errors\n",
    "                print(f\"\\n{error}\")\n",
    "    \n",
    "    # Save mapping CSV\n",
    "    mapping_df = pd.DataFrame(processed_data)\n",
    "    mapping_csv = os.path.join(output_dir, f'{split}_mapping.csv')\n",
    "    mapping_df.to_csv(mapping_csv, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PREPROCESSING {split.upper()} COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Successfully processed: {len(processed_data):,} / {total_images:,}\")\n",
    "    print(f\"Failed: {failed_count:,}\")\n",
    "    print(f\"Success rate: {len(processed_data)/total_images*100:.2f}%\")\n",
    "    print(f\"Saved to: {split_output_dir}\")\n",
    "    print(f\"Mapping saved to: {mapping_csv}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return len(processed_data), failed_count\n",
    "\n",
    "# ==============================\n",
    "# RUN PREPROCESSING\n",
    "# ==============================\n",
    "if __name__ == '__main__':  # Important for Windows multiprocessing\n",
    "    DATASET_ROOT = '/home/ie643_errorcode500/errorcode500-working/ProcessedFullMathwriting'\n",
    "    PROCESSED_ROOT = '/home/ie643_errorcode500/errorcode500-working/FinalMath'\n",
    "    \n",
    "    print(\"Starting PARALLEL dataset preprocessing...\")\n",
    "    print(f\"Source: {DATASET_ROOT}\")\n",
    "    print(f\"Output: {PROCESSED_ROOT}\")\n",
    "    print(f\"Available CPUs: {cpu_count()}\")\n",
    "    print(\"\\nThis will use multiprocessing for 10-20x speedup!\")\n",
    "    print(\"Estimated time for 454k images: 2-3 hours (vs 20+ hours sequential)\\n\")\n",
    "    \n",
    "    # You can adjust num_workers - more = faster (but uses more RAM)\n",
    "    NUM_WORKERS = 30  # Adjust based on your system (typically cpu_count() - 2)\n",
    "    \n",
    "    input(\"Press Enter to continue...\")\n",
    "    \n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Preprocess all splits\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        csv_file = os.path.join(DATASET_ROOT, f'{split}_database.csv')\n",
    "        if os.path.exists(csv_file):\n",
    "            success, failed = precompute_9channel_dataset_parallel(\n",
    "                csv_file, DATASET_ROOT, split, PROCESSED_ROOT, \n",
    "                num_workers=NUM_WORKERS\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: {csv_file} not found, skipping {split} split\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALL PREPROCESSING COMPLETE!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "    print(\"You can now use the Fast9ChDataset for training\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d146251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CPU cores available: 32\n",
      "Recommended workers: 30\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Check your system\n",
    "total_cores = cpu_count()\n",
    "print(f\"Total CPU cores available: {total_cores}\")\n",
    "\n",
    "# Rule of thumb for choosing workers:\n",
    "# 1. Leave 2-4 cores for system/other tasks\n",
    "# 2. Consider your RAM (each worker needs ~300-500MB)\n",
    "# 3. Consider I/O bottlenecks (SSD vs HDD)\n",
    "\n",
    "recommended_workers = max(1, total_cores - 2)\n",
    "print(f\"Recommended workers: {recommended_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: FAST Dataset Loading (loads precomputed 9-channel arrays)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "class Fast9ChDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Fast dataset that loads precomputed 9-channel numpy arrays\n",
    "    NO cv2.filter2D calls during training - 10x faster!\n",
    "    \"\"\"\n",
    "    def __init__(self, processed_root, split='train'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processed_root: Root directory of preprocessed data\n",
    "            split: 'train', 'val', or 'test'\n",
    "        \"\"\"\n",
    "        self.processed_root = processed_root\n",
    "        self.split = split\n",
    "        \n",
    "        # Load mapping CSV\n",
    "        mapping_csv = os.path.join(processed_root, f'{split}_mapping.csv')\n",
    "        if not os.path.exists(mapping_csv):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Mapping file not found: {mapping_csv}\\n\"\n",
    "                f\"Please run the preprocessing step first (Cell 0)\"\n",
    "            )\n",
    "        \n",
    "        self.data_frame = pd.read_csv(mapping_csv)\n",
    "        print(f\"Loaded {len(self.data_frame)} preprocessed {split} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.data_frame.iloc[idx]['filename']\n",
    "        label = self.data_frame.iloc[idx]['normalized_label']\n",
    "        \n",
    "        # Load precomputed 9-channel array (FAST!)\n",
    "        file_path = os.path.join(self.processed_root, self.split, filename)\n",
    "        \n",
    "        try:\n",
    "            data = np.load(file_path)\n",
    "            channels = data['channels']  # Already [9, H, W]\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"Could not load {file_path}: {str(e)}\")\n",
    "        \n",
    "        return {\n",
    "            'image': torch.from_numpy(channels).float(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# ==============================\n",
    "# Dataset Configuration\n",
    "# ==============================\n",
    "PROCESSED_ROOT = r'C:\\Users\\kani1\\Desktop\\IE643\\custom-dataset\\ProccessMathwritting-exercpt-9ch'\n",
    "\n",
    "# Load training dataset\n",
    "train_dataset = Fast9ChDataset(PROCESSED_ROOT, split='train')\n",
    "\n",
    "# Optimized DataLoader for A6000\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,  # Increased from 16 to 64 (4x more)\n",
    "    shuffle=True,\n",
    "    num_workers=12,  # More workers\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4  # Prefetch 4 batches per worker\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATASET READY FOR TRAINING\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Batch size: 64\")\n",
    "print(f\"Batches per epoch: {len(train_loader):,}\")\n",
    "print(f\"Workers: 12\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Test loading a batch\n",
    "try:\n",
    "    for batch in train_loader:\n",
    "        images, labels = batch['image'], batch['label']\n",
    "        print(f\"Test batch loaded successfully!\")\n",
    "        print(f\"Image shape: {images.shape}\")\n",
    "        print(f\"First 3 labels: {labels[:3]}\")\n",
    "        break\n",
    "except Exception as e:\n",
    "    print(f\"Error loading batch: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bf695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 91\n",
      "First 20 tokens: ['<PAD>', '<SOS>', '<EOS>', ' ', '!', '#', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4']\n",
      "Original label: \\vartheta=-\\frac{log\\frac{\\phi_{\\varsigma_{1}}}{\\phi_{\\varsigma_{2}}}}{log\\frac{\\varsigma_{1}}{\\varsigma_{2}}}\n",
      "Encoded: [1, 58, 83, 62, 79, 81, 69, 66, 81, 62, 28, 12, 58, 67, 79, 62, 64, 88, 73, 76]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "DATASET_ROOT = r\"C:\\Users\\kani1\\Desktop\\IE643\\custom-dataset\\ProcessedFullMathwriting\"\n",
    "# Load all labels from train/val/test CSVs\n",
    "csv_files = [\n",
    "    'train_database.csv',\n",
    "    'val_database.csv',\n",
    "    'test_database.csv'\n",
    "]\n",
    "\n",
    "\n",
    "all_labels = []\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(DATASET_ROOT, csv_file))\n",
    "    all_labels.extend(df['normalized_label'].astype(str).tolist())\n",
    "\n",
    "# Build character-level vocabulary\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>']\n",
    "char_counter = Counter()\n",
    "for label in all_labels:\n",
    "    char_counter.update(list(label))\n",
    "\n",
    "vocab = special_tokens + sorted(char_counter.keys())\n",
    "char2idx = {ch: idx for idx, ch in enumerate(vocab)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(\"First 20 tokens:\", vocab[:20])\n",
    "\n",
    "# Encode a label string to indices\n",
    "def encode_label(label, max_len=128):\n",
    "    tokens = [char2idx['<SOS>']] + [char2idx[ch] for ch in label] + [char2idx['<EOS>']]\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [char2idx['<PAD>']] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "sample_label = all_labels[0]\n",
    "encoded = encode_label(sample_label)\n",
    "print(\"Original label:\", sample_label)\n",
    "print(\"Encoded:\", encoded[:20])\n",
    "\n",
    "# For your dataset class, you can add:\n",
    "# label_indices = encode_label(label)\n",
    "# sample = {'image': image_tensor, 'label': label_indices}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9403e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Model Architecture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=4, dropout_p=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Conv2d(\n",
    "                in_channels if i == 0 else out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout_p > 0:\n",
    "                layers.append(nn.Dropout2d(p=dropout_p))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class WatcherFCN(nn.Module):\n",
    "    def __init__(self, in_channels=9):\n",
    "        super().__init__()\n",
    "        # Smaller architecture for faster training\n",
    "        self.block1 = ConvBlock(in_channels, 32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.block2 = ConvBlock(32, 64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.block3 = ConvBlock(64, 64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.block4 = ConvBlock(64, 128, dropout_p=0.2)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool4(x)\n",
    "        return x\n",
    "\n",
    "# Test model\n",
    "model = WatcherFCN(in_channels=9)\n",
    "dummy_input = torch.randn(2, 9, 480, 1600)\n",
    "output = model(dummy_input)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Count parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"WatcherFCN parameters: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9a66471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3000, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, channels, height, width = output.shape\n",
    "encoder_outputs = output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "# encoder_outputs: [batch, 3000, 512]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a64fe100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoverageAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim, coverage_dim):\n",
    "        super().__init__()\n",
    "        self.W_a = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U_a = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.U_f = nn.Linear(coverage_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden, coverage):\n",
    "        # encoder_outputs: [batch, L, encoder_dim]\n",
    "        # decoder_hidden: [batch, decoder_dim]\n",
    "        # coverage: [batch, L, coverage_dim]\n",
    "        Wh = self.W_a(decoder_hidden).unsqueeze(1)  # [batch, 1, att_dim]\n",
    "        Ua = self.U_a(encoder_outputs)              # [batch, L, att_dim]\n",
    "        Uf = self.U_f(coverage)                     # [batch, L, att_dim]\n",
    "        att = torch.tanh(Wh + Ua + Uf)              # [batch, L, att_dim]\n",
    "        scores = self.v(att).squeeze(-1)            # [batch, L]\n",
    "        alpha = F.softmax(scores, dim=1)            # [batch, L]\n",
    "        context = torch.sum(encoder_outputs * alpha.unsqueeze(-1), dim=1)  # [batch, encoder_dim]\n",
    "        return context, alpha\n",
    "\n",
    "# class ParserGRUDecoder(nn.Module):\n",
    "#     def __init__(self, vocab_size, encoder_dim=128, embed_dim=256, decoder_dim=256, attention_dim=256, coverage_dim=1):\n",
    "#         super().__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "#         self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "#         self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "#         self.fc = nn.Linear(decoder_dim + encoder_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, encoder_outputs, targets, max_len):\n",
    "#         batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "#         device = encoder_outputs.device\n",
    "#         coverage = torch.zeros(batch_size, L, 1, device=device)\n",
    "#         inputs = torch.full((batch_size,), 1, dtype=torch.long, device=device)  # <SOS> token index\n",
    "#         hidden = torch.zeros(batch_size, 256, device=device)\n",
    "#         outputs = []\n",
    "#         for t in range(max_len):\n",
    "#             embedded = self.embedding(inputs)  # [batch, embed_dim]\n",
    "#             context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "#             gru_input = torch.cat([embedded, context], dim=1)\n",
    "#             hidden = self.gru(gru_input, hidden)\n",
    "#             output = self.fc(torch.cat([hidden, context], dim=1))\n",
    "#             outputs.append(output)\n",
    "#             # Teacher forcing: use ground truth if available\n",
    "#             if targets is not None and t < targets.size(1):\n",
    "#                 inputs = targets[:, t]\n",
    "#             else:\n",
    "#                 inputs = output.argmax(dim=1)\n",
    "#             coverage = coverage + alpha.unsqueeze(-1)\n",
    "#         outputs = torch.stack(outputs, dim=1)  # [batch, max_len, vocab_size]\n",
    "#         return outputs\n",
    "\n",
    "\n",
    "#Modified ParserGRUDecoder\n",
    "\n",
    "class ParserGRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_dim=128, embed_dim=256, decoder_dim=256, attention_dim=256, coverage_dim=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Modify input size to include context vector\n",
    "        self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "        # Change output layer to use all available information\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(decoder_dim + encoder_dim + embed_dim, decoder_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(decoder_dim, vocab_size)\n",
    "        )\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "    def forward(self, encoder_outputs, targets, max_len):\n",
    "        batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "        device = encoder_outputs.device\n",
    "        coverage = torch.zeros(batch_size, L, 1, device=device)\n",
    "        inputs = torch.full((batch_size,), 1, dtype=torch.long, device=device)  # <SOS> token index\n",
    "        hidden = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(max_len):\n",
    "            # 1. Get current input embedding\n",
    "            embedded = self.embedding(inputs)  # [batch, embed_dim]\n",
    "            \n",
    "            # 2. Calculate attention and context\n",
    "            context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "            \n",
    "            # 3. Update GRU hidden state with concatenated input\n",
    "            gru_input = torch.cat([embedded, context], dim=1)\n",
    "            hidden = self.gru(gru_input, hidden)\n",
    "            \n",
    "            # 4. Generate output using all available information\n",
    "            # Concatenate current embedding, hidden state, and context\n",
    "            output = self.out(torch.cat([embedded, hidden, context], dim=1))\n",
    "            outputs.append(output)\n",
    "            \n",
    "            # 5. Teacher forcing or use own predictions\n",
    "            if targets is not None and t < targets.size(1):\n",
    "                inputs = targets[:, t]\n",
    "            else:\n",
    "                inputs = output.argmax(dim=1)\n",
    "            \n",
    "            # 6. Update coverage vector\n",
    "            coverage = coverage + alpha.unsqueeze(-1)\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)  # [batch, max_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "# encoder_outputs: [batch, L, encoder_dim] (flatten FCN output to [batch, L, 512])\n",
    "# targets: [batch, max_len] (token indices)\n",
    "# decoder = ParserGRUDecoder(vocab_size=len(vocab))\n",
    "# outputs = decoder(encoder_outputs, targets, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea65ca4",
   "metadata": {},
   "source": [
    "This is the main training code which we were using before. Now I am using a better code which is below this commented cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c6758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: OPTIMIZED TRAINING for A6000\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# ==============================\n",
    "# Device Setup\n",
    "# ==============================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    # A6000 optimizations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# ==============================\n",
    "# Initialize Models\n",
    "# ==============================\n",
    "watcher = WatcherFCN(in_channels=9).to(device)\n",
    "decoder = ParserGRUDecoder(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "# Print model sizes\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  WatcherFCN: {count_parameters(watcher):,}\")\n",
    "print(f\"  Decoder: {count_parameters(decoder):,}\")\n",
    "print(f\"  Total: {count_parameters(watcher) + count_parameters(decoder):,}\")\n",
    "\n",
    "# ==============================\n",
    "# Training Configuration\n",
    "# ==============================\n",
    "pad_idx = vocab.index('<PAD>')\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.AdamW(  # AdamW instead of Adadelta\n",
    "    list(watcher.parameters()) + list(decoder.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(  # Better scheduler\n",
    "    optimizer,\n",
    "    max_lr=1e-3,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.1\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "max_len = 128\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# ==============================\n",
    "# Helper Functions\n",
    "# ==============================\n",
    "def apply_weight_noise(model, std=0.01):\n",
    "    \"\"\"Adds Gaussian noise for regularization\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.add_(torch.randn_like(p) * std)\n",
    "\n",
    "# ==============================\n",
    "# Training Loop\n",
    "# ==============================\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STARTING TRAINING\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "training_start = time.time()\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start = time.time()\n",
    "        watcher.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            # Non-blocking transfer\n",
    "            images = batch['image'].to(device, non_blocking=True)\n",
    "            labels = [encode_label(lbl, max_len) for lbl in batch['label']]\n",
    "            labels = torch.tensor(labels, dtype=torch.long, device=device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            \n",
    "            try:\n",
    "                # Mixed precision forward pass\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    watcher_output = watcher(images)\n",
    "                    batch_size, channels, height, width = watcher_output.shape\n",
    "                    encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "                        batch_size, height * width, channels\n",
    "                    )\n",
    "\n",
    "                    outputs = decoder(encoder_outputs, labels, max_len)\n",
    "                    outputs_flat = outputs.view(-1, outputs.size(-1))\n",
    "                    labels_flat = labels.view(-1)\n",
    "\n",
    "                    loss = criterion(outputs_flat, labels_flat)\n",
    "\n",
    "                # Scaled backpropagation\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(watcher.parameters()) + list(decoder.parameters()), \n",
    "                    max_norm=5.0\n",
    "                )\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                scheduler.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'gpu': f'{torch.cuda.memory_allocated(0)/1e9:.1f}GB',\n",
    "                    'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
    "                })\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                print(f\"\\nError in batch {batch_idx}: {str(e)}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "        # Epoch summary\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / batch_count\n",
    "        imgs_per_sec = len(train_dataset) / epoch_time\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Summary\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"Time: {epoch_time/60:.1f} minutes\")\n",
    "        print(f\"Speed: {imgs_per_sec:.0f} images/sec\")\n",
    "        print(f\"GPU Memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # Apply weight noise\n",
    "        if epoch > 0 and epoch % 2 == 0:\n",
    "            apply_weight_noise(watcher, std=0.01)\n",
    "            apply_weight_noise(decoder, std=0.01)\n",
    "            print(\"Applied weight noise regularization\\n\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'watcher_state_dict': watcher.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"✓ Saved best model (loss: {best_loss:.4f})\\n\")\n",
    "\n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\nError during training: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'watcher_state_dict': watcher.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss / batch_count if 'batch_count' in locals() and batch_count > 0 else None,\n",
    "    }, 'final_model.pth')\n",
    "    \n",
    "    training_time = time.time() - training_start\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total time: {training_time/3600:.2f} hours\")\n",
    "    print(f\"Best loss: {best_loss:.4f}\")\n",
    "    print(f\"Models saved: best_model.pth, final_model.pth\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48364d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: EVALUATION\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(watcher, decoder, data_loader, split_name, max_samples=50):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    watcher.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    samples = []\n",
    "    \n",
    "    print(f\"\\nEvaluating on {split_name}...\")\n",
    "    for batch in tqdm(data_loader, desc=f'{split_name} Evaluation'):\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            watcher_output = watcher(images)\n",
    "            batch_size, channels, height, width = watcher_output.shape\n",
    "            encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(\n",
    "                batch_size, height * width, channels\n",
    "            )\n",
    "            outputs = decoder(encoder_outputs, None, max_len=128)\n",
    "        \n",
    "        for i in range(len(outputs)):\n",
    "            pred_indices = outputs[i].argmax(dim=-1)\n",
    "            pred_text = ''.join([idx2char[idx.item()] for idx in pred_indices \n",
    "                               if idx2char[idx.item()] not in ['<PAD>', '<SOS>', '<EOS>']])\n",
    "            true_text = labels[i]\n",
    "            \n",
    "            is_correct = (pred_text == true_text)\n",
    "            total_correct += is_correct\n",
    "            total_samples += 1\n",
    "            \n",
    "            if len(samples) < max_samples:\n",
    "                samples.append({\n",
    "                    'pred': pred_text,\n",
    "                    'true': true_text,\n",
    "                    'correct': is_correct\n",
    "                })\n",
    "        \n",
    "        if total_samples >= max_samples:\n",
    "            break\n",
    "    \n",
    "    accuracy = total_correct / total_samples\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{split_name.upper()} RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({total_correct}/{total_samples})\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return accuracy, samples\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "watcher = WatcherFCN(in_channels=9).to(device)\n",
    "decoder = ParserGRUDecoder(vocab_size=len(vocab)).to(device)\n",
    "watcher.load_state_dict(checkpoint['watcher_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "print(f\"Loaded model from epoch {checkpoint['epoch']} (loss: {checkpoint['loss']:.4f})\")\n",
    "\n",
    "# Validation\n",
    "val_dataset = Fast9ChDataset(PROCESSED_ROOT, split='val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
    "val_acc, val_samples = evaluate_model(watcher, decoder, val_loader, 'Validation', max_samples=50)\n",
    "\n",
    "# Test\n",
    "test_dataset = Fast9ChDataset(PROCESSED_ROOT, split='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_acc, test_samples = evaluate_model(watcher, decoder, test_loader, 'Test', max_samples=50)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
