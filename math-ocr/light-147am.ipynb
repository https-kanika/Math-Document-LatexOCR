{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7bb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: COMPLETE PREPROCESSING - Dataset Loading + Vocab Creation + Label Encoding\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "DATASET_ROOT = '/home/ie643_errorcode500/errorcode500-working/Mathwritting-1000'\n",
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PREPROCESSING PIPELINE\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 1: LaTeX Tokenization Function\n",
    "# ============================================================\n",
    "def custom_latex_tokenize(latex_str):\n",
    "    \"\"\"\n",
    "    Tokenize LaTeX string into meaningful units\n",
    "    Examples:\n",
    "        \"\\\\frac{a}{b}\" -> ['\\\\frac', '{', 'a', '}', '{', 'b', '}']\n",
    "        \"2x + 3\" -> ['2', 'x', '+', '3']\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    latex_str = latex_str.strip()\n",
    "    \n",
    "    while i < len(latex_str):\n",
    "        char = latex_str[i]\n",
    "        \n",
    "        # LaTeX commands (start with backslash)\n",
    "        if char == '\\\\':\n",
    "            if i + 1 < len(latex_str):\n",
    "                next_char = latex_str[i + 1]\n",
    "                # Single-character command (\\{, \\}, \\\\, \\%, etc.)\n",
    "                if not next_char.isalpha():\n",
    "                    tokens.append(latex_str[i:i+2])\n",
    "                    i += 2\n",
    "                # Multi-character alphabetic command (\\frac, \\sqrt, etc.)\n",
    "                else:\n",
    "                    j = i + 1\n",
    "                    while j < len(latex_str) and latex_str[j].isalpha():\n",
    "                        j += 1\n",
    "                    tokens.append(latex_str[i:j])\n",
    "                    i = j\n",
    "            else:\n",
    "                tokens.append('\\\\')\n",
    "                i += 1\n",
    "        \n",
    "        # Multi-digit numbers (including decimals)\n",
    "        elif char.isdigit():\n",
    "            j = i\n",
    "            while j < len(latex_str) and (latex_str[j].isdigit() or latex_str[j] == '.'):\n",
    "                j += 1\n",
    "            tokens.append(latex_str[i:j])\n",
    "            i = j\n",
    "        \n",
    "        # Skip whitespace\n",
    "        elif char.isspace():\n",
    "            i += 1\n",
    "        \n",
    "        # Single characters (brackets, operators, letters, symbols)\n",
    "        else:\n",
    "            tokens.append(char)\n",
    "            i += 1\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "print(\"✓ Step 1: LaTeX tokenization function loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 2: Build Vocabulary from ALL splits\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: BUILDING VOCABULARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "csv_files = ['train_database.csv', 'val_database.csv', 'test_database.csv']\n",
    "all_labels = []\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    csv_path = os.path.join(DATASET_ROOT, csv_file)\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        all_labels.extend(df['normalized_label'].astype(str).tolist())\n",
    "        print(f\"  Loaded {len(df):,} samples from {csv_file}\")\n",
    "    else:\n",
    "        print(f\"  ⚠️  Warning: {csv_path} not found\")\n",
    "\n",
    "print(f\"\\nTotal labels loaded: {len(all_labels):,}\")\n",
    "\n",
    "# Tokenize all labels and count frequencies\n",
    "special_tokens = ['<PAD>', '<SOS>', '<EOS>']\n",
    "token_counter = Counter()\n",
    "\n",
    "print(\"\\nTokenizing all LaTeX sequences...\")\n",
    "for label in tqdm(all_labels, desc=\"Tokenizing\"):\n",
    "    tokens = custom_latex_tokenize(label)\n",
    "    token_counter.update(tokens)\n",
    "\n",
    "# Build vocabulary: special tokens + sorted by frequency\n",
    "vocab_tokens = special_tokens + [tok for tok, _ in token_counter.most_common()]\n",
    "vocab_size = len(vocab_tokens)\n",
    "\n",
    "# Create mappings\n",
    "token2idx = {tok: idx for idx, tok in enumerate(vocab_tokens)}\n",
    "idx2token = {idx: tok for tok, idx in token2idx.items()}\n",
    "\n",
    "print(f\"\\n✓ Vocabulary created:\")\n",
    "print(f\"  Vocab size: {vocab_size:,}\")\n",
    "print(f\"  Most common tokens: {token_counter.most_common(10)}\")\n",
    "print(f\"  Special tokens: {special_tokens}\")\n",
    "print(vocab_tokens)\n",
    "\n",
    "# ============================================================\n",
    "# STEP 3: Encoding Function\n",
    "# ============================================================\n",
    "def encode_label_tokens(label, max_len=MAX_SEQ_LEN):\n",
    "    \"\"\"\n",
    "    Encode LaTeX string to token indices\n",
    "    Returns: List of token indices [<SOS>, tok1, tok2, ..., <EOS>, <PAD>, ...]\n",
    "    \"\"\"\n",
    "    tokens = custom_latex_tokenize(label)\n",
    "    token_indices = [token2idx['<SOS>']]\n",
    "    \n",
    "    for tok in tokens:\n",
    "        if tok in token2idx:\n",
    "            token_indices.append(token2idx[tok])\n",
    "        else:\n",
    "            print(f\"⚠️  Unknown token '{tok}' - skipping\")\n",
    "    \n",
    "    token_indices.append(token2idx['<EOS>'])\n",
    "    \n",
    "    # Pad or truncate\n",
    "    if len(token_indices) < max_len:\n",
    "        token_indices += [token2idx['<PAD>']] * (max_len - len(token_indices))\n",
    "    else:\n",
    "        token_indices = token_indices[:max_len]\n",
    "    \n",
    "    return token_indices\n",
    "\n",
    "print(\"\\n✓ Step 3: Encoding function loaded\")\n",
    "\n",
    "# ============================================================\n",
    "# STEP 4: Pre-encode ALL datasets and save to disk\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: PRE-ENCODING ALL DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def preprocess_and_save_dataset(csv_file, split_name):\n",
    "    \"\"\"Pre-encode labels and save to new CSV\"\"\"\n",
    "    csv_path = os.path.join(DATASET_ROOT, csv_file)\n",
    "    \n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"⚠️  Skipping {csv_file} - not found\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\nProcessing {split_name} set ({len(df):,} samples)...\")\n",
    "    \n",
    "    # Encode all labels\n",
    "    encoded_labels = []\n",
    "    for idx in tqdm(range(len(df)), desc=f\"Encoding {split_name}\"):\n",
    "        label = df.iloc[idx]['normalized_label']\n",
    "        encoded = encode_label_tokens(label, MAX_SEQ_LEN)\n",
    "        encoded_labels.append(str(encoded))  # Store as string for CSV\n",
    "    \n",
    "    # Add encoded column\n",
    "    df['encoded_label'] = encoded_labels\n",
    "    \n",
    "    # Save encoded dataset\n",
    "    output_path = csv_path.replace('.csv', '_encoded.csv')\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✓ Saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Pre-encode all splits\n",
    "train_encoded_path = preprocess_and_save_dataset('train_database.csv', 'TRAIN')\n",
    "val_encoded_path = preprocess_and_save_dataset('val_database.csv', 'VAL')\n",
    "test_encoded_path = preprocess_and_save_dataset('test_database.csv', 'TEST')\n",
    "\n",
    "# ============================================================\n",
    "# STEP 5: Dataset Class with Pre-Encoded Labels\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: CREATING DATASET WITH PRE-ENCODED LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class MathEquationEncodedDataset(Dataset):\n",
    "    def __init__(self, csv_file, dataset_root, split='train', transform=None):\n",
    "        \"\"\"Dataset that uses PRE-ENCODED labels\"\"\"\n",
    "        self.data_frame = pd.read_csv(csv_file)\n",
    "        self.dataset_root = dataset_root\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        if 'encoded_label' not in self.data_frame.columns:\n",
    "            raise ValueError(f\"CSV {csv_file} missing 'encoded_label' column!\")\n",
    "        \n",
    "        print(f\"  ✓ Loaded {split} dataset: {len(self.data_frame):,} samples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        filename = self.data_frame.iloc[idx]['filename']\n",
    "        img_path = os.path.join(self.dataset_root, self.split, filename)\n",
    "        img_path = os.path.normpath(img_path).replace('\\\\', '/')\n",
    "        \n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        \n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        channels = image[np.newaxis, :, :]  # [1, H, W]\n",
    "        \n",
    "        # Get PRE-ENCODED label (stored as string)\n",
    "        encoded_str = self.data_frame.iloc[idx]['encoded_label']\n",
    "        encoded_label = eval(encoded_str)  # Convert string back to list\n",
    "        \n",
    "        sample = {\n",
    "            'image': torch.tensor(channels, dtype=torch.float32),\n",
    "            'label': torch.tensor(encoded_label, dtype=torch.long)  # Already encoded!\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample['image'] = self.transform(sample['image'])\n",
    "        \n",
    "        return sample\n",
    "\n",
    "# ============================================================\n",
    "# STEP 6: Create DataLoaders\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 6: CREATING DATALOADERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_dataset = MathEquationEncodedDataset(\n",
    "    train_encoded_path, \n",
    "    DATASET_ROOT, \n",
    "    split='train'\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Train DataLoader created:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Batches per epoch: {len(train_loader)}\")\n",
    "\n",
    "# Test batch loading\n",
    "try:\n",
    "    test_batch = next(iter(train_loader))\n",
    "    print(f\"\\n✓ Test batch loaded successfully:\")\n",
    "    print(f\"  Image shape: {test_batch['image'].shape}\")\n",
    "    print(f\"  Label shape: {test_batch['label'].shape}\")\n",
    "    print(f\"  Label dtype: {test_batch['label'].dtype}\")\n",
    "    print(f\"  Sample label (first 10 tokens): {test_batch['label'][0][:10].tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Error loading test batch: {e}\")\n",
    "\n",
    "# ============================================================\n",
    "# PREPROCESSING COMPLETE\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"✓ Train samples: {len(train_dataset):,}\")\n",
    "print(f\"✓ Max sequence length: {MAX_SEQ_LEN}\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"✓ All labels pre-encoded and cached to disk\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Ready for training! All encoding is done.\")\n",
    "print(\"Training loop will use pre-encoded labels directly.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9403e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 30, 100])\n"
     ]
    }
   ],
   "source": [
    "# Replace your WatcherFCN cell with this LIGHTWEIGHT version:\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_layers=2, dropout_p=0.0):  # ← REDUCED from 4 to 2 layers!\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(nn.Conv2d(\n",
    "                in_channels if i == 0 else out_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ))\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            if dropout_p > 0:\n",
    "                layers.append(nn.Dropout2d(p=dropout_p))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class WatcherFCN(nn.Module):\n",
    "    \"\"\"\n",
    "    LIGHTWEIGHT Encoder with:\n",
    "    - Only 3 blocks (reduced from 4)\n",
    "    - Smaller channels (16->32->64 instead of 32->64->128)\n",
    "    - Adaptive pooling to fixed 500 sequence positions\n",
    "    - Final channel dim = 64 (reduced from 128)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Block 1: 1 -> 16 channels\n",
    "        self.block1 = ConvBlock(in_channels, 16, num_layers=2)  # ← Smaller!\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 2: 16 -> 32 channels\n",
    "        self.block2 = ConvBlock(16, 32, num_layers=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Block 3: 32 -> 64 channels (with dropout)\n",
    "        self.block3 = ConvBlock(32, 64, num_layers=2, dropout_p=0.2)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # ⭐ NEW: Adaptive pooling to FIXED size [10, 50] = 500 positions\n",
    "        # This works for ANY input image size!\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((10, 50))\n",
    "        \n",
    "        # ⭐ No need for channel reduction - already at 64!\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch, 1, H, W] -> [batch, 16, H/2, W/2]\n",
    "        x = self.block1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # [batch, 16, H/2, W/2] -> [batch, 32, H/4, W/4]\n",
    "        x = self.block2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # [batch, 32, H/4, W/4] -> [batch, 64, H/8, W/8]\n",
    "        x = self.block3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        # ⭐ [batch, 64, H/8, W/8] -> [batch, 64, 10, 50]\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        return x  # [batch, 64, 10, 50]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Test the new architecture\n",
    "# ============================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING LIGHTWEIGHT WATCHERFCN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = WatcherFCN(in_channels=1)\n",
    "dummy_input = torch.randn(2, 1, 480, 1600)\n",
    "output = model(dummy_input)\n",
    "\n",
    "print(f\"\\nInput shape:  {dummy_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "batch_size, channels, height, width = output.shape\n",
    "encoder_outputs = output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "\n",
    "print(f\"\\nEncoder outputs shape: {encoder_outputs.shape}\")\n",
    "print(f\"  Sequence length: {height * width} (reduced from 3000!)\")\n",
    "print(f\"  Channel dim:     {channels} (reduced from 128!)\")\n",
    "\n",
    "# Calculate memory savings\n",
    "old_memory = 2 * 3000 * 128 * 4 / 1e6  # batch=2, old size\n",
    "new_memory = 2 * 500 * 64 * 4 / 1e6    # batch=2, new size\n",
    "print(f\"\\nMemory comparison:\")\n",
    "print(f\"  OLD: {old_memory:.2f} MB per batch\")\n",
    "print(f\"  NEW: {new_memory:.2f} MB per batch\")\n",
    "print(f\"  Savings: {old_memory/new_memory:.1f}× less memory!\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel parameters: {total_params:,}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9a66471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3000, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, channels, height, width = output.shape\n",
    "encoder_outputs = output.permute(0, 2, 3, 1).reshape(batch_size, height * width, channels)\n",
    "# encoder_outputs: [batch, 3000, 512]\n",
    "encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c683663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoverageAttention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim, coverage_dim):\n",
    "        super().__init__()\n",
    "        self.W_a = nn.Linear(decoder_dim, attention_dim)\n",
    "        self.U_a = nn.Linear(encoder_dim, attention_dim)\n",
    "        self.U_f = nn.Linear(coverage_dim, attention_dim)\n",
    "        self.v = nn.Linear(attention_dim, 1)\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_hidden, coverage):\n",
    "        # encoder_outputs: [batch, L, encoder_dim]\n",
    "        # decoder_hidden: [batch, decoder_dim]\n",
    "        # coverage: [batch, L, coverage_dim]\n",
    "        Wh = self.W_a(decoder_hidden).unsqueeze(1)  # [batch, 1, att_dim]\n",
    "        Ua = self.U_a(encoder_outputs)              # [batch, L, att_dim]\n",
    "        Uf = self.U_f(coverage)                     # [batch, L, att_dim]\n",
    "        att = torch.tanh(Wh + Ua + Uf)              # [batch, L, att_dim]\n",
    "        scores = self.v(att).squeeze(-1)            # [batch, L]\n",
    "        alpha = F.softmax(scores, dim=1)            # [batch, L]\n",
    "        context = torch.sum(encoder_outputs * alpha.unsqueeze(-1), dim=1)  # [batch, encoder_dim]\n",
    "        return context, alpha\n",
    "\n",
    "\n",
    "# In your decoder cell, update the class:\n",
    "\n",
    "class ParserGRUDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_dim=64, embed_dim=128, decoder_dim=128, attention_dim=128, coverage_dim=1):  # ← CHANGED dims!\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRUCell(embed_dim + encoder_dim, decoder_dim)\n",
    "        self.attention = CoverageAttention(encoder_dim, decoder_dim, attention_dim, coverage_dim)\n",
    "        \n",
    "        # Simpler output layer\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(decoder_dim + encoder_dim + embed_dim, decoder_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(decoder_dim, vocab_size)\n",
    "        )\n",
    "        self.decoder_dim = decoder_dim\n",
    "\n",
    "    def forward(self, encoder_outputs, targets, max_len):\n",
    "        batch_size, L, encoder_dim = encoder_outputs.size()\n",
    "        device = encoder_outputs.device\n",
    "        coverage = torch.zeros(batch_size, L, 1, device=device)  \n",
    "\n",
    "        hidden = torch.zeros(batch_size, self.decoder_dim, device=device)\n",
    "        sos_token_idx = token2idx['<SOS>']\n",
    "        inputs = torch.full((batch_size,), sos_token_idx, dtype=torch.long, device=device)\n",
    "        \n",
    "        outputs = []\n",
    "\n",
    "        for t in range(max_len):\n",
    "            embedded = self.embedding(inputs)\n",
    "            context, alpha = self.attention(encoder_outputs, hidden, coverage)\n",
    "            gru_input = torch.cat([embedded, context], dim=1)\n",
    "            hidden = self.gru(gru_input, hidden)\n",
    "            output = self.out(torch.cat([embedded, hidden, context], dim=1))\n",
    "            outputs.append(output)\n",
    "            \n",
    "            if targets is not None and t < targets.size(1):\n",
    "                inputs = targets[:, t]\n",
    "            else:\n",
    "                inputs = output.argmax(dim=1)\n",
    "            \n",
    "            coverage = coverage + alpha.unsqueeze(-1)\n",
    "            \n",
    "            # ⭐ Memory cleanup every 20 steps\n",
    "            if t % 20 == 0 and t > 0:\n",
    "                del embedded, context, alpha, gru_input\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "        \n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "# encoder_outputs: [batch, L, encoder_dim] (flatten FCN output to [batch, L, 512])\n",
    "# targets: [batch, max_len] (token indices)\n",
    "# decoder = ParserGRUDecoder(vocab_size=len(vocab))\n",
    "# outputs = decoder(encoder_outputs, targets, max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09650a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# ============================================================\n",
    "# Memory Configuration\n",
    "# ============================================================\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ============================================================\n",
    "# Device Setup\n",
    "# ============================================================\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(f\"Free Memory: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)) / 1e9:.2f} GB\")\n",
    "\n",
    "# ============================================================\n",
    "# Initialize Models\n",
    "# ============================================================\n",
    "watcher = WatcherFCN(in_channels=1).to(device)\n",
    "decoder = ParserGRUDecoder(\n",
    "    vocab_size=vocab_size,\n",
    "    encoder_dim=64,    # ← CHANGED from 128!\n",
    "    embed_dim=128,     # ← REDUCED from 256!\n",
    "    decoder_dim=128,   # ← REDUCED from 256!\n",
    "    attention_dim=128  # ← REDUCED from 256!\n",
    ").to(device)\n",
    "\n",
    "# Loss function\n",
    "pad_idx = token2idx['<PAD>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx, label_smoothing=0.1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    list(watcher.parameters()) + list(decoder.parameters()),\n",
    "    lr=1e-3,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=5, T_mult=2, eta_min=1e-6\n",
    ")\n",
    "\n",
    "# Training config\n",
    "num_epochs = 10\n",
    "max_len = MAX_SEQ_LEN\n",
    "best_loss = float('inf')\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"Max sequence length: {max_len}\")\n",
    "print(f\"Batch size: {train_loader.batch_size}\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "total_params = sum(p.numel() for p in list(watcher.parameters()) + list(decoder.parameters()))\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "print(f\"GPU Memory (after load): {torch.cuda.memory_allocated(0)/1e9:.2f}GB\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# TRAINING LOOP - NO ENCODING!\n",
    "# ============================================================\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        watcher.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, batch in enumerate(pbar):\n",
    "            try:\n",
    "                # ⭐ NO ENCODING! Labels are already encoded tensors\n",
    "                images = batch['image'].to(device, non_blocking=True)\n",
    "                labels = batch['label'].to(device, non_blocking=True)  # Already torch.Tensor!\n",
    "                \n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # Forward pass\n",
    "                    watcher_output = watcher(images)\n",
    "                    b, c, h, w = watcher_output.shape\n",
    "                    encoder_outputs = watcher_output.permute(0, 2, 3, 1).reshape(b, h*w, c)\n",
    "                    \n",
    "                    outputs = decoder(encoder_outputs, labels, max_len)\n",
    "                    outputs = outputs.view(-1, vocab_size)\n",
    "                    labels_flat = labels.view(-1)\n",
    "                    \n",
    "                    loss = criterion(outputs, labels_flat)\n",
    "                \n",
    "                # Backward pass\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(watcher.parameters()) + list(decoder.parameters()), \n",
    "                    max_norm=1.0\n",
    "                )\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Update progress\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}',\n",
    "                    'gpu': f'{torch.cuda.memory_allocated(0)/1e9:.1f}GB'\n",
    "                })\n",
    "                \n",
    "                # Cleanup\n",
    "                del watcher_output, encoder_outputs, outputs, labels_flat, loss\n",
    "                \n",
    "                if batch_idx % 5 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(f\"\\n⚠️  OOM at batch {batch_idx}\")\n",
    "                    print(f\"    Memory: {torch.cuda.memory_allocated(0)/1e9:.2f}GB / \"\n",
    "                          f\"{torch.cuda.memory_reserved(0)/1e9:.2f}GB\")\n",
    "                    \n",
    "                    # Cleanup\n",
    "                    for var in ['images', 'labels', 'watcher_output', 'encoder_outputs', \n",
    "                                'outputs', 'labels_flat', 'loss']:\n",
    "                        if var in locals():\n",
    "                            del locals()[var]\n",
    "                    \n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        # End of epoch\n",
    "        avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"  Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        print(f\"  GPU: {torch.cuda.memory_allocated(0)/1e9:.2f}GB / \"\n",
    "              f\"{torch.cuda.memory_reserved(0)/1e9:.2f}GB\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'watcher_state_dict': watcher.state_dict(),\n",
    "                'decoder_state_dict': decoder.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'loss': best_loss,\n",
    "                'vocab_size': vocab_size,\n",
    "                'token2idx': token2idx,\n",
    "                'idx2token': idx2token,\n",
    "            }, 'best_model.pth')\n",
    "            print(f\"  ✓ Saved best model (loss: {best_loss:.4f})\")\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠️  Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'watcher_state_dict': watcher.state_dict(),\n",
    "        'decoder_state_dict': decoder.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss / batch_count if 'total_loss' in locals() and batch_count > 0 else None,\n",
    "        'vocab_size': vocab_size,\n",
    "        'token2idx': token2idx,\n",
    "        'idx2token': idx2token,\n",
    "    }, 'final_model.pth')\n",
    "    print(\"\\n✓ Final model saved\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43408d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: UPDATED Beam Search Decoding for Token-Based Vocabulary\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(watcher, decoder, image, beam_width=5, max_len=128, length_penalty=0.7):\n",
    "    \"\"\"\n",
    "    Beam search decoding with coverage penalty and length normalization\n",
    "    UPDATED: Uses token-based vocabulary (token2idx/idx2token)\n",
    "    \"\"\"\n",
    "    watcher.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Encode image\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    \n",
    "    encoder_out = watcher(image)\n",
    "    batch_size, channels, height, width = encoder_out.shape\n",
    "    encoder_outputs = encoder_out.permute(0, 2, 3, 1).reshape(\n",
    "        batch_size, height * width, channels\n",
    "    )\n",
    "\n",
    "    # CHANGED: Use token2idx for special tokens\n",
    "    start_token = token2idx['<SOS>']\n",
    "    end_token = token2idx['<EOS>']\n",
    "    pad_token = token2idx['<PAD>']\n",
    "    \n",
    "    # Initialize\n",
    "    coverage = torch.zeros(1, encoder_outputs.size(1), 1, device=device)\n",
    "    hidden = torch.zeros(1, decoder.decoder_dim, device=device)\n",
    "\n",
    "    beams = [(torch.tensor([start_token], device=device), hidden, 0.0, coverage, 0.0)]\n",
    "    completed_beams = []\n",
    "\n",
    "    for step in range(max_len):\n",
    "        if len(completed_beams) >= beam_width:\n",
    "            break\n",
    "            \n",
    "        candidates = []\n",
    "        \n",
    "        for seq, h, score, cov, _ in beams:\n",
    "            if seq[-1].item() == end_token:\n",
    "                completed_beams.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            # Decode one step\n",
    "            embedded = decoder.embedding(seq[-1].unsqueeze(0))\n",
    "            context, alpha = decoder.attention(encoder_outputs, h, cov)\n",
    "            gru_input = torch.cat([embedded, context], dim=1)\n",
    "            new_hidden = decoder.gru(gru_input, h)\n",
    "            output = decoder.out(torch.cat([embedded, new_hidden, context], dim=1))\n",
    "            log_probs = F.log_softmax(output, dim=1)\n",
    "\n",
    "            topk_probs, topk_idx = log_probs.topk(beam_width * 2, dim=1)\n",
    "            \n",
    "            # Coverage penalty\n",
    "            coverage_penalty = 0.5 * torch.sum(torch.min(cov.squeeze(-1), alpha)).item()\n",
    "            \n",
    "            for k in range(topk_probs.size(1)):\n",
    "                next_token = topk_idx[0, k].item()\n",
    "                \n",
    "                # Skip PAD tokens\n",
    "                if next_token == pad_token:\n",
    "                    continue\n",
    "                \n",
    "                # Block immediate repetitions (except special tokens)\n",
    "                if (len(seq) > 0 and \n",
    "                    next_token == seq[-1].item() and \n",
    "                    next_token not in [start_token, end_token, pad_token]):\n",
    "                    continue\n",
    "                \n",
    "                next_seq = torch.cat([seq, topk_idx[0, k].unsqueeze(0)])\n",
    "                new_cov = cov + alpha.unsqueeze(-1)\n",
    "                raw_score = score + topk_probs[0, k].item() - coverage_penalty\n",
    "                \n",
    "                # Length normalization\n",
    "                length = next_seq.size(0)\n",
    "                norm_score = raw_score / (length ** length_penalty)\n",
    "                \n",
    "                candidates.append((next_seq, new_hidden, raw_score, new_cov, norm_score))\n",
    "\n",
    "        if not candidates:\n",
    "            break\n",
    "        \n",
    "        beams = sorted(candidates, key=lambda x: x[4], reverse=True)[:beam_width]\n",
    "\n",
    "    # Add incomplete beams\n",
    "    for seq, _, score, _, _ in beams:\n",
    "        if seq[-1].item() != end_token:\n",
    "            completed_beams.append((seq, score))\n",
    "\n",
    "    if not completed_beams:\n",
    "        completed_beams = [(seq, score) for seq, _, score, _, _ in beams]\n",
    "\n",
    "    best_seq, best_score = max(completed_beams, key=lambda x: x[1])\n",
    "    \n",
    "    # CHANGED: Decode tokens to LaTeX string\n",
    "    decoded_tokens = []\n",
    "    for idx in best_seq:\n",
    "        token_idx = idx.item()\n",
    "        if token_idx == start_token:\n",
    "            continue\n",
    "        if token_idx == end_token:\n",
    "            break\n",
    "        if token_idx == pad_token:\n",
    "            continue\n",
    "        \n",
    "        # CHANGED: Use idx2token instead of idx2char\n",
    "        if token_idx in idx2token:\n",
    "            decoded_tokens.append(idx2token[token_idx])\n",
    "    \n",
    "    # Join tokens (no separator for LaTeX)\n",
    "    return ''.join(decoded_tokens)\n",
    "\n",
    "\n",
    "print(\"Beam search function loaded successfully\")\n",
    "print(\"  Using token-based vocabulary\")\n",
    "print(f\"  Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48364d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Best training loss: 2.0836\n",
      "Evaluating on Validation Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Accuracy: 0.00% (0/10)\n",
      "============================================================\n",
      "\n",
      "Sample 1 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : f_{\\omega+1}(f_{\\omega}(3))-2\n",
      "------------------------------------------------------------\n",
      "Sample 2 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : s\\notin\\alpha,t\\notin\\gamma\n",
      "------------------------------------------------------------\n",
      "Sample 3 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : \\int f(x)dx\n",
      "------------------------------------------------------------\n",
      "Sample 4 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : \\hat{y}(f)\n",
      "------------------------------------------------------------\n",
      "Sample 5 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : g(n)=\\prod_{k=1}^{n}f(k)\n",
      "------------------------------------------------------------\n",
      "Sample 6 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : PPI(monitor)=\\frac{NumberofPixels}{SizeinInches}=\\frac{1920}{20}=96ppi\n",
      "------------------------------------------------------------\n",
      "Sample 7 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : [1-\\lambda R(\\underline{k},\\omega)]^{n}u(n)\n",
      "------------------------------------------------------------\n",
      "Sample 8 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : \\frac{1372}{3}\\pi\n",
      "------------------------------------------------------------\n",
      "Sample 9 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : \\{e_{\\mu}\\}\n",
      "------------------------------------------------------------\n",
      "Sample 10 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : \\langle(\\delta u(r))^{n}\\rangle=C_{n}\\langle(\\epsilon r)^{\\frac{n}{3}}\\rangle,\n",
      "------------------------------------------------------------\n",
      "\n",
      "Evaluating on Test Set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Accuracy: 0.00% (0/10)\n",
      "============================================================\n",
      "\n",
      "Sample 1 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : M,w\\models I^{\\alpha}(e)\n",
      "------------------------------------------------------------\n",
      "Sample 2 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : dX=\\frac{\\partial X}{\\partial x}dx=F^{-1}dx=HdxordX_{M}=\\frac{\\partial X_{M}}{\\partial x_{n}}dx_{n}\n",
      "------------------------------------------------------------\n",
      "Sample 3 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : TU=\\sqrt{\\frac{DU^{3}}{G*M}}\n",
      "------------------------------------------------------------\n",
      "Sample 4 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : (\\frac{4}{7}-9)^{204\\cdot\\sqrt{5}}\n",
      "------------------------------------------------------------\n",
      "Sample 5 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)\n",
      "True     : \\frac{1}{M^{n-1}\\cdot s}\n",
      "------------------------------------------------------------\n",
      "Sample 6 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)\n",
      "True     : \\frac{1}{5}\n",
      "------------------------------------------------------------\n",
      "Sample 7 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : V=\\frac{1}{3}Ah\\rightarrow3V=Ah\\rightarrow Ah=3V\\rightarrow h=3\\frac{V}{A}\n",
      "------------------------------------------------------------\n",
      "Sample 8 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : w_{z}^{\\psi}\n",
      "------------------------------------------------------------\n",
      "Sample 9 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : \\alpha\\vee\\rho\n",
      "------------------------------------------------------------\n",
      "Sample 10 ✗\n",
      "Predicted: \\tilde{n}(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(x)(\n",
      "True     : \\kappa:X\\times B\\rightarrow[0,1]\n",
      "------------------------------------------------------------\n",
      "\n",
      "Final GPU Memory Usage: 0.18GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell: FIXED Evaluation - Uses Pre-Encoded Labels\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import editdistance\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "watcher = WatcherFCN(in_channels=1)\n",
    "decoder = ParserGRUDecoder(vocab_size=vocab_size)\n",
    "\n",
    "checkpoint = torch.load('best_model.pth', map_location=device)\n",
    "watcher.load_state_dict(checkpoint['watcher_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "if 'vocab_size' in checkpoint:\n",
    "    assert checkpoint['vocab_size'] == vocab_size, \"Vocabulary size mismatch!\"\n",
    "    print(f\"✓ Vocabulary size verified: {vocab_size}\")\n",
    "\n",
    "watcher = watcher.to(device)\n",
    "decoder = decoder.to(device)\n",
    "watcher.eval()\n",
    "decoder.eval()\n",
    "\n",
    "print(\"✓ Model loaded successfully!\")\n",
    "print(f\"  Best training loss: {checkpoint['loss']:.4f}\")\n",
    "\n",
    "def calculate_wer(reference, hypothesis):\n",
    "    \"\"\"Calculate Word Error Rate for LaTeX tokens\"\"\"\n",
    "    ref_tokens = custom_latex_tokenize(reference)\n",
    "    hyp_tokens = custom_latex_tokenize(hypothesis)\n",
    "    \n",
    "    if len(ref_tokens) == 0 and len(hyp_tokens) == 0:\n",
    "        return 0.0\n",
    "    if len(ref_tokens) == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    distance = editdistance.eval(ref_tokens, hyp_tokens)\n",
    "    wer = distance / len(ref_tokens)\n",
    "    return wer\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(watcher, decoder, data_loader, max_samples=50):\n",
    "    \"\"\"\n",
    "    FIXED: Evaluate using PRE-ENCODED labels\n",
    "    \"\"\"\n",
    "    watcher.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_wer = 0.0\n",
    "    samples = []\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc='Evaluating'):\n",
    "        images = batch['image'].to(device, non_blocking=True)\n",
    "        # CHANGED: Labels are already encoded tensors, need to decode them\n",
    "        encoded_labels = batch['label']  # [batch, 128] tensor of token indices\n",
    "        \n",
    "        for i in range(len(images)):\n",
    "            if total_samples >= max_samples:\n",
    "                break\n",
    "            \n",
    "            single_img = images[i]  # [1, H, W]\n",
    "            encoded_label = encoded_labels[i]  # [128] tensor\n",
    "            \n",
    "            # DECODE the encoded label to LaTeX string\n",
    "            true_text = decode_tokens_to_latex(encoded_label)\n",
    "            \n",
    "            # Predict\n",
    "            pred_text = beam_search_decode(watcher, decoder, single_img, beam_width=5)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            is_correct = (pred_text == true_text)\n",
    "            wer = calculate_wer(true_text, pred_text)\n",
    "            \n",
    "            total_correct += is_correct\n",
    "            total_wer += wer\n",
    "            total_samples += 1\n",
    "            \n",
    "            if len(samples) < max_samples:\n",
    "                samples.append({\n",
    "                    'pred': pred_text,\n",
    "                    'true': true_text,\n",
    "                    'correct': is_correct,\n",
    "                    'wer': wer\n",
    "                })\n",
    "        \n",
    "        if total_samples % 20 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        if total_samples >= max_samples:\n",
    "            break\n",
    "    \n",
    "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
    "    avg_wer = total_wer / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"METRICS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Exact Match Accuracy: {accuracy:.2%} ({total_correct}/{total_samples})\")\n",
    "    print(f\"Average WER:          {avg_wer:.4f}\")\n",
    "    print(f\"WER Percentage:       {avg_wer*100:.2f}%\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Show samples\n",
    "    print(\"Sample Predictions (first 10):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, sample in enumerate(samples[:10], 1):\n",
    "        status = '✓' if sample['correct'] else '✗'\n",
    "        print(f\"\\n{i}. {status} [WER: {sample['wer']:.4f}]\")\n",
    "        print(f\"   True:      {sample['true']}\")\n",
    "        print(f\"   Predicted: {sample['pred']}\")\n",
    "        \n",
    "        if not sample['correct']:\n",
    "            true_tokens = custom_latex_tokenize(sample['true'])\n",
    "            pred_tokens = custom_latex_tokenize(sample['pred'])\n",
    "            print(f\"   True tokens:  {true_tokens[:10]}{'...' if len(true_tokens) > 10 else ''}\")\n",
    "            print(f\"   Pred tokens:  {pred_tokens[:10]}{'...' if len(pred_tokens) > 10 else ''}\")\n",
    "            \n",
    "            distance = editdistance.eval(true_tokens, pred_tokens)\n",
    "            print(f\"   Token count (true/pred): {len(true_tokens)}/{len(pred_tokens)}\")\n",
    "            print(f\"   Edit distance: {distance}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # WER distribution\n",
    "    print(\"\\nWER Distribution:\")\n",
    "    wer_perfect = sum(1 for s in samples if s['wer'] == 0.0)\n",
    "    wer_low = sum(1 for s in samples if 0.0 < s['wer'] <= 0.1)\n",
    "    wer_medium = sum(1 for s in samples if 0.1 < s['wer'] <= 0.5)\n",
    "    wer_high = sum(1 for s in samples if s['wer'] > 0.5)\n",
    "    \n",
    "    print(f\"  Perfect (WER = 0.0):      {wer_perfect:3d} ({wer_perfect/len(samples)*100:.1f}%)\")\n",
    "    print(f\"  Low (0.0 < WER ≤ 0.1):    {wer_low:3d} ({wer_low/len(samples)*100:.1f}%)\")\n",
    "    print(f\"  Medium (0.1 < WER ≤ 0.5): {wer_medium:3d} ({wer_medium/len(samples)*100:.1f}%)\")\n",
    "    print(f\"  High (WER > 0.5):         {wer_high:3d} ({wer_high/len(samples)*100:.1f}%)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    return accuracy, avg_wer, samples\n",
    "\n",
    "\n",
    "# HELPER: Decode token indices back to LaTeX string\n",
    "def decode_tokens_to_latex(encoded_label):\n",
    "    \"\"\"\n",
    "    Convert encoded label tensor to LaTeX string\n",
    "    Args:\n",
    "        encoded_label: torch.Tensor [128] of token indices\n",
    "    Returns:\n",
    "        latex_str: LaTeX string (without special tokens/padding)\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for idx in encoded_label:\n",
    "        token_idx = idx.item()\n",
    "        \n",
    "        # Stop at EOS or PAD\n",
    "        if token_idx == token2idx['<EOS>'] or token_idx == token2idx['<PAD>']:\n",
    "            break\n",
    "        \n",
    "        # Skip SOS\n",
    "        if token_idx == token2idx['<SOS>']:\n",
    "            continue\n",
    "        \n",
    "        # Add token\n",
    "        if token_idx in idx2token:\n",
    "            tokens.append(idx2token[token_idx])\n",
    "    \n",
    "    return ''.join(tokens)\n",
    "\n",
    "\n",
    "# FIXED: Use MathEquationEncodedDataset (with pre-encoded labels)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VALIDATION SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "VAL_CSV_ENCODED = os.path.join(DATASET_ROOT, 'val_database_encoded.csv')  # ← Use encoded CSV!\n",
    "val_dataset = MathEquationEncodedDataset(VAL_CSV_ENCODED, DATASET_ROOT, split='val')\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=4,  # Reduced from 8\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_accuracy, val_wer, val_samples = evaluate_model(watcher, decoder, val_loader, max_samples=50)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "TEST_CSV_ENCODED = os.path.join(DATASET_ROOT, 'test_database_encoded.csv')  # ← Use encoded CSV!\n",
    "test_dataset = MathEquationEncodedDataset(TEST_CSV_ENCODED, DATASET_ROOT, split='test')\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_accuracy, test_wer, test_samples = evaluate_model(watcher, decoder, test_loader, max_samples=50)\n",
    "\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<25} {'Validation':<15} {'Test':<15}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Exact Match Accuracy':<25} {val_accuracy:>14.2%} {test_accuracy:>14.2%}\")\n",
    "print(f\"{'Average WER':<25} {val_wer:>14.4f} {test_wer:>14.4f}\")\n",
    "print(f\"{'WER Percentage':<25} {val_wer*100:>13.2f}% {test_wer*100:>13.2f}%\")\n",
    "print(f\"{'Vocabulary Size':<25} {vocab_size:>14,}\")\n",
    "print(f\"{'GPU Memory (Reserved)':<25} {torch.cuda.memory_reserved(0)/1e9:>13.2f}GB\")\n",
    "print(f\"{'GPU Memory (Allocated)':<25} {torch.cuda.memory_allocated(0)/1e9:>13.2f}GB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfff850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
